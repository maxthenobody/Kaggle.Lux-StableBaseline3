{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from Modified_stablebaseline3_PPO.modified_ppo_20250228_01 import PPO\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import copy\n",
    "from GreedyLRScheduler import GreedyLR\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-01 15:22:56,778:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "init_env = ModifiedLuxAIS3GymEnv(numpy_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MultiInputPolicy\", init_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiInputActorCriticPolicy(\n",
       "  (features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=2466, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=2466, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=576, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  1,  5],\n",
       "         [ 0,  0, -5],\n",
       "         [ 5,  7, -6],\n",
       "         [ 2,  2, -2],\n",
       "         [ 2, -6, -3],\n",
       "         [ 3, -7,  5],\n",
       "         [ 5,  0, -1],\n",
       "         [ 5,  0, -7],\n",
       "         [ 5, -5,  4],\n",
       "         [ 3,  1, -5],\n",
       "         [ 4, -6,  5],\n",
       "         [ 2,  7,  2],\n",
       "         [ 2,  2, -6],\n",
       "         [ 5,  3,  7],\n",
       "         [ 5,  3, -4],\n",
       "         [ 2,  0,  0]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}\n",
    "\n",
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)\n",
    "\n",
    "actions = copy.deepcopy(action_distribution.reshape(-1, 16, 3))\n",
    "actions[:, :, 1] = actions[:, :, 1] - 7\n",
    "actions[:, :, 2] = actions[:, :, 2] - 7\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([5, 1, 5], device='cuda:0')\n",
      "1 tensor([ 0,  0, -5], device='cuda:0')\n",
      "2 tensor([ 5,  7, -6], device='cuda:0')\n",
      "3 tensor([ 2,  2, -2], device='cuda:0')\n",
      "4 tensor([ 2, -6, -3], device='cuda:0')\n",
      "5 tensor([ 3, -7,  5], device='cuda:0')\n",
      "6 tensor([ 5,  0, -1], device='cuda:0')\n",
      "7 tensor([ 5,  0, -7], device='cuda:0')\n",
      "8 tensor([ 5, -5,  4], device='cuda:0')\n",
      "9 tensor([ 3,  1, -5], device='cuda:0')\n",
      "10 tensor([ 4, -6,  5], device='cuda:0')\n",
      "11 tensor([2, 7, 2], device='cuda:0')\n",
      "12 tensor([ 2,  2, -6], device='cuda:0')\n",
      "13 tensor([5, 3, 7], device='cuda:0')\n",
      "14 tensor([ 5,  3, -4], device='cuda:0')\n",
      "15 tensor([2, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(actions.squeeze()):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  8, 12,  0,  7,  2,  5, 14,  1,  2,  9,  5,  2,  1,  4,  3,  0, 12,\n",
       "          5,  7,  6,  5,  7,  0,  5,  2, 11,  3,  8,  2,  4,  1, 12,  2, 14,  9,\n",
       "          2,  9,  1,  5, 10, 14,  5, 10,  3,  2,  7,  7]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3088]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-115.3207], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3088]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([-115.3207], device='cuda:0', grad_fn=<SumBackward1>),\n",
       " tensor([115.3256], device='cuda:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.evaluate_actions(obs, action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[115.6296]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value + -log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution2, value2, log2 = model.policy(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    something = model.policy(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.policy.extract_features(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2466])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  97., -104.,  359.,  ...,   33.,    3.,    4.]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pi, latent_vf = model.policy.mlp_extractor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2595, -0.0433,  0.9991,  0.9792,  0.4206, -0.2526, -0.6704, -0.5540,\n",
       "         -0.0934, -0.9404, -0.6263,  0.8389,  0.9515, -0.8615, -0.8529, -0.4819,\n",
       "          0.5591,  0.8549, -0.4783,  0.0616, -0.1377, -0.0559, -0.2873,  0.9998,\n",
       "         -0.9815, -0.1951, -0.8463,  0.8768,  0.9118,  0.5954,  0.7028, -0.6369,\n",
       "          0.6047,  0.7684, -0.3009, -0.7373,  0.0529, -0.3780,  0.4561,  0.8869,\n",
       "         -0.9491,  0.4357,  0.9860, -0.7408,  0.6284,  0.7749,  0.5563,  0.9782,\n",
       "         -0.5009, -0.9394,  0.4296, -0.6890,  0.9836,  0.9156, -0.8435,  0.4221,\n",
       "         -0.2909,  0.0390, -0.0872,  0.4957,  0.9817,  0.9930,  0.8953, -0.8103]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5122, -0.9038, -0.9311,  0.5610, -0.6714, -0.2262,  0.7510, -0.1469,\n",
       "          0.8442,  0.9737, -0.9787, -0.9602,  0.9222,  0.7987,  0.8580, -0.1741,\n",
       "         -0.8663, -0.8598,  0.9726,  0.9065,  0.8920,  0.4981,  0.1220,  0.9883,\n",
       "          0.3116, -0.3982,  0.2209, -0.6473,  0.9317, -0.9827, -0.1555, -0.9034,\n",
       "          0.9014,  0.8704, -0.6282, -0.3511,  0.5608,  0.8297, -0.4596,  0.4936,\n",
       "         -0.7391, -0.7619,  0.9309, -0.7326,  0.9359,  0.8413, -0.6815, -0.7928,\n",
       "         -0.2650, -0.3836, -0.9805,  0.6087,  0.7684,  0.7558,  0.9632, -0.2257,\n",
       "         -0.4769, -0.3067,  0.5773,  0.7750,  0.9995,  0.9967, -0.9689,  0.9716]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.distributions.MultiCategoricalDistribution at 0x7c72b09f6ff0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = model.policy._get_action_dist_from_latent(latent_pi)\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution.get_actions().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  5,  5,  5,  2,  3,  2,  5,  7,  0,  2,  2,  6,  3,  2,  1,\n",
       "          1, 14, 14,  3,  0, 13,  0, 11,  1,  5,  3,  7,  0, 13,  8,  0,  5,  4,\n",
       "          5,  3,  3,  5, 14,  1,  5, 12,  3,  4, 12,  9]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution.get_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = distribution.log_prob(action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-115.3207], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = model.policy.value_net(latent_vf)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3088]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = distribution.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([115.3256], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.buffers import DictRolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_term_rollout_buffer = DictRolloutBuffer(1, env.observation_space, env.action_space, device=\"cuda\")\n",
    "short_term_rollout_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_term_rollout_buffer.add(obs, action_distribution, values, log_prob, entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rollout_data in short_term_rollout_buffer.get(1):\n",
    "    print(rollout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_gain_reward_func(reward_score) -> float:\n",
    "\n",
    "    return reward_score * 20 if reward_score > 0.0 else -1\n",
    "\n",
    "def match_won_reward_func(match_won) -> float:\n",
    "\n",
    "    return 5000.0 if match_won else 0.0\n",
    "\n",
    "def match_lost_reward_func(match_lost) -> float:\n",
    "\n",
    "    return -3000.0 if match_lost else 0.0\n",
    "\n",
    "def game_won_reward_func(game_won) -> float:\n",
    "\n",
    "    return 1000000.0 if game_won else 0.0\n",
    "\n",
    "def game_lost_reward_func(game_lost) -> float:\n",
    "\n",
    "    return -1000000.0 if game_lost else 0.0\n",
    "\n",
    "def map_reveal_reward_func(map_reveal_score):\n",
    "\n",
    "    return map_reveal_score * 10\n",
    "\n",
    "def attack_reward_func(actions, sap_range, enemy_unit_mask) -> float:\n",
    "\n",
    "    attack_score = 0.0\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        if action_num >= 5:\n",
    "            if enemy_unit_mask.sum() != 0:\n",
    "                sap_action_range = max(abs(dx), abs(dy))\n",
    "                if sap_action_range > sap_range:\n",
    "                    attack_score -= 0.5\n",
    "            else:\n",
    "                attack_score -= 5.0\n",
    "    \n",
    "    return attack_score\n",
    "\n",
    "def next_position_calculator(action_num, unit_positions):\n",
    "    # 0: stay, 1: up, 2: right, 3: down, 4: left\n",
    "\n",
    "    if action_num == 1:\n",
    "        next_position = (unit_positions[0], unit_positions[1] - 1)\n",
    "    elif action_num == 2:\n",
    "        next_position = (unit_positions[0] + 1, unit_positions[1])\n",
    "    elif action_num == 3:\n",
    "        next_position = (unit_positions[0], unit_positions[1] + 1)\n",
    "    elif action_num == 4:\n",
    "        next_position = (unit_positions[0] - 1, unit_positions[1])\n",
    "    else:\n",
    "        next_position = unit_positions\n",
    "    \n",
    "    return next_position\n",
    "\n",
    "def movement_reward_func(actions, obs, team_id) -> float:\n",
    "\n",
    "    movement_score = 0.0\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        unit_positions = obs[\"units\"][\"position\"][team_id][i]\n",
    "        unit_energy = obs[\"units\"][\"energy\"][team_id][i]\n",
    "\n",
    "        # give penalty if try to move unit that doesn't exist\n",
    "        if (unit_positions == (-1, -1)).sum() == 2 and action_num != 0:\n",
    "            movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if dx or dy is not 0 when not attacking\n",
    "        if action_num != 5:\n",
    "            if dx != 0 or dy != 0:\n",
    "                movement_score -= 0.25\n",
    "\n",
    "        \n",
    "        if unit_positions[0] >= 0 and unit_positions[1] >= 0:\n",
    "            # give penalty if try to move unit that has no energy\n",
    "            if unit_energy <= 0 and action_num != 0:\n",
    "                movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if try to move unit out of map\n",
    "        next_position = next_position_calculator(action_num, unit_positions)\n",
    "        if next_position[0] < 0 or next_position[1] < 0 or next_position[0] > 23 or next_position[1] > 23:\n",
    "            movement_score -= 0.5\n",
    "        else:\n",
    "            movement_score += 2.0\n",
    "    \n",
    "\n",
    "    return movement_score\n",
    "\n",
    "def relic_discovery_reward_func(relic_discovery_reward) -> float:\n",
    "\n",
    "    return relic_discovery_reward * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all[\"player_0\"][\"team_points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all[\"player_0\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all[\"player_0\"][\"units_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg = info[\"params\"]\n",
    "env_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_0,\n",
    "        model_1,\n",
    "        reward_functions: list,\n",
    "        num_games=1000,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "    ):\n",
    "        self.model_0 = model_0\n",
    "        self.model_1 = model_1\n",
    "        self.reward_functions = reward_functions\n",
    "        self.num_games = num_games\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.optimizer_0 = AdamW(self.model_0.policy.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "        self.optimizer_1 = AdamW(self.model_1.policy.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "\n",
    "        self.scheduler_0 = GreedyLR(self.optimizer_0, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "        self.scheduler_1 = GreedyLR(self.optimizer_1, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "\n",
    "        self.env = LuxAIS3GymEnv(numpy_output=True)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for game in range(1, self.num_games + 1):\n",
    "            print(\"=\"*15 + f\"Game {game} Started\" + \"=\"*15)\n",
    "\n",
    "            obs_all, info = self.env.reset()\n",
    "            self.env_cfg = info['params']\n",
    "\n",
    "            game_ended = False\n",
    "\n",
    "            player_0_previous_score = 0.0\n",
    "            player_1_previous_score = 0.0\n",
    "\n",
    "            first_spawn = False\n",
    "\n",
    "            self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "            self.map_explored_status = np.zeros((2, 24, 24), dtype=bool)\n",
    "\n",
    "            player_0_previous_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "            player_1_previous_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "            player_0_match_won_num = 0\n",
    "            player_1_match_won_num = 0\n",
    "\n",
    "            player_0_previous_relic_discovery_points = 0\n",
    "            player_1_previous_relic_discovery_points = 0\n",
    "\n",
    "            victor = None\n",
    "\n",
    "            while game_ended is not True:\n",
    "\n",
    "                player_0_current_score = obs_all['player_0']['team_points'][0]\n",
    "                player_1_current_score = obs_all['player_1']['team_points'][1]\n",
    "\n",
    "                player_0_reward_score = player_0_current_score - player_0_previous_score\n",
    "                player_1_reward_score = player_1_current_score - player_1_previous_score\n",
    "\n",
    "                player_0_previous_score = player_0_current_score\n",
    "                player_1_previous_score = player_1_current_score\n",
    "\n",
    "                current_match_step = obs_all[\"player_0\"][\"match_steps\"]\n",
    "\n",
    "                player_0_match_won = False\n",
    "                player_0_match_lost = False\n",
    "                player_1_match_won = False\n",
    "                player_1_match_lost = False\n",
    "\n",
    "                player_0_game_won = False\n",
    "                player_0_game_lost = False\n",
    "                player_1_game_won = False\n",
    "                player_1_game_lost = False\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    if player_0_current_score > player_1_current_score:\n",
    "                        player_0_match_won = True\n",
    "                        player_1_match_lost = True\n",
    "                        player_0_match_won_num += 1\n",
    "                    elif player_0_current_score < player_1_current_score:\n",
    "                        player_0_match_lost = True\n",
    "                        player_1_match_won = True\n",
    "                        player_1_match_won_num += 1\n",
    "\n",
    "                if player_0_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 0 won the game.\")\n",
    "                    victor = \"player_0\"\n",
    "                    break\n",
    "\n",
    "                if player_1_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 1 won the game.\")\n",
    "                    victor = \"player_1\"\n",
    "                    break\n",
    "\n",
    "                player_0_unit_positions = np.array(obs_all['player_0'][\"units\"][\"position\"][0])\n",
    "                player_1_unit_positions = np.array(obs_all['player_1'][\"units\"][\"position\"][1])\n",
    "\n",
    "                player_0_unit_mask = np.array(obs_all['player_0'][\"units_mask\"][0])\n",
    "                player_1_unit_mask = np.array(obs_all['player_1'][\"units_mask\"][1])\n",
    "\n",
    "                player_0_available_unit_ids = np.where(player_0_unit_mask)[0]\n",
    "                player_1_available_unit_ids = np.where(player_1_unit_mask)[0]\n",
    "\n",
    "                if player_0_available_unit_ids.shape[0] == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if first_spawn == False:\n",
    "                        player_0_first_unit_id = player_0_available_unit_ids[0]\n",
    "                        player_0_first_unit_pos = player_0_unit_positions[player_0_first_unit_id]\n",
    "                        self.spawn_location[0] = (player_0_first_unit_pos[0], player_0_first_unit_pos[1])\n",
    "                        player_1_first_unit_id = player_1_available_unit_ids[0]\n",
    "                        player_1_first_unit_pos = player_1_unit_positions[player_1_first_unit_id]\n",
    "                        self.spawn_location[1] = (player_1_first_unit_pos[0], player_1_first_unit_pos[1])\n",
    "                        first_spawn = True\n",
    "\n",
    "                player_0_map_features = obs_all['player_0']['map_features']\n",
    "                player_1_map_features = obs_all['player_1']['map_features']\n",
    "\n",
    "                player_0_current_map_tile_type = player_0_map_features['tile_type'].T\n",
    "                player_1_current_map_tile_type = player_1_map_features['tile_type'].T\n",
    "\n",
    "                self.map_explored_status[0][player_0_current_map_tile_type != -1] = True\n",
    "                self.map_explored_status[1][player_1_current_map_tile_type != -1] = True\n",
    "\n",
    "                player_0_current_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "                player_1_current_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "                player_0_map_explored_status_reward = player_0_current_map_explored_status_score - player_0_previous_map_explored_status_score\n",
    "                player_1_map_explored_status_reward = player_1_current_map_explored_status_score - player_1_previous_map_explored_status_score\n",
    "\n",
    "                player_0_previous_map_explored_status_score = player_0_current_map_explored_status_score\n",
    "                player_1_previous_map_explored_status_score = player_1_current_map_explored_status_score\n",
    "\n",
    "                ### Reward caclulation\n",
    "                player_0_relic_point_reward = point_gain_reward_func(player_0_reward_score)\n",
    "                player_1_relic_point_reward = point_gain_reward_func(player_1_reward_score)\n",
    "\n",
    "                player_0_match_won_reward = match_won_reward_func(player_0_match_won)\n",
    "                player_0_match_lost_reward = match_lost_reward_func(player_0_match_lost)\n",
    "                player_1_match_won_reward = match_won_reward_func(player_1_match_won)\n",
    "                player_1_match_lost_reward = match_lost_reward_func(player_1_match_lost)\n",
    "\n",
    "                player_0_game_won_reward = game_won_reward_func(player_0_game_won)\n",
    "                player_0_game_lost_reward = game_lost_reward_func(player_0_game_lost)\n",
    "                player_1_game_won_reward = game_won_reward_func(player_1_game_won)\n",
    "                player_1_game_lost_reward = game_lost_reward_func(player_1_game_lost)\n",
    "\n",
    "                player_0_map_reveal_reward = map_reveal_reward_func(player_0_map_explored_status_reward)\n",
    "                player_1_map_reveal_reward = map_reveal_reward_func(player_1_map_explored_status_reward)\n",
    "\n",
    "\n",
    "\n",
    "                ### model input\n",
    "                player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    player_0_action_distribution, player_0_value, player_0_log = self.model_0.forward(player_0_model_input)\n",
    "                    player_1_action_distribution, player_1_value, player_1_log = self.model_1.forward(player_1_model_input)\n",
    "\n",
    "                player_0_actions = copy.deepcopy(player_0_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_0_actions[:, 1] = player_0_actions[:, 1] - 7\n",
    "                player_0_actions[:, 2] = player_0_actions[:, 2] - 7\n",
    "                player_1_actions = copy.deepcopy(player_1_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_1_actions[:, 1] = player_1_actions[:, 1] - 7\n",
    "                player_1_actions[:, 2] = player_1_actions[:, 2] - 7\n",
    "\n",
    "                player_0_attack_reward = attack_reward_func(player_0_actions, self.env_cfg[\"unit_sap_range\"], player_1_unit_mask)\n",
    "                player_1_attack_reward = attack_reward_func(player_1_actions, self.env_cfg[\"unit_sap_range\"], player_0_unit_mask)\n",
    "\n",
    "                player_0_movement_reward = movement_reward_func(player_0_actions, obs_all[\"player_0\"], 0)\n",
    "                player_1_movement_reward = movement_reward_func(player_1_actions, obs_all[\"player_1\"], 1)\n",
    "\n",
    "                player_0_reward = player_0_relic_point_reward + player_0_match_won_reward + player_0_match_lost_reward + player_0_game_won_reward + player_0_game_lost_reward + player_0_map_reveal_reward + player_0_attack_reward + player_0_movement_reward\n",
    "                player_1_reward = player_1_relic_point_reward + player_1_match_won_reward + player_1_match_lost_reward + player_1_game_won_reward + player_1_game_lost_reward + player_1_map_reveal_reward + player_1_attack_reward + player_1_movement_reward\n",
    "\n",
    "                player_0_features = self.model_0.extract_features(player_0_model_input)\n",
    "                player_1_features = self.model_1.extract_features(player_1_model_input)\n",
    "\n",
    "                player_0_latent_pi, player_0_latent_vf = self.model_0.mlp_extractor(player_0_features)\n",
    "                player_1_latent_pi, player_1_latent_vf = self.model_1.mlp_extractor(player_1_features)\n",
    "\n",
    "                player_0_distribution = self.model_0._get_action_dist_from_latent(player_0_latent_pi)\n",
    "                player_1_distribution = self.model_1._get_action_dist_from_latent(player_1_latent_pi)\n",
    "\n",
    "                player_0_log_prob = player_0_distribution.log_prob(player_0_action_distribution)\n",
    "                player_1_log_prob = player_1_distribution.log_prob(player_1_action_distribution)\n",
    "\n",
    "                player_0_values = self.model_0.value_net(player_0_latent_vf)\n",
    "                player_1_values = self.model_1.value_net(player_1_latent_vf)\n",
    "\n",
    "                player_0_entropy = player_0_distribution.entropy()\n",
    "                player_1_entropy = player_1_distribution.entropy()\n",
    "\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def prepare_model_input(self, obs, my_team_id):\n",
    "        enemy_team_id = 1 - my_team_id\n",
    "\n",
    "        model_input = {\n",
    "            \"enemy_energies\": obs[\"units\"][\"energy\"][enemy_team_id],\n",
    "            \"enemy_positions\": obs[\"units\"][\"position\"][enemy_team_id],\n",
    "            \"enemy_spawn_location\": self.spawn_location[enemy_team_id],\n",
    "            \"enemy_visible_mask\": obs[\"units_mask\"][enemy_team_id],\n",
    "            \"map_explored_status\": self.map_explored_status[my_team_id],\n",
    "            \"map_features_energy\": obs[\"map_features\"][\"energy\"],\n",
    "            \"map_features_tile_type\": obs[\"map_features\"][\"tile_type\"],\n",
    "            \"match_steps\": np.array([obs[\"match_steps\"]]),\n",
    "            \"my_spawn_location\": self.spawn_location[my_team_id],\n",
    "            \"relic_nodes\": obs[\"relic_nodes\"],\n",
    "            \"relic_nodes_mask\": obs[\"relic_nodes_mask\"],\n",
    "            \"sensor_mask\": obs[\"sensor_mask\"],\n",
    "            \"steps\": np.array([obs[\"steps\"]]),\n",
    "            \"team_id\": np.array([my_team_id]),\n",
    "            \"team_points\": obs[\"team_points\"],\n",
    "            \"team_wins\": obs[\"team_wins\"],\n",
    "            \"unit_active_mask\": obs[\"units_mask\"][my_team_id],\n",
    "            \"unit_energies\": obs[\"units\"][\"energy\"][my_team_id],\n",
    "            \"unit_move_cost\": np.array([self.env_cfg[\"unit_move_cost\"]]),\n",
    "            \"unit_positions\": obs[\"units\"][\"position\"][my_team_id],\n",
    "            \"unit_sap_cost\": np.array([self.env_cfg[\"unit_sap_cost\"]]),\n",
    "            \"unit_sap_range\": np.array([self.env_cfg[\"unit_sap_range\"]]),\n",
    "            \"unit_sensor_range\": np.array([self.env_cfg[\"unit_sensor_range\"]]),\n",
    "        }\n",
    "\n",
    "        model_input = {k: torch.tensor(np.expand_dims(v, axis=0), dtype=torch.int32, device=\"cuda\") for k, v in model_input.items()}\n",
    "\n",
    "        return model_input\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = model.policy.extract_features(obs)\n",
    "obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}\n",
    "\n",
    "# Convert observation to tensor and check shape\n",
    "obs_tensor = model.policy.extract_features(obs)\n",
    "print(f\"Extracted Feature Shape: {obs_tensor.shape}\")  # Expected: (batch_size, 2464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor = torch.compile(model.policy.mlp_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade luxai-s3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
