{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from Modified_stablebaseline3_PPO.modified_ppo_20250228_01 import PPO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import copy\n",
    "from GreedyLRScheduler import GreedyLR\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "init_ppo = PPO(\"MultiInputPolicy\", init_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_ppo.policy\n",
    "model_1 = copy.deepcopy(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_gain_reward_func(reward_score) -> float:\n",
    "\n",
    "    return reward_score * 20 if reward_score > 0.0 else -1\n",
    "\n",
    "def match_won_reward_func(match_won) -> float:\n",
    "\n",
    "    return 5000.0 if match_won else 0.0\n",
    "\n",
    "def match_lost_reward_func(match_lost) -> float:\n",
    "\n",
    "    return -3000.0 if match_lost else 0.0\n",
    "\n",
    "def game_won_reward_func(game_won) -> float:\n",
    "\n",
    "    return 1000000000.0 if game_won else 0.0\n",
    "\n",
    "def game_lost_reward_func(game_lost) -> float:\n",
    "\n",
    "    return -1000000000.0 if game_lost else 0.0\n",
    "\n",
    "def map_reveal_reward_func(map_reveal_score):\n",
    "\n",
    "    return map_reveal_score * 10\n",
    "\n",
    "def attack_reward_func(actions, sap_range, enemy_unit_mask) -> float:\n",
    "\n",
    "    attack_score = 0.0\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        if action_num >= 5:\n",
    "            if enemy_unit_mask.sum() != 0:\n",
    "                sap_action_range = max(abs(dx), abs(dy))\n",
    "                if sap_action_range > sap_range:\n",
    "                    attack_score -= 0.5\n",
    "            else:\n",
    "                attack_score -= 5.0\n",
    "    \n",
    "    return attack_score\n",
    "\n",
    "def next_position_calculator(action_num, unit_positions):\n",
    "    # 0: stay, 1: up, 2: right, 3: down, 4: left\n",
    "\n",
    "    if action_num == 1:\n",
    "        next_position = (unit_positions[0], unit_positions[1] - 1)\n",
    "    elif action_num == 2:\n",
    "        next_position = (unit_positions[0] + 1, unit_positions[1])\n",
    "    elif action_num == 3:\n",
    "        next_position = (unit_positions[0], unit_positions[1] + 1)\n",
    "    elif action_num == 4:\n",
    "        next_position = (unit_positions[0] - 1, unit_positions[1])\n",
    "    else:\n",
    "        next_position = unit_positions\n",
    "    \n",
    "    return next_position\n",
    "\n",
    "def movement_reward_func(actions, obs, team_id) -> float:\n",
    "\n",
    "    movement_score = 0.0\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        unit_positions = obs[\"units\"][\"position\"][team_id][i]\n",
    "        unit_energy = obs[\"units\"][\"energy\"][team_id][i]\n",
    "\n",
    "        # give penalty if try to move unit that doesn't exist\n",
    "        if (unit_positions == (-1, -1)).sum() == 2 and action_num != 0:\n",
    "            movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if dx or dy is not 0 when not attacking\n",
    "        if action_num != 5:\n",
    "            if dx != 0 or dy != 0:\n",
    "                movement_score -= 0.25\n",
    "\n",
    "        \n",
    "        if unit_positions[0] >= 0 and unit_positions[1] >= 0:\n",
    "            # give penalty if try to move unit that has no energy\n",
    "            if unit_energy <= 0 and action_num != 0:\n",
    "                movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if try to move unit out of map\n",
    "        next_position = next_position_calculator(action_num, unit_positions)\n",
    "        if next_position[0] < 0 or next_position[1] < 0 or next_position[0] > 23 or next_position[1] > 23:\n",
    "            movement_score -= 0.5\n",
    "        else:\n",
    "            movement_score += 2.0\n",
    "    \n",
    "\n",
    "    return movement_score\n",
    "\n",
    "def relic_discovery_reward_func(relic_discovery_reward) -> float:\n",
    "\n",
    "    return relic_discovery_reward * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_0,\n",
    "        model_1,\n",
    "        num_games=1000,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=None,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "    ):\n",
    "        self.model_0 = model_0\n",
    "        self.model_1 = model_1\n",
    "        self.num_games = num_games\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.optimizer_0 = AdamW(self.model_0.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "        self.optimizer_1 = AdamW(self.model_1.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "\n",
    "        self.scheduler_0 = GreedyLR(self.optimizer_0, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "        self.scheduler_1 = GreedyLR(self.optimizer_1, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "\n",
    "        self.env = LuxAIS3GymEnv(numpy_output=True)\n",
    "\n",
    "        self.model_0.mlp_extractor = torch.compile(self.model_0.mlp_extractor)\n",
    "        self.model_1.mlp_extractor = torch.compile(self.model_1.mlp_extractor)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for game in range(1, self.num_games + 1):\n",
    "            print(\"=\"*15 + f\" Game {game} Started \" + \"=\"*15)\n",
    "\n",
    "            obs_all, info = self.env.reset()\n",
    "            self.env_cfg = info['params']\n",
    "\n",
    "            game_ended = False\n",
    "\n",
    "            player_0_previous_score = 0.0\n",
    "            player_1_previous_score = 0.0\n",
    "\n",
    "            first_spawn = False\n",
    "\n",
    "            self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "            self.map_explored_status = np.zeros((2, 24, 24), dtype=bool)\n",
    "\n",
    "            player_0_previous_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "            player_1_previous_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "            player_0_match_won_num = 0\n",
    "            player_1_match_won_num = 0\n",
    "\n",
    "            player_0_previous_relic_discovery_points = 0\n",
    "            player_1_previous_relic_discovery_points = 0\n",
    "\n",
    "            victor = None\n",
    "\n",
    "            game_start = True\n",
    "\n",
    "            match_number = 1\n",
    "\n",
    "            while game_ended is not True:\n",
    "\n",
    "                player_0_match_won = False\n",
    "                player_0_match_lost = False\n",
    "                player_1_match_won = False\n",
    "                player_1_match_lost = False\n",
    "\n",
    "                player_0_game_won = False\n",
    "                player_0_game_lost = False\n",
    "                player_1_game_won = False\n",
    "                player_1_game_lost = False\n",
    "\n",
    "                player_0_current_score = obs_all['player_0']['team_points'][0]\n",
    "                player_1_current_score = obs_all['player_1']['team_points'][1]\n",
    "\n",
    "                player_0_reward_score = player_0_current_score - player_0_previous_score\n",
    "                player_1_reward_score = player_1_current_score - player_1_previous_score\n",
    "\n",
    "                player_0_previous_score = player_0_current_score\n",
    "                player_1_previous_score = player_1_current_score\n",
    "\n",
    "                current_match_step = obs_all[\"player_0\"][\"match_steps\"]\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    if player_0_current_score > player_1_current_score:\n",
    "                        player_0_match_won = True\n",
    "                        player_1_match_lost = True\n",
    "                        player_0_match_won_num += 1\n",
    "                    elif player_0_current_score < player_1_current_score:\n",
    "                        player_0_match_lost = True\n",
    "                        player_1_match_won = True\n",
    "                        player_1_match_won_num += 1\n",
    "\n",
    "                if player_0_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 0 won the game.\")\n",
    "                    victor = \"player_0\"\n",
    "                    player_0_game_won = True\n",
    "                    player_1_game_lost = True\n",
    "\n",
    "                if player_1_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 1 won the game.\")\n",
    "                    victor = \"player_1\"\n",
    "                    player_0_game_lost = True\n",
    "                    player_1_game_won = True\n",
    "\n",
    "                player_0_unit_positions = np.array(obs_all['player_0'][\"units\"][\"position\"][0])\n",
    "                player_1_unit_positions = np.array(obs_all['player_1'][\"units\"][\"position\"][1])\n",
    "\n",
    "                player_0_unit_mask = np.array(obs_all['player_0'][\"units_mask\"][0])\n",
    "                player_1_unit_mask = np.array(obs_all['player_1'][\"units_mask\"][1])\n",
    "\n",
    "                player_0_available_unit_ids = np.where(player_0_unit_mask)[0]\n",
    "                player_1_available_unit_ids = np.where(player_1_unit_mask)[0]\n",
    "\n",
    "                if player_0_available_unit_ids.shape[0] == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if first_spawn == False:\n",
    "                        player_0_first_unit_id = player_0_available_unit_ids[0]\n",
    "                        player_0_first_unit_pos = player_0_unit_positions[player_0_first_unit_id]\n",
    "                        self.spawn_location[0] = (player_0_first_unit_pos[0], player_0_first_unit_pos[1])\n",
    "                        player_1_first_unit_id = player_1_available_unit_ids[0]\n",
    "                        player_1_first_unit_pos = player_1_unit_positions[player_1_first_unit_id]\n",
    "                        self.spawn_location[1] = (player_1_first_unit_pos[0], player_1_first_unit_pos[1])\n",
    "                        first_spawn = True\n",
    "\n",
    "                player_0_map_features = obs_all['player_0']['map_features']\n",
    "                player_1_map_features = obs_all['player_1']['map_features']\n",
    "\n",
    "                player_0_current_map_tile_type = player_0_map_features['tile_type'].T\n",
    "                player_1_current_map_tile_type = player_1_map_features['tile_type'].T\n",
    "\n",
    "                self.map_explored_status[0][player_0_current_map_tile_type != -1] = True\n",
    "                self.map_explored_status[1][player_1_current_map_tile_type != -1] = True\n",
    "\n",
    "                player_0_current_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "                player_1_current_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "                player_0_map_explored_status_reward = player_0_current_map_explored_status_score - player_0_previous_map_explored_status_score\n",
    "                player_1_map_explored_status_reward = player_1_current_map_explored_status_score - player_1_previous_map_explored_status_score\n",
    "\n",
    "                player_0_previous_map_explored_status_score = player_0_current_map_explored_status_score\n",
    "                player_1_previous_map_explored_status_score = player_1_current_map_explored_status_score\n",
    "\n",
    "                ### Reward caclulation\n",
    "                player_0_relic_point_reward = point_gain_reward_func(player_0_reward_score)\n",
    "                player_1_relic_point_reward = point_gain_reward_func(player_1_reward_score)\n",
    "\n",
    "                player_0_match_won_reward = match_won_reward_func(player_0_match_won)\n",
    "                player_0_match_lost_reward = match_lost_reward_func(player_0_match_lost)\n",
    "                player_1_match_won_reward = match_won_reward_func(player_1_match_won)\n",
    "                player_1_match_lost_reward = match_lost_reward_func(player_1_match_lost)\n",
    "\n",
    "                player_0_game_won_reward = game_won_reward_func(player_0_game_won)\n",
    "                player_0_game_lost_reward = game_lost_reward_func(player_0_game_lost)\n",
    "                player_1_game_won_reward = game_won_reward_func(player_1_game_won)\n",
    "                player_1_game_lost_reward = game_lost_reward_func(player_1_game_lost)\n",
    "\n",
    "                player_0_map_reveal_reward = map_reveal_reward_func(player_0_map_explored_status_reward)\n",
    "                player_1_map_reveal_reward = map_reveal_reward_func(player_1_map_explored_status_reward)\n",
    "\n",
    "                ### model input\n",
    "                if game_start == True:\n",
    "                    player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                    player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "                    game_start = False\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    player_0_action_distribution, _, _ = self.model_0(player_0_model_input)\n",
    "                    player_1_action_distribution, _, _ = self.model_1(player_1_model_input)\n",
    "\n",
    "                player_0_action = copy.deepcopy(player_0_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_0_action[:, 1] = player_0_action[:, 1] - 7\n",
    "                player_0_action[:, 2] = player_0_action[:, 2] - 7\n",
    "                player_1_action = copy.deepcopy(player_1_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_1_action[:, 1] = player_1_action[:, 1] - 7\n",
    "                player_1_action[:, 2] = player_1_action[:, 2] - 7\n",
    "\n",
    "                print(player_0_action)\n",
    "                print(obs_all[\"player_0\"][\"map_features\"][\"tile_type\"].T)\n",
    "\n",
    "                player_0_attack_reward = attack_reward_func(player_0_action, self.env_cfg[\"unit_sap_range\"], player_1_unit_mask)\n",
    "                player_1_attack_reward = attack_reward_func(player_1_action, self.env_cfg[\"unit_sap_range\"], player_0_unit_mask)\n",
    "\n",
    "                player_0_movement_reward = movement_reward_func(player_0_action, obs_all[\"player_0\"], 0)\n",
    "                player_1_movement_reward = movement_reward_func(player_1_action, obs_all[\"player_1\"], 1)\n",
    "\n",
    "                player_0_reward = player_0_relic_point_reward + player_0_match_won_reward + player_0_match_lost_reward + player_0_game_won_reward + player_0_game_lost_reward + player_0_map_reveal_reward + player_0_attack_reward + player_0_movement_reward\n",
    "                player_1_reward = player_1_relic_point_reward + player_1_match_won_reward + player_1_match_lost_reward + player_1_game_won_reward + player_1_game_lost_reward + player_1_map_reveal_reward + player_1_attack_reward + player_1_movement_reward\n",
    "                # player_0_reward = torch.tensor(player_0_reward, dtype=torch.float32, device=\"cuda\")\n",
    "                # player_1_reward = torch.tensor(player_1_reward, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "                player_0_features = self.model_0.extract_features(player_0_model_input)\n",
    "                player_1_features = self.model_1.extract_features(player_1_model_input)\n",
    "\n",
    "                player_0_latent_pi, player_0_latent_vf = self.model_0.mlp_extractor(player_0_features)\n",
    "                player_1_latent_pi, player_1_latent_vf = self.model_1.mlp_extractor(player_1_features)\n",
    "\n",
    "                player_0_distribution = self.model_0._get_action_dist_from_latent(player_0_latent_pi)\n",
    "                player_1_distribution = self.model_1._get_action_dist_from_latent(player_1_latent_pi)\n",
    "\n",
    "                player_0_log_prob = player_0_distribution.log_prob(player_0_action_distribution)\n",
    "                player_1_log_prob = player_1_distribution.log_prob(player_1_action_distribution)\n",
    "\n",
    "                player_0_value = self.model_0.value_net(player_0_latent_vf)\n",
    "                player_1_value = self.model_1.value_net(player_1_latent_vf)\n",
    "\n",
    "                player_0_entropy = player_0_distribution.entropy()\n",
    "                player_1_entropy = player_1_distribution.entropy()\n",
    "\n",
    "                obs_all, _, _, _, _ = self.env.step({\n",
    "                    \"player_0\": player_0_action.detach(),\n",
    "                    \"player_1\": player_1_action.detach()\n",
    "                })\n",
    "\n",
    "                player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Compute value for the last timestep\n",
    "                    player_0_new_value = self.model_0.predict_values(player_0_model_input)  # type: ignore[arg-type]\n",
    "                    player_1_new_value = self.model_1.predict_values(player_1_model_input)\n",
    "\n",
    "                # player_0_delta = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                # player_0_advantage = player_0_delta + self.gamma * self.gae_lambda\n",
    "                player_0_advantage = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                player_0_advantage = player_0_advantage.detach()\n",
    "                # player_0_advantage = torch.tensor(player_0_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_0_return = player_0_advantage + player_0_value\n",
    "\n",
    "                # player_1_delta = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                # player_1_advantage = player_1_delta + self.gamma * self.gae_lambda\n",
    "                player_1_advantage = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                player_1_advantage = player_1_advantage.detach()\n",
    "                # player_1_advantage = torch.tensor(player_1_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_1_return = player_1_advantage + player_1_value\n",
    "\n",
    "                player_0_policy_loss_1 = player_0_advantage\n",
    "                player_0_policy_loss_2 = player_0_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_0_policy_loss = -torch.min(player_0_policy_loss_1, player_0_policy_loss_2).mean()\n",
    "\n",
    "                player_1_policy_loss_1 = player_1_advantage\n",
    "                player_1_policy_loss_2 = player_1_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_1_policy_loss = -torch.min(player_1_policy_loss_1, player_1_policy_loss_2).mean()\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    player_0_values_pred = player_0_new_value\n",
    "                    player_1_values_pred = player_1_new_value\n",
    "                else:\n",
    "                    player_0_values_pred = player_0_value + torch.clamp(player_0_new_value - player_0_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "                    player_1_values_pred = player_1_value + torch.clamp(player_1_new_value - player_1_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "\n",
    "                player_0_value_loss = F.mse_loss(player_0_return, player_0_values_pred)\n",
    "                player_1_value_loss = F.mse_loss(player_1_return, player_1_values_pred)\n",
    "\n",
    "                player_0_entropy_loss = -torch.mean(-player_0_entropy)\n",
    "                player_1_entropy_loss = -torch.mean(-player_1_entropy)\n",
    "\n",
    "                player_0_loss = player_0_policy_loss + self.ent_coef * player_0_entropy_loss + self.vf_coef * player_0_value_loss\n",
    "                player_1_loss = player_1_policy_loss + self.ent_coef * player_1_entropy_loss + self.vf_coef * player_1_value_loss\n",
    "\n",
    "                self.optimizer_0.zero_grad()\n",
    "                player_0_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_0.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_0.step()\n",
    "                self.scheduler_0.step(player_0_loss.item())\n",
    "\n",
    "                self.optimizer_1.zero_grad()\n",
    "                player_1_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_1.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_1.step()\n",
    "                self.scheduler_1.step(player_1_loss.item())\n",
    "\n",
    "                if match_number >= 5 and current_match_step == 100:\n",
    "                    game_ended = True\n",
    "                    print(\"Game ended.\")\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    match_number += 1\n",
    "\n",
    "            if victor == \"player_0\":\n",
    "                self.synchronize_models(self.model_0, self.model_1)\n",
    "            elif victor == \"player_1\":\n",
    "                self.synchronize_models(self.model_1, self.model_0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "                \n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def prepare_model_input(self, obs, my_team_id):\n",
    "        enemy_team_id = 1 - my_team_id\n",
    "\n",
    "        model_input = {\n",
    "            \"enemy_energies\": obs[\"units\"][\"energy\"][enemy_team_id],\n",
    "            \"enemy_positions\": obs[\"units\"][\"position\"][enemy_team_id],\n",
    "            \"enemy_spawn_location\": self.spawn_location[enemy_team_id],\n",
    "            \"enemy_visible_mask\": obs[\"units_mask\"][enemy_team_id],\n",
    "            \"map_explored_status\": self.map_explored_status[my_team_id],\n",
    "            \"map_features_energy\": obs[\"map_features\"][\"energy\"],\n",
    "            \"map_features_tile_type\": obs[\"map_features\"][\"tile_type\"],\n",
    "            \"match_steps\": np.array([obs[\"match_steps\"]]),\n",
    "            \"my_spawn_location\": self.spawn_location[my_team_id],\n",
    "            \"relic_nodes\": obs[\"relic_nodes\"],\n",
    "            \"relic_nodes_mask\": obs[\"relic_nodes_mask\"],\n",
    "            \"sensor_mask\": obs[\"sensor_mask\"],\n",
    "            \"steps\": np.array([obs[\"steps\"]]),\n",
    "            \"team_id\": np.array([my_team_id]),\n",
    "            \"team_points\": obs[\"team_points\"],\n",
    "            \"team_wins\": obs[\"team_wins\"],\n",
    "            \"unit_active_mask\": obs[\"units_mask\"][my_team_id],\n",
    "            \"unit_energies\": obs[\"units\"][\"energy\"][my_team_id],\n",
    "            \"unit_move_cost\": np.array([self.env_cfg[\"unit_move_cost\"]]),\n",
    "            \"unit_positions\": obs[\"units\"][\"position\"][my_team_id],\n",
    "            \"unit_sap_cost\": np.array([self.env_cfg[\"unit_sap_cost\"]]),\n",
    "            \"unit_sap_range\": np.array([self.env_cfg[\"unit_sap_range\"]]),\n",
    "            \"unit_sensor_range\": np.array([self.env_cfg[\"unit_sensor_range\"]]),\n",
    "        }\n",
    "\n",
    "        model_input = {k: torch.tensor(np.expand_dims(v, axis=0), dtype=torch.int32, device=\"cuda\") for k, v in model_input.items()}\n",
    "\n",
    "        return model_input\n",
    "    \n",
    "    def synchronize_models(self, winner_model, loser_model):\n",
    "        with torch.no_grad():\n",
    "            for p1, p2 in zip(winner_model.parameters(), loser_model.parameters()):\n",
    "                p2.data.copy_(p1.data)\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainPPO(model_0, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = model.policy.extract_features(obs)\n",
    "obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}\n",
    "\n",
    "# Convert observation to tensor and check shape\n",
    "obs_tensor = model.policy.extract_features(obs)\n",
    "print(f\"Extracted Feature Shape: {obs_tensor.shape}\")  # Expected: (batch_size, 2464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor = torch.compile(model.policy.mlp_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade luxai-s3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
