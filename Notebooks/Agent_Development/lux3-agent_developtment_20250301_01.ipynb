{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "import os\n",
    "# import copy\n",
    "# from GreedyLRScheduler import GreedyLR\n",
    "# from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gc\n",
    "gc.enable()\n",
    "# from stable_baselines3.common.buffers import DictRolloutBuffer\n",
    "# from tqdm.notebook import tqdm\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-03 20:43:50,310:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=5e-5, ent_coef=0.015, vf_coef=0.75, clip_range_vf=0.15, n_steps=505, batch_size=505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiInputActorCriticPolicy(\n",
       "  (features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): CombinedExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (enemy_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (enemy_visible_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_explored_status): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_energy): Flatten(start_dim=1, end_dim=-1)\n",
       "      (map_features_tile_type): Flatten(start_dim=1, end_dim=-1)\n",
       "      (match_steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (my_spawn_location): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes): Flatten(start_dim=1, end_dim=-1)\n",
       "      (relic_nodes_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (sensor_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (steps): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_id): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_points): Flatten(start_dim=1, end_dim=-1)\n",
       "      (team_wins): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_active_mask): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_energies): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_move_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_positions): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_cost): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sap_range): Flatten(start_dim=1, end_dim=-1)\n",
       "      (unit_sensor_range): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): OptimizedModule(\n",
       "    (_orig_mod): MlpExtractor(\n",
       "      (policy_net): Sequential(\n",
       "        (0): Linear(in_features=2465, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): Tanh()\n",
       "      )\n",
       "      (value_net): Sequential(\n",
       "        (0): Linear(in_features=2465, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (3): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=576, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor that:\n",
    "    - Processes 24x24 grid features using CNN.\n",
    "    - Flattens and concatenates other features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Dict, features_dim: int = 512):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "\n",
    "        # Identify 24x24 grid features\n",
    "        self.grid_features = [\"map_explored_status\", \"map_features_energy\", \"map_features_tile_type\", \"sensor_mask\"]\n",
    "\n",
    "        # Identify 1D and 2D features (excluding grid)\n",
    "        self.scalar_features = []\n",
    "        self.vector_features = []\n",
    "        for key, space in observation_space.spaces.items():\n",
    "            if key in self.grid_features:\n",
    "                continue  # Grid features are processed separately\n",
    "            elif space.shape == ():  # Scalar value (e.g., team_id)\n",
    "                self.scalar_features.append(key)\n",
    "            elif len(space.shape) == 1:  # 1D vector (e.g., enemy_energies)\n",
    "                self.vector_features.append(key)\n",
    "            elif len(space.shape) == 2:  # 2D tensor (e.g., enemy_positions)\n",
    "                self.vector_features.append(key)  # Flattened separately\n",
    "\n",
    "        # **CNN for 24x24 Grid Features** (Expects input shape [batch, channels, 24, 24])\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(len(self.grid_features), 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # **Conv3D for 24x24 Grid Features** \n",
    "        # Input shape: [batch, depth=1, channels=4, height=24, width=24]\n",
    "        self.conv3d_extractor = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1)),  # 16 channels\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv3d(16, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1)),  # 32 channels\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute CNN output dimension (using a dummy input)\n",
    "        dummy_input = torch.zeros((1, len(self.grid_features), 24, 24))\n",
    "        cnn_output_dim = self.cnn_extractor(dummy_input).shape[1]\n",
    "\n",
    "        # Compute CNN output dimensions\n",
    "        # dummy_input_3d = torch.zeros((1, 1, len(self.grid_features), 24, 24))\n",
    "        cnn_output_3d_dim = self.conv3d_extractor(dummy_input.unsqueeze(1)).shape[1]\n",
    "\n",
    "        # **Flatten layers for non-grid features**\n",
    "        self.extractors = nn.ModuleDict()\n",
    "        vector_dim = 0\n",
    "\n",
    "        for key in self.vector_features:\n",
    "            space_shape = observation_space.spaces[key].shape\n",
    "            self.extractors[key] = nn.Flatten()\n",
    "            vector_dim += torch.prod(torch.tensor(space_shape)).item()\n",
    "\n",
    "        # Scalar features are just concatenated directly\n",
    "        scalar_dim = len(self.scalar_features)\n",
    "\n",
    "        # Compute total feature dimension\n",
    "        self._features_dim = cnn_output_dim + cnn_output_3d_dim + vector_dim + scalar_dim\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        - Grid features go through CNN\n",
    "        - Other features are flattened\n",
    "        - Both are concatenated into a single tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Stack grid features into CNN input\n",
    "        # print(observations.keys())\n",
    "        # print(observations[\"map_explored_status\"])\n",
    "        # print(observations.items())\n",
    "        # for key, value in observations.items():\n",
    "        #     observations[key] = torch.tensor(value, dtype=torch.float32)\n",
    "        grid_stack = torch.stack([observations[key] for key in self.grid_features], dim=1).float()\n",
    "        grid_features = self.cnn_extractor(grid_stack)\n",
    "        grid_features_3d = self.conv3d_extractor(grid_stack.unsqueeze(1))\n",
    "\n",
    "        # Flatten vector features\n",
    "        vector_features = torch.cat([self.extractors[key](observations[key]) for key in self.vector_features], dim=1)\n",
    "\n",
    "        # Concatenate scalar features\n",
    "        # scalar_features = torch.cat([observations[key].unsqueeze(1) for key in self.scalar_features], dim=1)\n",
    "        # [print(observations[key], key) for key in self.scalar_features]\n",
    "        # scalar_features = torch.cat([observations[key] for key in self.scalar_features], dim=0)\n",
    "\n",
    "        # print(grid_features.shape, grid_features_3d.shape, vector_features.shape, scalar_features.shape)\n",
    "\n",
    "        # Combine everything into one feature vector\n",
    "        # return torch.cat([grid_features, grid_features_3d, vector_features, scalar_features], dim=1)\n",
    "        return torch.cat([grid_features, grid_features_3d, vector_features], dim=1)\n",
    "    \n",
    "\n",
    "class CustomMlpExtractor(nn.ModuleDict):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        reduced_dim = 1024  # Reduce 92,321 → 1024\n",
    "        self.feature_reduction = nn.Sequential(\n",
    "            nn.Linear(feature_dim, reduced_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(reduced_dim),  # **LayerNorm for stability**\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(reduced_dim, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(reduced_dim, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        latent_pi = self.policy_net(x)\n",
    "        latent_vf = self.value_net(x)\n",
    "        return latent_pi, latent_vf\n",
    "\n",
    "    def forward_actor(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        return self.policy_net(x)\n",
    "\n",
    "    def forward_critic(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        return self.value_net(x)\n",
    "\n",
    "\n",
    "class CustomMultiInputPolicy(MultiInputActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom MultiInput Policy that:\n",
    "    - Uses CNN for spatial features.\n",
    "    - Uses MLP for non-spatial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomMultiInputPolicy, self).__init__(\n",
    "            *args, **kwargs,\n",
    "            features_extractor_class=CustomFeatureExtractor,\n",
    "            # features_extractor_kwargs={\"features_dim\": 512}  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        # print(self.features_extractor._features_dim)\n",
    "\n",
    "        # **Feature Reduction Layer**\n",
    "        # reduced_dim = 1024  # Reduce 92,321 → 1024\n",
    "        # self.feature_reduction = nn.Sequential(\n",
    "        #     nn.Linear(self.features_extractor._features_dim, reduced_dim),\n",
    "        #     nn.SiLU(),\n",
    "        #     nn.LayerNorm(reduced_dim),  # **LayerNorm for stability**\n",
    "        #     nn.Dropout(0.2)\n",
    "        # )\n",
    "\n",
    "        # **Improved MLP Extractor (3 layers)**\n",
    "        # self.mlp_extractor = nn.ModuleDict(\n",
    "        #     {\n",
    "        #         \"policy_net\": nn.Sequential(\n",
    "        #             nn.Linear(reduced_dim, 512),\n",
    "        #             nn.SiLU(),\n",
    "        #             nn.Dropout(0.2),\n",
    "        #             nn.Linear(512, 128),\n",
    "        #             nn.SiLU(),\n",
    "        #             nn.Dropout(0.2),\n",
    "        #             nn.Linear(128, 64),\n",
    "        #             nn.SiLU()\n",
    "        #         ),\n",
    "        #         \"value_net\": nn.Sequential(\n",
    "        #             nn.Linear(reduced_dim, 512),\n",
    "        #             nn.SiLU(),\n",
    "        #             nn.Dropout(0.2),\n",
    "        #             nn.Linear(512, 128),\n",
    "        #             nn.SiLU(),\n",
    "        #             nn.Dropout(0.2),\n",
    "        #             nn.Linear(128, 64),\n",
    "        #             nn.SiLU()\n",
    "        #         )\n",
    "        #     }\n",
    "        # )\n",
    "        self.mlp_extractor = CustomMlpExtractor(self.features_extractor._features_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.action_net = nn.Linear(64, 16*6 + 16*2*15)  # Adjust for action space\n",
    "        self.value_net = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, obs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass through the policy.\n",
    "        \"\"\"\n",
    "        features = self.features_extractor(obs)\n",
    "        # reduced_features = self.mlp_extractor.feature_reduction(features)\n",
    "        # print(\"Extracted Features Shape:\", features.shape)  # Debugging\n",
    "        # reduced_features = self.feature_reduction(features)\n",
    "        # print(\"Reduced Features Shape:\", reduced_features.shape)  # Debugging\n",
    "        # policy_features = self.mlp_extractor[\"policy_net\"](reduced_features)\n",
    "        # value_features = self.mlp_extractor[\"value_net\"](reduced_features)\n",
    "        policy_features = self.mlp_extractor.forward_actor(features)\n",
    "        value_features = self.mlp_extractor.forward_critic(features)\n",
    "\n",
    "        # Get logits for discrete action space\n",
    "        logits = self.action_net(policy_features)\n",
    "\n",
    "        action_logits = logits[:, :16*6].view(-1, 16, 6)\n",
    "        dxdy_logits = logits[:, 16*6:].view(-1, 16, 2, 15)\n",
    "        dx_logits = dxdy_logits[:, :, 0, :]\n",
    "        dy_logits = dxdy_logits[:, :, 1, :]\n",
    "\n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        dx_probs = F.softmax(dx_logits, dim=-1)\n",
    "        dy_probs = F.softmax(dy_logits, dim=-1)\n",
    "\n",
    "        actions_dist = torch.distributions.Categorical(probs=action_probs)\n",
    "        dx_dist = torch.distributions.Categorical(probs=dx_probs)\n",
    "        dy_dist = torch.distributions.Categorical(probs=dy_probs)\n",
    "\n",
    "        actions = actions_dist.sample()\n",
    "        dx = dx_dist.sample()\n",
    "        dy = dy_dist.sample()\n",
    "\n",
    "        zeros = torch.zeros((actions.shape[0], 16, 3), dtype=actions.dtype, device=actions.device)\n",
    "        zeros[:, :, 0] = actions\n",
    "        sap_mask = zeros == 5\n",
    "        sap_mask_dxdy = sap_mask[:, :, 0]\n",
    "        batch_idx, unit_idx = sap_mask_dxdy.nonzero(as_tuple=True)\n",
    "\n",
    "        zeros[batch_idx, unit_idx, 1] = dx[batch_idx, unit_idx]\n",
    "        zeros[batch_idx, unit_idx, 2] = dy[batch_idx, unit_idx]\n",
    "\n",
    "        # ---- Compute log_probs ----\n",
    "        actions_log_probs = actions_dist.log_prob(actions)  # (batch_size, 16)\n",
    "        dx_log_probs = dx_dist.log_prob(dx)        # (batch_size, 16)\n",
    "        dy_log_probs = dy_dist.log_prob(dy)        # (batch_size, 16)\n",
    "\n",
    "        # Apply SAP mask to sum only dx/dy log_probs where action == 5\n",
    "        dxdy_log_probs = torch.zeros_like(actions_log_probs)  # Initialize to zeros\n",
    "        dxdy_log_probs[batch_idx, unit_idx] = dx_log_probs[batch_idx, unit_idx] + dy_log_probs[batch_idx, unit_idx]\n",
    "\n",
    "        total_log_probs = actions_log_probs + dxdy_log_probs  # Final log probability per unit\n",
    "\n",
    "        return zeros, self.value_net(value_features), total_log_probs.sum(dim=-1)\n",
    "    \n",
    "    # def predict_values(self, obs):\n",
    "    #     \"\"\"\n",
    "    #     Ensure the correct feature transformation is applied before predicting values.\n",
    "    #     \"\"\"\n",
    "    #     features = self.features_extractor(obs)\n",
    "    #     reduced_features = self.feature_reduction(features)  # Ensure reduced size\n",
    "    #     return self.value_net(self.mlp_extractor.forward_critic(reduced_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy = CustomMultiInputPolicy(model.observation_space, model.action_space, model.lr_schedule).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy = torch.compile(custom_policy, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy_2 = CustomMultiInputPolicy(model.observation_space, model.action_space, model.lr_schedule).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy_2 = torch.compile(custom_policy_2, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy = custom_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy_2 = custom_policy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 503/10000 [00:14<01:34, 100.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 28  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 17  |\n",
      "|    total_timesteps | 505 |\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1010/10000 [00:21<01:32, 97.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.74817 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.00995  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | 0.184    |\n",
      "|    value_loss           | 0.0165   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.74071 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.00657 |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 10       |\n",
      "|    policy_gradient_loss | 0.187    |\n",
      "|    value_loss           | 0.0184   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 41       |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 24       |\n",
      "|    total_timesteps      | 1010     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1514/10000 [00:27<01:24, 99.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.590164 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0376   |\n",
      "|    loss                 | -1.54     |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.18      |\n",
      "|    value_loss           | 0.0165    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 72.135956 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.00846  |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.185     |\n",
      "|    value_loss           | 0.0184    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 49        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 30        |\n",
      "|    total_timesteps      | 1515      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2018/10000 [00:33<01:20, 99.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.412   |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0429   |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | 0.186    |\n",
      "|    value_loss           | 0.0182   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.55161 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0205   |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | 0.182    |\n",
      "|    value_loss           | 0.0162   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 55       |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 36       |\n",
      "|    total_timesteps      | 2020     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2519/10000 [00:39<01:17, 96.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.84091 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.00721  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | 0.186    |\n",
      "|    value_loss           | 0.0185   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 70.76323 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.00877 |\n",
      "|    loss                 | -1.54    |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | 0.18     |\n",
      "|    value_loss           | 0.016    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 59       |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 42       |\n",
      "|    total_timesteps      | 2525     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3025/10000 [00:45<01:09, 99.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.35313 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0366   |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 50       |\n",
      "|    policy_gradient_loss | 0.182    |\n",
      "|    value_loss           | 0.0183   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 72.0317  |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0241   |\n",
      "|    loss                 | -1.54    |\n",
      "|    n_updates            | 50       |\n",
      "|    policy_gradient_loss | 0.18     |\n",
      "|    value_loss           | 0.0152   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 62       |\n",
      "|    iterations           | 6        |\n",
      "|    time_elapsed         | 48       |\n",
      "|    total_timesteps      | 3030     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3533/10000 [00:51<01:05, 99.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.512634 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.187     |\n",
      "|    value_loss           | 0.018     |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 72.22669  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.00906   |\n",
      "|    loss                 | -1.54     |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | 0.181     |\n",
      "|    value_loss           | 0.0162    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 64        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 54        |\n",
      "|    total_timesteps      | 3535      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4037/10000 [00:57<01:01, 96.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 71.89127 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0513   |\n",
      "|    loss                 | -1.52    |\n",
      "|    n_updates            | 70       |\n",
      "|    policy_gradient_loss | 0.191    |\n",
      "|    value_loss           | 0.0183   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.30463 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0237   |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 70       |\n",
      "|    policy_gradient_loss | 0.184    |\n",
      "|    value_loss           | 0.0161   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 66       |\n",
      "|    iterations           | 8        |\n",
      "|    time_elapsed         | 60       |\n",
      "|    total_timesteps      | 4040     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4540/10000 [01:03<00:55, 98.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.378876 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0624   |\n",
      "|    loss                 | -1.54     |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.176     |\n",
      "|    value_loss           | 0.0165    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.71397  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.000219  |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.187     |\n",
      "|    value_loss           | 0.0193    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 68        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 66        |\n",
      "|    total_timesteps      | 4545      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5042/10000 [01:09<00:50, 97.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.968155 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | 0.186     |\n",
      "|    value_loss           | 0.018     |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.02992  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.00833   |\n",
      "|    loss                 | -1.54     |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | 0.18      |\n",
      "|    value_loss           | 0.0163    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 72        |\n",
      "|    total_timesteps      | 5050      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5552/10000 [01:15<00:42, 103.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.133064 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0624    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.184     |\n",
      "|    value_loss           | 0.018     |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 72.12213  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0422    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.182     |\n",
      "|    value_loss           | 0.0161    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 78        |\n",
      "|    total_timesteps      | 5555      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6058/10000 [01:21<00:38, 102.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.47327 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0155  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 110      |\n",
      "|    policy_gradient_loss | 0.185    |\n",
      "|    value_loss           | 0.0164   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.28944 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.0164   |\n",
      "|    loss                 | -1.52    |\n",
      "|    n_updates            | 110      |\n",
      "|    policy_gradient_loss | 0.191    |\n",
      "|    value_loss           | 0.0188   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 71       |\n",
      "|    iterations           | 12       |\n",
      "|    time_elapsed         | 84       |\n",
      "|    total_timesteps      | 6060     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6557/10000 [01:27<00:35, 96.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.63625 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.00413 |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 120      |\n",
      "|    policy_gradient_loss | 0.181    |\n",
      "|    value_loss           | 0.0164   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 72.10843 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.00101  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 120      |\n",
      "|    policy_gradient_loss | 0.188    |\n",
      "|    value_loss           | 0.0185   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 72       |\n",
      "|    iterations           | 13       |\n",
      "|    time_elapsed         | 89       |\n",
      "|    total_timesteps      | 6565     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7069/10000 [01:33<00:28, 102.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.207695 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0251   |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 130       |\n",
      "|    policy_gradient_loss | 0.182     |\n",
      "|    value_loss           | 0.0165    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.49586  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0649   |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 130       |\n",
      "|    policy_gradient_loss | 0.187     |\n",
      "|    value_loss           | 0.0196    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 95        |\n",
      "|    total_timesteps      | 7070      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7574/10000 [01:39<00:25, 95.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.51657 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0308  |\n",
      "|    loss                 | -1.54    |\n",
      "|    n_updates            | 140      |\n",
      "|    policy_gradient_loss | 0.181    |\n",
      "|    value_loss           | 0.0163   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.68925 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0318  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 140      |\n",
      "|    policy_gradient_loss | 0.188    |\n",
      "|    value_loss           | 0.0193   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 15       |\n",
      "|    time_elapsed         | 101      |\n",
      "|    total_timesteps      | 7575     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8074/10000 [01:45<00:19, 97.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.367516 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | 0.189     |\n",
      "|    value_loss           | 0.0181    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.563324 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0219    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | 0.184     |\n",
      "|    value_loss           | 0.0161    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 107       |\n",
      "|    total_timesteps      | 8080      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8584/10000 [01:51<00:15, 93.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 72.61287  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0204   |\n",
      "|    loss                 | -1.52     |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | 0.189     |\n",
      "|    value_loss           | 0.0183    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.255325 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0172    |\n",
      "|    loss                 | -1.54     |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | 0.181     |\n",
      "|    value_loss           | 0.0162    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 75        |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 114       |\n",
      "|    total_timesteps      | 8585      |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9084/10000 [01:57<00:09, 96.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.99519 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0117  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 170      |\n",
      "|    policy_gradient_loss | 0.188    |\n",
      "|    value_loss           | 0.0189   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 72.21143 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | 0.000701 |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 170      |\n",
      "|    policy_gradient_loss | 0.181    |\n",
      "|    value_loss           | 0.0158   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 75       |\n",
      "|    iterations           | 18       |\n",
      "|    time_elapsed         | 120      |\n",
      "|    total_timesteps      | 9090     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9590/10000 [02:03<00:04, 98.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| player_0_train/         |          |\n",
      "|    approx_kl            | 72.0298  |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0344  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 180      |\n",
      "|    policy_gradient_loss | 0.181    |\n",
      "|    value_loss           | 0.0166   |\n",
      "| player_1_train/         |          |\n",
      "|    approx_kl            | 71.23213 |\n",
      "|    clip_fraction        | 1        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    clip_range_vf        | 0.15     |\n",
      "|    entropy_loss         | -115     |\n",
      "|    explained_variance   | -0.0114  |\n",
      "|    loss                 | -1.53    |\n",
      "|    n_updates            | 180      |\n",
      "|    policy_gradient_loss | 0.186    |\n",
      "|    value_loss           | 0.0182   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 76       |\n",
      "|    iterations           | 19       |\n",
      "|    time_elapsed         | 126      |\n",
      "|    total_timesteps      | 9595     |\n",
      "| train/                  |          |\n",
      "|    learning_rate        | 5e-05    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10093it [02:09, 95.69it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| player_0_train/         |           |\n",
      "|    approx_kl            | 71.79315  |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | 0.0105    |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | 0.181     |\n",
      "|    value_loss           | 0.0164    |\n",
      "| player_1_train/         |           |\n",
      "|    approx_kl            | 71.517136 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    clip_range_vf        | 0.15      |\n",
      "|    entropy_loss         | -115      |\n",
      "|    explained_variance   | -0.0138   |\n",
      "|    loss                 | -1.53     |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | 0.186     |\n",
      "|    value_loss           | 0.0185    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 76        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 132       |\n",
      "|    total_timesteps      | 10100     |\n",
      "| train/                  |           |\n",
      "|    learning_rate        | 5e-05     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                          \r"
     ]
    }
   ],
   "source": [
    "learn_results = model.learn(total_timesteps=10000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=10000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy.predict_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.observation_space.items():\n",
    "    print(k, v)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.observation_space[\"team_id\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.observation_space[\"team_id\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.as_tensor(np.array(model.observation_space[\"map_explored_status\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model.observation_space[\"map_explored_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.observation_space[\"map_explored_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}\n",
    "\n",
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_model2(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2].sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output2 = model.policy(obs)\n",
    "test_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model2.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict = nn.ModuleDict(\n",
    "    {\n",
    "        \"policy_net\": nn.Sequential(\n",
    "            nn.Linear(2466, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        ),\n",
    "        \"value_net\": nn.Sequential(\n",
    "            nn.Linear(2466, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        ),\n",
    "        \"something_else\": nn.Linear(55, 6969)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.features_extractor.extractors.enemy_energies = prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mlp = nn.Sequential(\n",
    "    nn.Linear(2466, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor.policy_net = new_mlp\n",
    "model.policy.mlp_extractor.value_net = new_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CustomExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict):\n",
    "        super().__init__(observation_space, features_dim=512)\n",
    "\n",
    "        # Define CNN for grid-based inputs\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        map_features = self.cnn(observations[\"map_features_tile_type\"].unsqueeze(1))\n",
    "        return th.cat([map_features, observations[\"unit_positions\"].flatten(1)], dim=1)\n",
    "\n",
    "\n",
    "# Replace the feature extractor\n",
    "model.policy.features_extractor = CustomExtractor(model.policy.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomCNNExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    CNN Feature Extractor for spatial inputs (map-based features).\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Dict, features_dim=128):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        # CNN for 2D map-like inputs (assuming 24x24 grid)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute CNN output size dynamically\n",
    "        with th.no_grad():\n",
    "            dummy_input = th.zeros(1, 3, 24, 24)\n",
    "            cnn_out_size = self.cnn(dummy_input).shape[1]\n",
    "\n",
    "        # MLP for non-spatial inputs\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(50, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multi-Head Self-Attention for units\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4)\n",
    "        \n",
    "        # Final feature size\n",
    "        self.final_linear = nn.Linear(cnn_out_size + 64, features_dim)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        map_input = observations[\"map_features_tile_type\"].view(-1, 3, 24, 24)  # Reshape as (batch, channels, H, W)\n",
    "        non_spatial_input = observations[\"team_points\"]  # Example non-spatial input\n",
    "        unit_features = observations[\"unit_positions\"].view(-1, 16, 3)  # Reshape for attention\n",
    "        \n",
    "        map_features = self.cnn(map_input)\n",
    "        non_spatial_features = self.mlp(non_spatial_input)\n",
    "        attn_out, _ = self.attention(unit_features, unit_features, unit_features)\n",
    "        attn_out = attn_out.mean(dim=1)  # Pool across units\n",
    "        \n",
    "        combined = th.cat([map_features, attn_out], dim=1)\n",
    "        return self.final_linear(combined)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom PPO Policy with optimized architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            features_extractor_class=CustomCNNExtractor,\n",
    "            features_extractor_kwargs={\"features_dim\": 128},\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# Example Model Usage\n",
    "# env = YourEnvironment()\n",
    "# model = PPO(CustomActorCriticPolicy, env, verbose=1, ent_coef=0.015, vf_coef=0.75, clip_range_vf=0.15, n_steps=505, batch_size=505)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.pi_features_extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel environments (adjust based on CPU cores)\n",
    "NUM_ENVS = 8\n",
    "\n",
    "def make_env():\n",
    "    return ModifiedLuxAIS3GymEnv(numpy_output=True)  # Use your custom environment\n",
    "\n",
    "env = SubprocVecEnv([lambda: make_env() for _ in range(NUM_ENVS)])\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, n_steps=2048 * NUM_ENVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel environments (adjust based on CPU cores)\n",
    "NUM_ENVS = 8\n",
    "\n",
    "def make_env():\n",
    "    return ModifiedLuxAIS3GymEnv(numpy_output=True)  # Use your custom environment\n",
    "\n",
    "env = SubprocVecEnv([lambda: make_env() for _ in range(NUM_ENVS)])\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, n_steps=2048 * NUM_ENVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_zeros = np.zeros(2, dtype=np.int32)\n",
    "temp_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_zeros[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.bool(False) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_points=jnp.zeros(shape=(2), dtype=jnp.int32),\n",
    "team_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jnp.where(True, 3, -1) != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_points.at[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action0 = np.zeros((16, 3), dtype=np.int8)\n",
    "action1 = np.zeros((16, 3), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step({\n",
    "    \"player_0\": action0,\n",
    "    \"player_1\": action1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(\"player_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from Modified_stablebaseline3_PPO.modified_ppo_20250228_01 import PPO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import copy\n",
    "from GreedyLRScheduler import GreedyLR\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gc\n",
    "gc.enable()\n",
    "from stable_baselines3.common.buffers import DictRolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "init_ppo = PPO(\"MultiInputPolicy\", init_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_ppo.policy\n",
    "model_1 = copy.deepcopy(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer = DictRolloutBuffer(1000, model_0.observation_space, model_0.action_space, model_0.device)\n",
    "temp_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rollout_data in temp_buffer.get(1):\n",
    "    print(rollout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_gain_reward_func(reward_score) -> float:\n",
    "\n",
    "    return reward_score * 20 if reward_score > 0.0 else -1\n",
    "\n",
    "def match_won_reward_func(match_won) -> float:\n",
    "\n",
    "    return 5000.0 if match_won else 0.0\n",
    "\n",
    "def match_lost_reward_func(match_lost) -> float:\n",
    "\n",
    "    return -3000.0 if match_lost else 0.0\n",
    "\n",
    "def game_won_reward_func(game_won) -> float:\n",
    "\n",
    "    return 1000000000.0 if game_won else 0.0\n",
    "\n",
    "def game_lost_reward_func(game_lost) -> float:\n",
    "\n",
    "    return -1000000000.0 if game_lost else 0.0\n",
    "\n",
    "def map_reveal_reward_func(map_reveal_score):\n",
    "\n",
    "    return map_reveal_score * 10\n",
    "\n",
    "def attack_reward_func(actions, sap_range, enemy_unit_mask) -> float:\n",
    "\n",
    "    attack_score = 0.0\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        if action_num >= 5:\n",
    "            if enemy_unit_mask.sum() != 0:\n",
    "                sap_action_range = max(abs(dx), abs(dy))\n",
    "                if sap_action_range > sap_range:\n",
    "                    attack_score -= 0.5\n",
    "            else:\n",
    "                attack_score -= 5.0\n",
    "    \n",
    "    return attack_score\n",
    "\n",
    "def next_position_calculator(action_num, unit_positions):\n",
    "    # 0: stay, 1: up, 2: right, 3: down, 4: left\n",
    "\n",
    "    if action_num == 1:\n",
    "        next_position = (unit_positions[0], unit_positions[1] - 1)\n",
    "    elif action_num == 2:\n",
    "        next_position = (unit_positions[0] + 1, unit_positions[1])\n",
    "    elif action_num == 3:\n",
    "        next_position = (unit_positions[0], unit_positions[1] + 1)\n",
    "    elif action_num == 4:\n",
    "        next_position = (unit_positions[0] - 1, unit_positions[1])\n",
    "    else:\n",
    "        next_position = unit_positions\n",
    "    \n",
    "    return next_position\n",
    "\n",
    "def movement_reward_func(actions, obs, team_id) -> float:\n",
    "\n",
    "    movement_score = 0.0\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        unit_positions = obs[\"units\"][\"position\"][team_id][i]\n",
    "        unit_energy = obs[\"units\"][\"energy\"][team_id][i]\n",
    "\n",
    "        # give penalty if try to move unit that doesn't exist\n",
    "        if (unit_positions == (-1, -1)).sum() == 2 and action_num != 0:\n",
    "            movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if dx or dy is not 0 when not attacking\n",
    "        if action_num != 5:\n",
    "            if dx != 0 or dy != 0:\n",
    "                movement_score -= 0.25\n",
    "\n",
    "        \n",
    "        if unit_positions[0] >= 0 and unit_positions[1] >= 0:\n",
    "            # give penalty if try to move unit that has no energy\n",
    "            if unit_energy <= 0 and action_num != 0:\n",
    "                movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if try to move unit out of map\n",
    "        next_position = next_position_calculator(action_num, unit_positions)\n",
    "        if next_position[0] < 0 or next_position[1] < 0 or next_position[0] > 23 or next_position[1] > 23:\n",
    "            movement_score -= 0.5\n",
    "        else:\n",
    "            movement_score += 2.0\n",
    "    \n",
    "\n",
    "    return movement_score\n",
    "\n",
    "def relic_discovery_reward_func(relic_discovery_reward) -> float:\n",
    "\n",
    "    return relic_discovery_reward * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_0,\n",
    "        model_1,\n",
    "        num_games=1000,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=None,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "    ):\n",
    "        self.model_0 = model_0\n",
    "        self.model_1 = model_1\n",
    "        self.num_games = num_games\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.optimizer_0 = AdamW(self.model_0.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "        self.optimizer_1 = AdamW(self.model_1.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "\n",
    "        self.scheduler_0 = GreedyLR(self.optimizer_0, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "        self.scheduler_1 = GreedyLR(self.optimizer_1, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "\n",
    "        self.step_rollout_buffer_0 = DictRolloutBuffer(10, self.model_0.observation_space, self.model_0.action_space, device=\"cuda\")\n",
    "        self.step_rollout_buffer_1 = DictRolloutBuffer(10, self.model_1.observation_space, self.model_1.action_space, device=\"cuda\")\n",
    "\n",
    "        self.match_rollout_buffer_0 = DictRolloutBuffer(101, self.model_0.observation_space, self.model_0.action_space, device=\"cuda\")\n",
    "        self.match_rollout_buffer_1 = DictRolloutBuffer(101, self.model_1.observation_space, self.model_1.action_space, device=\"cuda\")\n",
    "\n",
    "        self.env = LuxAIS3GymEnv(numpy_output=True)\n",
    "\n",
    "        self.model_0.mlp_extractor = torch.compile(self.model_0.mlp_extractor)\n",
    "        self.model_1.mlp_extractor = torch.compile(self.model_1.mlp_extractor)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for game in range(1, self.num_games + 1):\n",
    "            print(\"=\"*15 + f\" Game {game} Started \" + \"=\"*15)\n",
    "\n",
    "            obs_all, info = self.env.reset()\n",
    "            self.env_cfg = info['params']\n",
    "\n",
    "            game_ended = False\n",
    "\n",
    "            player_0_previous_score = 0.0\n",
    "            player_1_previous_score = 0.0\n",
    "\n",
    "            first_spawn = False\n",
    "\n",
    "            self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "            self.map_explored_status = np.zeros((2, 24, 24), dtype=bool)\n",
    "\n",
    "            player_0_previous_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "            player_1_previous_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "            player_0_match_won_num = 0\n",
    "            player_1_match_won_num = 0\n",
    "\n",
    "            player_0_previous_relic_discovery_points = 0\n",
    "            player_1_previous_relic_discovery_points = 0\n",
    "\n",
    "            victor = None\n",
    "\n",
    "            game_start = True\n",
    "\n",
    "            match_number = 1\n",
    "\n",
    "            while game_ended is not True:\n",
    "\n",
    "                player_0_match_won = False\n",
    "                player_0_match_lost = False\n",
    "                player_1_match_won = False\n",
    "                player_1_match_lost = False\n",
    "\n",
    "                player_0_game_won = False\n",
    "                player_0_game_lost = False\n",
    "                player_1_game_won = False\n",
    "                player_1_game_lost = False\n",
    "\n",
    "                player_0_current_score = obs_all['player_0']['team_points'][0]\n",
    "                player_1_current_score = obs_all['player_1']['team_points'][1]\n",
    "\n",
    "                player_0_reward_score = player_0_current_score - player_0_previous_score\n",
    "                player_1_reward_score = player_1_current_score - player_1_previous_score\n",
    "\n",
    "                player_0_previous_score = player_0_current_score\n",
    "                player_1_previous_score = player_1_current_score\n",
    "\n",
    "                current_match_step = obs_all[\"player_0\"][\"match_steps\"]\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    if player_0_current_score > player_1_current_score:\n",
    "                        player_0_match_won = True\n",
    "                        player_1_match_lost = True\n",
    "                        player_0_match_won_num += 1\n",
    "                    elif player_0_current_score < player_1_current_score:\n",
    "                        player_0_match_lost = True\n",
    "                        player_1_match_won = True\n",
    "                        player_1_match_won_num += 1\n",
    "\n",
    "                if player_0_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 0 won the game.\")\n",
    "                    victor = \"player_0\"\n",
    "                    player_0_game_won = True\n",
    "                    player_1_game_lost = True\n",
    "\n",
    "                if player_1_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 1 won the game.\")\n",
    "                    victor = \"player_1\"\n",
    "                    player_0_game_lost = True\n",
    "                    player_1_game_won = True\n",
    "\n",
    "                player_0_unit_positions = np.array(obs_all['player_0'][\"units\"][\"position\"][0])\n",
    "                player_1_unit_positions = np.array(obs_all['player_1'][\"units\"][\"position\"][1])\n",
    "\n",
    "                player_0_unit_mask = np.array(obs_all['player_0'][\"units_mask\"][0])\n",
    "                player_1_unit_mask = np.array(obs_all['player_1'][\"units_mask\"][1])\n",
    "\n",
    "                player_0_available_unit_ids = np.where(player_0_unit_mask)[0]\n",
    "                player_1_available_unit_ids = np.where(player_1_unit_mask)[0]\n",
    "\n",
    "                if player_0_available_unit_ids.shape[0] == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if first_spawn == False:\n",
    "                        player_0_first_unit_id = player_0_available_unit_ids[0]\n",
    "                        player_0_first_unit_pos = player_0_unit_positions[player_0_first_unit_id]\n",
    "                        self.spawn_location[0] = (player_0_first_unit_pos[0], player_0_first_unit_pos[1])\n",
    "                        player_1_first_unit_id = player_1_available_unit_ids[0]\n",
    "                        player_1_first_unit_pos = player_1_unit_positions[player_1_first_unit_id]\n",
    "                        self.spawn_location[1] = (player_1_first_unit_pos[0], player_1_first_unit_pos[1])\n",
    "                        first_spawn = True\n",
    "\n",
    "                player_0_map_features = obs_all['player_0']['map_features']\n",
    "                player_1_map_features = obs_all['player_1']['map_features']\n",
    "\n",
    "                player_0_current_map_tile_type = player_0_map_features['tile_type'].T\n",
    "                player_1_current_map_tile_type = player_1_map_features['tile_type'].T\n",
    "\n",
    "                self.map_explored_status[0][player_0_current_map_tile_type != -1] = True\n",
    "                self.map_explored_status[1][player_1_current_map_tile_type != -1] = True\n",
    "\n",
    "                player_0_current_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "                player_1_current_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "                player_0_map_explored_status_reward = player_0_current_map_explored_status_score - player_0_previous_map_explored_status_score\n",
    "                player_1_map_explored_status_reward = player_1_current_map_explored_status_score - player_1_previous_map_explored_status_score\n",
    "\n",
    "                player_0_previous_map_explored_status_score = player_0_current_map_explored_status_score\n",
    "                player_1_previous_map_explored_status_score = player_1_current_map_explored_status_score\n",
    "\n",
    "                ### Reward caclulation\n",
    "                player_0_relic_point_reward = point_gain_reward_func(player_0_reward_score)\n",
    "                player_1_relic_point_reward = point_gain_reward_func(player_1_reward_score)\n",
    "\n",
    "                player_0_match_won_reward = match_won_reward_func(player_0_match_won)\n",
    "                player_0_match_lost_reward = match_lost_reward_func(player_0_match_lost)\n",
    "                player_1_match_won_reward = match_won_reward_func(player_1_match_won)\n",
    "                player_1_match_lost_reward = match_lost_reward_func(player_1_match_lost)\n",
    "\n",
    "                player_0_game_won_reward = game_won_reward_func(player_0_game_won)\n",
    "                player_0_game_lost_reward = game_lost_reward_func(player_0_game_lost)\n",
    "                player_1_game_won_reward = game_won_reward_func(player_1_game_won)\n",
    "                player_1_game_lost_reward = game_lost_reward_func(player_1_game_lost)\n",
    "\n",
    "                player_0_map_reveal_reward = map_reveal_reward_func(player_0_map_explored_status_reward)\n",
    "                player_1_map_reveal_reward = map_reveal_reward_func(player_1_map_explored_status_reward)\n",
    "\n",
    "                ### model input\n",
    "                if game_start == True:\n",
    "                    player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                    player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "                    game_start = False\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    player_0_action_distribution, _, _ = self.model_0(player_0_model_input)\n",
    "                    player_1_action_distribution, _, _ = self.model_1(player_1_model_input)\n",
    "\n",
    "                player_0_action = copy.deepcopy(player_0_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_0_action[:, 1] = player_0_action[:, 1] - 7\n",
    "                player_0_action[:, 2] = player_0_action[:, 2] - 7\n",
    "                player_1_action = copy.deepcopy(player_1_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_1_action[:, 1] = player_1_action[:, 1] - 7\n",
    "                player_1_action[:, 2] = player_1_action[:, 2] - 7\n",
    "\n",
    "                print(player_0_action)\n",
    "                print(obs_all[\"player_0\"][\"map_features\"][\"tile_type\"].T)\n",
    "\n",
    "                player_0_attack_reward = attack_reward_func(player_0_action, self.env_cfg[\"unit_sap_range\"], player_1_unit_mask)\n",
    "                player_1_attack_reward = attack_reward_func(player_1_action, self.env_cfg[\"unit_sap_range\"], player_0_unit_mask)\n",
    "\n",
    "                player_0_movement_reward = movement_reward_func(player_0_action, obs_all[\"player_0\"], 0)\n",
    "                player_1_movement_reward = movement_reward_func(player_1_action, obs_all[\"player_1\"], 1)\n",
    "\n",
    "                player_0_reward = player_0_relic_point_reward + player_0_match_won_reward + player_0_match_lost_reward + player_0_game_won_reward + player_0_game_lost_reward + player_0_map_reveal_reward + player_0_attack_reward + player_0_movement_reward\n",
    "                player_1_reward = player_1_relic_point_reward + player_1_match_won_reward + player_1_match_lost_reward + player_1_game_won_reward + player_1_game_lost_reward + player_1_map_reveal_reward + player_1_attack_reward + player_1_movement_reward\n",
    "                # player_0_reward = torch.tensor(player_0_reward, dtype=torch.float32, device=\"cuda\")\n",
    "                # player_1_reward = torch.tensor(player_1_reward, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "                player_0_features = self.model_0.extract_features(player_0_model_input)\n",
    "                player_1_features = self.model_1.extract_features(player_1_model_input)\n",
    "\n",
    "                player_0_latent_pi, player_0_latent_vf = self.model_0.mlp_extractor(player_0_features)\n",
    "                player_1_latent_pi, player_1_latent_vf = self.model_1.mlp_extractor(player_1_features)\n",
    "\n",
    "                player_0_distribution = self.model_0._get_action_dist_from_latent(player_0_latent_pi)\n",
    "                player_1_distribution = self.model_1._get_action_dist_from_latent(player_1_latent_pi)\n",
    "\n",
    "                player_0_log_prob = player_0_distribution.log_prob(player_0_action_distribution)\n",
    "                player_1_log_prob = player_1_distribution.log_prob(player_1_action_distribution)\n",
    "\n",
    "                player_0_value = self.model_0.value_net(player_0_latent_vf)\n",
    "                player_1_value = self.model_1.value_net(player_1_latent_vf)\n",
    "\n",
    "                player_0_entropy = player_0_distribution.entropy()\n",
    "                player_1_entropy = player_1_distribution.entropy()\n",
    "\n",
    "                obs_all, _, _, _, _ = self.env.step({\n",
    "                    \"player_0\": player_0_action.detach(),\n",
    "                    \"player_1\": player_1_action.detach()\n",
    "                })\n",
    "\n",
    "                player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Compute value for the last timestep\n",
    "                    player_0_new_value = self.model_0.predict_values(player_0_model_input)  # type: ignore[arg-type]\n",
    "                    player_1_new_value = self.model_1.predict_values(player_1_model_input)\n",
    "\n",
    "                # player_0_delta = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                # player_0_advantage = player_0_delta + self.gamma * self.gae_lambda\n",
    "                player_0_advantage = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                player_0_advantage = player_0_advantage.detach()\n",
    "                # player_0_advantage = torch.tensor(player_0_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_0_return = player_0_advantage + player_0_value\n",
    "\n",
    "                # player_1_delta = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                # player_1_advantage = player_1_delta + self.gamma * self.gae_lambda\n",
    "                player_1_advantage = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                player_1_advantage = player_1_advantage.detach()\n",
    "                # player_1_advantage = torch.tensor(player_1_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_1_return = player_1_advantage + player_1_value\n",
    "\n",
    "                player_0_policy_loss_1 = player_0_advantage\n",
    "                player_0_policy_loss_2 = player_0_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_0_policy_loss = -torch.min(player_0_policy_loss_1, player_0_policy_loss_2).mean()\n",
    "\n",
    "                player_1_policy_loss_1 = player_1_advantage\n",
    "                player_1_policy_loss_2 = player_1_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_1_policy_loss = -torch.min(player_1_policy_loss_1, player_1_policy_loss_2).mean()\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    player_0_values_pred = player_0_new_value\n",
    "                    player_1_values_pred = player_1_new_value\n",
    "                else:\n",
    "                    player_0_values_pred = player_0_value + torch.clamp(player_0_new_value - player_0_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "                    player_1_values_pred = player_1_value + torch.clamp(player_1_new_value - player_1_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "\n",
    "                player_0_value_loss = F.mse_loss(player_0_return, player_0_values_pred)\n",
    "                player_1_value_loss = F.mse_loss(player_1_return, player_1_values_pred)\n",
    "\n",
    "                player_0_entropy_loss = -torch.mean(-player_0_entropy)\n",
    "                player_1_entropy_loss = -torch.mean(-player_1_entropy)\n",
    "\n",
    "                player_0_loss = player_0_policy_loss + self.ent_coef * player_0_entropy_loss + self.vf_coef * player_0_value_loss\n",
    "                player_1_loss = player_1_policy_loss + self.ent_coef * player_1_entropy_loss + self.vf_coef * player_1_value_loss\n",
    "\n",
    "                self.optimizer_0.zero_grad()\n",
    "                player_0_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_0.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_0.step()\n",
    "                self.scheduler_0.step(player_0_loss.item())\n",
    "\n",
    "                self.optimizer_1.zero_grad()\n",
    "                player_1_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_1.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_1.step()\n",
    "                self.scheduler_1.step(player_1_loss.item())\n",
    "\n",
    "                if match_number >= 5 and current_match_step == 100:\n",
    "                    game_ended = True\n",
    "                    print(\"Game ended.\")\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    match_number += 1\n",
    "\n",
    "            if victor == \"player_0\":\n",
    "                self.synchronize_models(self.model_0, self.model_1)\n",
    "            elif victor == \"player_1\":\n",
    "                self.synchronize_models(self.model_1, self.model_0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "                \n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def prepare_model_input(self, obs, my_team_id):\n",
    "        enemy_team_id = 1 - my_team_id\n",
    "\n",
    "        self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "        \n",
    "\n",
    "        model_input = {\n",
    "            \"enemy_energies\": obs[\"units\"][\"energy\"][enemy_team_id],\n",
    "            \"enemy_positions\": obs[\"units\"][\"position\"][enemy_team_id],\n",
    "            \"enemy_spawn_location\": self.spawn_location[enemy_team_id],\n",
    "            \"enemy_visible_mask\": obs[\"units_mask\"][enemy_team_id],\n",
    "            \"map_explored_status\": self.map_explored_status[my_team_id],\n",
    "            \"map_features_energy\": obs[\"map_features\"][\"energy\"],\n",
    "            \"map_features_tile_type\": obs[\"map_features\"][\"tile_type\"],\n",
    "            \"match_steps\": np.array([obs[\"match_steps\"]]),\n",
    "            \"my_spawn_location\": self.spawn_location[my_team_id],\n",
    "            \"relic_nodes\": obs[\"relic_nodes\"],\n",
    "            \"relic_nodes_mask\": obs[\"relic_nodes_mask\"],\n",
    "            \"sensor_mask\": obs[\"sensor_mask\"],\n",
    "            \"steps\": np.array([obs[\"steps\"]]),\n",
    "            \"team_id\": np.array([my_team_id]),\n",
    "            \"team_points\": obs[\"team_points\"],\n",
    "            \"team_wins\": obs[\"team_wins\"],\n",
    "            \"unit_active_mask\": obs[\"units_mask\"][my_team_id],\n",
    "            \"unit_energies\": obs[\"units\"][\"energy\"][my_team_id],\n",
    "            \"unit_move_cost\": np.array([self.env_cfg[\"unit_move_cost\"]]),\n",
    "            \"unit_positions\": obs[\"units\"][\"position\"][my_team_id],\n",
    "            \"unit_sap_cost\": np.array([self.env_cfg[\"unit_sap_cost\"]]),\n",
    "            \"unit_sap_range\": np.array([self.env_cfg[\"unit_sap_range\"]]),\n",
    "            \"unit_sensor_range\": np.array([self.env_cfg[\"unit_sensor_range\"]]),\n",
    "        }\n",
    "\n",
    "        model_input = {k: torch.tensor(np.expand_dims(v, axis=0), dtype=torch.int32, device=\"cuda\") for k, v in model_input.items()}\n",
    "\n",
    "        return model_input\n",
    "    \n",
    "    def synchronize_models(self, winner_model, loser_model):\n",
    "        with torch.no_grad():\n",
    "            for p1, p2 in zip(winner_model.parameters(), loser_model.parameters()):\n",
    "                p2.data.copy_(p1.data)\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainPPO(model_0, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = model.policy.extract_features(obs)\n",
    "obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}\n",
    "\n",
    "# Convert observation to tensor and check shape\n",
    "obs_tensor = model.policy.extract_features(obs)\n",
    "print(f\"Extracted Feature Shape: {obs_tensor.shape}\")  # Expected: (batch_size, 2464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor = torch.compile(model.policy.mlp_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade luxai-s3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
