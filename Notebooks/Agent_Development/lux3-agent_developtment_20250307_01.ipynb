{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "import os\n",
    "# import copy\n",
    "# from GreedyLRScheduler import GreedyLR\n",
    "# from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gc\n",
    "gc.enable()\n",
    "# from stable_baselines3.common.buffers import DictRolloutBuffer\n",
    "# from tqdm.notebook import tqdm\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, learning_rate=learning_rate, ent_coef=0.015, vf_coef=0.75, clip_range_vf=0.15, clip_range=0.2, n_steps=505, batch_size=101, max_grad_norm=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_observation(obs: dict, obs_space: spaces.Dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize continuous features in the observation dict using min-max scaling,\n",
    "    while leaving discrete or binary features unchanged.\n",
    "    \"\"\"\n",
    "    norm_obs = {}\n",
    "    for key, space in obs_space.spaces.items():\n",
    "        value = obs[key]\n",
    "        # For Box spaces with numeric types (and not MultiBinary)\n",
    "        if isinstance(space, spaces.Box) and np.issubdtype(space.dtype, np.number):\n",
    "            # If the range is [0,1] (or binary), assume it's already normalized\n",
    "            if (space.low == 0).all() and (space.high == 1).all():\n",
    "                norm_obs[key] = value\n",
    "            else:\n",
    "                # Convert to float and apply min-max normalization:\n",
    "                # norm = (value - low) / (high - low)\n",
    "                low = torch.tensor(space.low, device=value.device, dtype=torch.float32)\n",
    "                high = torch.tensor(space.high, device=value.device, dtype=torch.float32)\n",
    "                # print(value)\n",
    "                norm_obs[key] = (value.to(dtype=torch.float32) - low) / (high - low + 1e-8)\n",
    "        else:\n",
    "            # For discrete or MultiBinary spaces, just copy the values\n",
    "            norm_obs[key] = value\n",
    "    return norm_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor that:\n",
    "    - Processes 24x24 grid features using CNN.\n",
    "    - Flattens and concatenates other features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Dict, features_dim: int = 0):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "\n",
    "        # Identify 24x24 grid features\n",
    "        self.grid_features = [\"map_explored_status\", \"map_features_energy\", \"map_features_tile_type\", \"sensor_mask\"]\n",
    "        self.features = []\n",
    "        for key in observation_space.keys():\n",
    "            self.features.append(key)\n",
    "\n",
    "        # **CNN for 24x24 Grid Features** (Expects input shape [batch, channels, 24, 24])\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(len(self.grid_features), 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.cnn_extractor = torch.compile(self.cnn_extractor)\n",
    "\n",
    "        # Compute CNN output dimension (using a dummy input)\n",
    "        dummy_input = torch.zeros((1, len(self.grid_features), 24, 24))\n",
    "        cnn_output_dim = self.cnn_extractor(dummy_input).shape[1]\n",
    "\n",
    "        # **Flatten layers for non-grid features**\n",
    "        self.extractors = nn.ModuleDict()\n",
    "        flatten_dim = 0\n",
    "\n",
    "        for key in observation_space.keys():\n",
    "            # print(key)\n",
    "            space_shape = observation_space.spaces[key].shape\n",
    "            self.extractors[key] = nn.Flatten()\n",
    "            flatten_dim += torch.prod(torch.tensor(space_shape)).item()\n",
    "\n",
    "        # Compute total feature dimension\n",
    "        self._features_dim = cnn_output_dim + flatten_dim\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        - Grid features go through CNN\n",
    "        - Other features are flattened\n",
    "        - Both are concatenated into a single tensor\n",
    "        \"\"\"\n",
    "        # observations = normalize_observation(observations, model.observation_space)\n",
    "\n",
    "        grid_stack = torch.stack([observations[key] for key in self.grid_features], dim=1).float()\n",
    "        grid_features = self.cnn_extractor(grid_stack)\n",
    "\n",
    "        # Flatten vector features\n",
    "        features = torch.cat([self.extractors[key](observations[key]) for key in self.features], dim=1)\n",
    "\n",
    "        combined_features = torch.cat([grid_features, features], dim=1)\n",
    "\n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=20897),\n",
    "    activation_fn=nn.SiLU,\n",
    "    # net_arch=dict(pi=[8192, 4096, 2048, 1024], vf=[8192, 4096, 2048, 1024, 512, 256, 128, 64]),\n",
    "    net_arch=dict(pi=[4096, 2048, 1024], vf=[4096, 2048, 1024, 512, 256, 128]),\n",
    "    # net_arch=dict(pi=[128, 64], vf=[128, 64]),\n",
    ")\n",
    "env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "learning_rate = 6e-4\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=0, learning_rate=learning_rate, ent_coef=0.04, vf_coef=0.75, clip_range_vf=0.2, clip_range=0.3, n_steps=505, batch_size=505,\n",
    "    max_grad_norm=0.5, n_epochs=15, save_dir=\"saved_policies/\", tensorboard_log=\"logs/\", gamma=0.99, target_kl=None, gae_lambda=0.95, load_models=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "policy1 = copy.deepcopy(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in policy1.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get observation space\n",
    "obs_space = model.policy.observation_space\n",
    "\n",
    "# Create dummy inputs\n",
    "dummy_input = {\n",
    "    key: torch.tensor(np.zeros(space.shape, dtype=np.float32)).to(\"cuda\", dtype=torch.float32).unsqueeze(0) for key, space in obs_space.spaces.items()\n",
    "}\n",
    "\n",
    "# If the model needs a single tensor, flatten and concatenate everything\n",
    "flat_input = torch.cat([v.flatten() for v in dummy_input.values()]).unsqueeze(0)  # Add batch dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_output = policy1(dummy_input)\n",
    "print(type(dummy_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy1.state_dict(), \"policy1_after_quantization.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ONNX\n",
    "torch.onnx.export(policy1, (dummy_input,), \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, model.policy.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(policy1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.policy.state_dict(), f\"policy1.pth\")\n",
    "torch.save(model.policy_2.state_dict(), f\"policy2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy_class.__init__.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=5050000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model.policy_2 = copy.deepcopy(model.policy)\n",
    "model.policy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.load_state_dict(torch.load(\"saved_policies_20250307_06/policy_200.pth\"))\n",
    "model.policy_2.load_state_dict(torch.load(\"saved_policies_20250307_06/policy_2_200.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.policy.state_dict(), \"saved_policies/ppo_policy_20250306_01.pth\")\n",
    "torch.save(model.policy_2.state_dict(), \"saved_policies/ppo_policy_2_20250306_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.1):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel environments (adjust based on CPU cores)\n",
    "NUM_ENVS = 2\n",
    "\n",
    "def make_env():\n",
    "    return ModifiedLuxAIS3GymEnv(numpy_output=True)  # Use your custom environment\n",
    "\n",
    "env = SubprocVecEnv([lambda: make_env() for _ in range(NUM_ENVS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "\n",
    "class CustomMlpExtractor(MlpExtractor):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__(feature_dim, net_arch=[4096, 1024, 256])\n",
    "\n",
    "        # Redefine policy_net with Dropout & LayerNorm\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 2048),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # Redefine value_net with Dropout & LayerNorm\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 2048),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy_net(x), self.value_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor.policy_net[2] = nn.Dropout(0.2)\n",
    "model.policy.mlp_extractor.policy_net[3] = nn.Linear(4096, 1024)\n",
    "model.policy.mlp_extractor.policy_net[4] = nn.SiLU()\n",
    "model.policy.mlp_extractor.policy_net[5] = nn.Dropout(0.2)\n",
    "model.policy.mlp_extractor.policy_net[6] = nn.Linear(1024, 512)\n",
    "model.policy.mlp_extractor.policy_net[7] = nn.SiLU()\n",
    "model.policy.mlp_extractor.policy_net.add_module(\"8\", nn.Dropout(0.2))\n",
    "model.policy.mlp_extractor.policy_net.add_module(\"9\", nn.Linear(512, 256))\n",
    "model.policy.mlp_extractor.policy_net.add_module(\"10\", nn.SiLU())\n",
    "model.policy.mlp_extractor.policy_net.add_module(\"11\", nn.Dropout(0.2))\n",
    "model.policy.mlp_extractor.value_net[2] = nn.Dropout(0.2)\n",
    "model.policy.mlp_extractor.value_net[3] = nn.Linear(4096, 1024)\n",
    "model.policy.mlp_extractor.value_net[4] = nn.SiLU()\n",
    "model.policy.mlp_extractor.value_net[5] = nn.Dropout(0.2)\n",
    "model.policy.mlp_extractor.value_net[6] = nn.Linear(1024, 512)\n",
    "model.policy.mlp_extractor.value_net[7] = nn.SiLU()\n",
    "model.policy.mlp_extractor.value_net.add_module(\"8\", nn.Dropout(0.2))\n",
    "model.policy.mlp_extractor.value_net.add_module(\"9\", nn.Linear(512, 128))\n",
    "model.policy.mlp_extractor.value_net.add_module(\"10\", nn.SiLU())\n",
    "model.policy.mlp_extractor.value_net.add_module(\"11\", nn.Dropout(0.2))\n",
    "model.policy.mlp_extractor.value_net.add_module(\"12\", nn.Linear(128, 32))\n",
    "model.policy.mlp_extractor.value_net.add_module(\"13\", nn.SiLU())\n",
    "model.policy.mlp_extractor.value_net.add_module(\"14\", nn.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy_2.mlp_extractor.policy_net[2] = nn.Dropout(0.2)\n",
    "model.policy_2.mlp_extractor.policy_net[3] = nn.Linear(4096, 1024)\n",
    "model.policy_2.mlp_extractor.policy_net[4] = nn.SiLU()\n",
    "model.policy_2.mlp_extractor.policy_net[5] = nn.Dropout(0.2)\n",
    "model.policy_2.mlp_extractor.policy_net[6] = nn.Linear(1024, 512)\n",
    "model.policy_2.mlp_extractor.policy_net[7] = nn.SiLU()\n",
    "model.policy_2.mlp_extractor.policy_net.add_module(\"8\", nn.Dropout(0.2))\n",
    "model.policy_2.mlp_extractor.policy_net.add_module(\"9\", nn.Linear(512, 256))\n",
    "model.policy_2.mlp_extractor.policy_net.add_module(\"10\", nn.SiLU())\n",
    "model.policy_2.mlp_extractor.policy_net.add_module(\"11\", nn.Dropout(0.2))\n",
    "model.policy_2.mlp_extractor.value_net[2] = nn.Dropout(0.2)\n",
    "model.policy_2.mlp_extractor.value_net[3] = nn.Linear(4096, 1024)\n",
    "model.policy_2.mlp_extractor.value_net[4] = nn.SiLU()\n",
    "model.policy_2.mlp_extractor.value_net[5] = nn.Dropout(0.2)\n",
    "model.policy_2.mlp_extractor.value_net[6] = nn.Linear(1024, 512)\n",
    "model.policy_2.mlp_extractor.value_net[7] = nn.SiLU()\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"8\", nn.Dropout(0.2))\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"9\", nn.Linear(512, 128))\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"10\", nn.SiLU())\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"11\", nn.Dropout(0.2))\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"12\", nn.Linear(128, 32))\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"13\", nn.SiLU())\n",
    "model.policy_2.mlp_extractor.value_net.add_module(\"14\", nn.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor that:\n",
    "    - Processes 24x24 grid features using CNN.\n",
    "    - Flattens and concatenates other features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Dict, features_dim: int = model.policy.features_dim):\n",
    "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "\n",
    "        # Identify 24x24 grid features\n",
    "        self.grid_features = [\"map_explored_status\", \"map_features_energy\", \"map_features_tile_type\", \"sensor_mask\"]\n",
    "\n",
    "        # Identify 1D and 2D features (excluding grid)\n",
    "        # self.scalar_features = []\n",
    "        self.vector_features = []\n",
    "        for key, space in observation_space.spaces.items():\n",
    "            if key in self.grid_features:\n",
    "                continue  # Grid features are processed separately\n",
    "            # elif space.shape == ():  # Scalar value (e.g., team_id)\n",
    "            #     self.scalar_features.append(key)\n",
    "            elif len(space.shape) == 1:  # 1D vector (e.g., enemy_energies)\n",
    "                self.vector_features.append(key)\n",
    "            elif len(space.shape) == 2:  # 2D tensor (e.g., enemy_positions)\n",
    "                self.vector_features.append(key)  # Flattened separately\n",
    "\n",
    "        # **CNN for 24x24 Grid Features** (Expects input shape [batch, channels, 24, 24])\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv2d(len(self.grid_features), 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Compute CNN output dimension (using a dummy input)\n",
    "        dummy_input = torch.zeros((1, len(self.grid_features), 24, 24))\n",
    "        cnn_output_dim = self.cnn_extractor(dummy_input).shape[1]\n",
    "\n",
    "        # **Flatten layers for non-grid features**\n",
    "        self.extractors = nn.ModuleDict()\n",
    "        vector_dim = 0\n",
    "\n",
    "        for key in self.vector_features:\n",
    "            # print(key)\n",
    "            space_shape = observation_space.spaces[key].shape\n",
    "            self.extractors[key] = nn.Flatten()\n",
    "            vector_dim += torch.prod(torch.tensor(space_shape)).item()\n",
    "\n",
    "        # Scalar features are just concatenated directly\n",
    "        # scalar_dim = len(self.scalar_features)\n",
    "\n",
    "        # Compute total feature dimension\n",
    "        self._features_dim = cnn_output_dim + vector_dim # + scalar_dim\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        - Grid features go through CNN\n",
    "        - Other features are flattened\n",
    "        - Both are concatenated into a single tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"--- Feature Extractor Forward Pass ---\") # Separator\n",
    "        # print(\"Input Observations (first element of batch):\\n\", observations) # Print input observations (first batch element)\n",
    "        observations = normalize_observation(observations, model.observation_space)\n",
    "        # print(observations)\n",
    "\n",
    "\n",
    "\n",
    "        grid_stack = torch.stack([observations[key] for key in self.grid_features], dim=1).float()\n",
    "        grid_features = self.cnn_extractor(grid_stack)\n",
    "\n",
    "        # Flatten vector features\n",
    "        vector_features = torch.cat([self.extractors[key](observations[key]) for key in self.vector_features], dim=1)\n",
    "\n",
    "        combined_features = torch.cat([grid_features, vector_features], dim=1)\n",
    "\n",
    "        # print(\"Output Features (first element of batch):\\n\", combined_features) # Print output features (first batch element)\n",
    "\n",
    "        return combined_features\n",
    "    \n",
    "\n",
    "class CustomMlpExtractor(nn.ModuleDict):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        reduced_dim = 1024  # Reduce 92,321 → 1024\n",
    "        # 18593\n",
    "        # 8192\n",
    "        self.feature_reduction = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 8192),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(8192),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, reduced_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(reduced_dim),  # **LayerNorm for stability**\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(reduced_dim, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(reduced_dim, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        latent_pi = self.policy_net(x)\n",
    "        latent_vf = self.value_net(x)\n",
    "        return latent_pi, latent_vf\n",
    "\n",
    "    def forward_actor(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        return self.policy_net(x)\n",
    "\n",
    "    def forward_critic(self, x):\n",
    "        x = self.feature_reduction(x)\n",
    "        return self.value_net(x)\n",
    "\n",
    "\n",
    "class CustomMultiInputPolicy(MultiInputActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom MultiInput Policy that:\n",
    "    - Uses CNN for spatial features.\n",
    "    - Uses MLP for non-spatial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomMultiInputPolicy, self).__init__(\n",
    "            *args, **kwargs,\n",
    "            features_extractor_class=CustomFeatureExtractor,\n",
    "        )\n",
    "        print(self.features_extractor._features_dim)\n",
    "        self.mlp_extractor = CustomMlpExtractor(self.features_extractor._features_dim)\n",
    "\n",
    "        # Output layers\n",
    "        # self.action_net = nn.Linear(128, 16*6 + 16*2*15)  # Adjust for action space\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 16*6 + 16*2*15)\n",
    "        )\n",
    "        # self.value_net = nn.Linear(128, 1)\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass through the policy.\n",
    "        \"\"\"\n",
    "        # obs = normalize_observation(obs, model.observation_space)\n",
    "        features = self.features_extractor(obs)\n",
    "        policy_features = self.mlp_extractor.forward_actor(features)\n",
    "        value_features = self.mlp_extractor.forward_critic(features)\n",
    "\n",
    "        # Get logits for discrete action space\n",
    "        logits = self.action_net(policy_features) #/ 10.0  # Divide by 10 for stability\n",
    "        # logits = torch.tanh(logits) / 20.0\n",
    "        logits = logits / 20.0\n",
    "\n",
    "        action_logits = logits[:, :16*6].view(-1, 16, 6)\n",
    "        dxdy_logits = logits[:, 16*6:].view(-1, 16, 2, 15)\n",
    "        dx_logits = dxdy_logits[:, :, 0, :]\n",
    "        dy_logits = dxdy_logits[:, :, 1, :]\n",
    "\n",
    "        # print(\"--- Policy Forward Pass ---\") # Add a separator for clarity\n",
    "        # print(\"Action Logits (before softmax):\\n\", action_logits) # Print raw logits\n",
    "        # print(\"DX Logits (before softmax):\\n\", dx_logits) # Print raw logits\n",
    "        # print(\"DY Logits (before softmax):\\n\", dy_logits) # Print raw logits\n",
    "\n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        dx_probs = F.softmax(dx_logits, dim=-1)\n",
    "        dy_probs = F.softmax(dy_logits, dim=-1)\n",
    "\n",
    "        # print(\"Action Probs (after softmax):\\n\", action_probs) # Print probabilities\n",
    "        # print(\"DX Probs (after softmax):\\n\", dx_probs) # Print probabilities\n",
    "        # print(\"DY Probs (after softmax):\\n\", dy_probs) # Print probabilities\n",
    "\n",
    "        actions_dist = torch.distributions.Categorical(probs=action_probs)\n",
    "        dx_dist = torch.distributions.Categorical(probs=dx_probs)\n",
    "        dy_dist = torch.distributions.Categorical(probs=dy_probs)\n",
    "\n",
    "        # print(\"Action Entropy:\", actions_dist.entropy().mean()) # Print Entropy\n",
    "        # print(\"DX Entropy:\", dx_dist.entropy().mean()) # Print Entropy\n",
    "        # print(\"DY Entropy:\", dy_dist.entropy().mean()) # Print Entropy\n",
    "\n",
    "        actions = actions_dist.sample()\n",
    "        dx = dx_dist.sample()\n",
    "        dy = dy_dist.sample()\n",
    "\n",
    "        zeros = torch.zeros((actions.shape[0], 16, 3), dtype=actions.dtype, device=actions.device)\n",
    "        zeros[:, :, 0] = actions\n",
    "        sap_mask = zeros == 5\n",
    "        sap_mask_dxdy = sap_mask[:, :, 0]\n",
    "        batch_idx, unit_idx = sap_mask_dxdy.nonzero(as_tuple=True)\n",
    "\n",
    "        zeros[batch_idx, unit_idx, 1] = dx[batch_idx, unit_idx]\n",
    "        zeros[batch_idx, unit_idx, 2] = dy[batch_idx, unit_idx]\n",
    "\n",
    "        # ---- Compute log_probs ----\n",
    "        actions_log_probs = actions_dist.log_prob(actions)  # (batch_size, 16)\n",
    "        dx_log_probs = dx_dist.log_prob(dx)        # (batch_size, 16)\n",
    "        dy_log_probs = dy_dist.log_prob(dy)        # (batch_size, 16)\n",
    "\n",
    "        # Apply SAP mask to sum only dx/dy log_probs where action == 5\n",
    "        dxdy_log_probs = torch.zeros_like(actions_log_probs)  # Initialize to zeros\n",
    "        dxdy_log_probs[batch_idx, unit_idx] = dx_log_probs[batch_idx, unit_idx] + dy_log_probs[batch_idx, unit_idx]\n",
    "\n",
    "        total_log_probs = actions_log_probs + dxdy_log_probs  # Final log probability per unit\n",
    "\n",
    "        return zeros, self.value_net(value_features), total_log_probs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy = CustomMultiInputPolicy(model.observation_space, model.action_space, model.lr_schedule).to(\"cuda\")\n",
    "custom_policy_2 = CustomMultiInputPolicy(model.observation_space, model.action_space, model.lr_schedule).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class GreedyLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, factor=0.1, patience=10, cooldown=0, warmup=0, \n",
    "                 min_lr=0, max_lr=10, smooth=False, window=5, reset=None):\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.cooldown = cooldown\n",
    "        self.warmup = warmup\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.smooth = smooth\n",
    "        self.window = window\n",
    "        self.reset = reset\n",
    "        \n",
    "        self.best_loss = float('inf')\n",
    "        self.warmup_counter = 0\n",
    "        self.cooldown_counter = 0\n",
    "        self.num_good_epochs = 0\n",
    "        self.num_bad_epochs = 0\n",
    "        self.loss_window = []\n",
    "        \n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def step(self, metrics=None):\n",
    "        if metrics is not None:\n",
    "            current_lr = self.get_lr()[0]\n",
    "            \n",
    "            if self.smooth:\n",
    "                self.loss_window.append(metrics)\n",
    "                if len(self.loss_window) > self.window:\n",
    "                    self.loss_window.pop(0)\n",
    "                metrics = sum(self.loss_window) / len(self.loss_window)\n",
    "            \n",
    "            if metrics < self.best_loss:\n",
    "                self.best_loss = metrics\n",
    "                self.num_good_epochs += 1\n",
    "                self.num_bad_epochs = 0\n",
    "            else:\n",
    "                self.num_good_epochs = 0\n",
    "                self.num_bad_epochs += 1\n",
    "            \n",
    "            if self.warmup_counter < self.warmup:\n",
    "                self.warmup_counter += 1\n",
    "                new_lr = min(current_lr / self.factor, self.max_lr)\n",
    "            elif self.cooldown_counter < self.cooldown:\n",
    "                self.cooldown_counter += 1\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "            elif self.num_good_epochs >= self.patience:\n",
    "                new_lr = min(current_lr / self.factor, self.max_lr)\n",
    "                self.cooldown_counter = 0\n",
    "            elif self.num_bad_epochs >= self.patience:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                self.warmup_counter = 0\n",
    "            else:\n",
    "                new_lr = current_lr\n",
    "            \n",
    "            new_lr = max(self.min_lr, min(new_lr, self.max_lr))\n",
    "            \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            \n",
    "            if self.reset and self.last_epoch % self.reset == 0:\n",
    "                self.best_loss = float('inf')\n",
    "                self.warmup_counter = 0\n",
    "                self.cooldown_counter = 0\n",
    "                self.num_good_epochs = 0\n",
    "                self.num_bad_epochs = 0\n",
    "                self.loss_window = []\n",
    "            \n",
    "            self.last_epoch += 1\n",
    "            return [new_lr]\n",
    "        else:\n",
    "            return self.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 5e-3\n",
    "min_lr = 1e-12\n",
    "\n",
    "# custom_policy.optimizer = torch.optim.AdamW(custom_policy.parameters(), lr=learning_rate, fused=True)  # type: ignore[call-arg]\n",
    "custom_policy.optimizer = torch.optim.Adam(custom_policy.parameters(), lr=learning_rate, fused=True)  # type: ignore[call-arg]\n",
    "custom_policy.scheduler = GreedyLR(custom_policy.optimizer, factor=1, patience=10, cooldown=3, warmup=0, min_lr=min_lr, max_lr=max_lr, smooth=False, window=5, reset=None)\n",
    "\n",
    "# custom_policy_2.optimizer = torch.optim.AdamW(custom_policy_2.parameters(), lr=learning_rate, fused=True)  # type: ignore[call-arg]\n",
    "custom_policy_2.optimizer = torch.optim.Adam(custom_policy_2.parameters(), lr=learning_rate, fused=True)  # type: ignore[call-arg]\n",
    "custom_policy_2.scheduler = GreedyLR(custom_policy_2.optimizer, factor=1, patience=10, cooldown=3, warmup=0, min_lr=min_lr, max_lr=max_lr, smooth=False, window=5, reset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy.train()\n",
    "custom_policy_2.train()\n",
    "\n",
    "for module in custom_policy.children():\n",
    "    module.train(True)\n",
    "\n",
    "for module in custom_policy_2.children():\n",
    "    module.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy = custom_policy\n",
    "model.policy_2 = custom_policy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=10000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.observation_space.items():\n",
    "    print(k, v)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.observation_space[\"team_id\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.observation_space[\"team_id\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.as_tensor(np.array(model.observation_space[\"map_explored_status\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model.observation_space[\"map_explored_status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.observation_space[\"map_explored_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}\n",
    "\n",
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_model2(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2].sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output2 = model.policy(obs)\n",
    "test_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model2.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict = nn.ModuleDict(\n",
    "    {\n",
    "        \"policy_net\": nn.Sequential(\n",
    "            nn.Linear(2466, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        ),\n",
    "        \"value_net\": nn.Sequential(\n",
    "            nn.Linear(2466, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        ),\n",
    "        \"something_else\": nn.Linear(55, 6969)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.features_extractor.extractors.enemy_energies = prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac_module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mlp = nn.Sequential(\n",
    "    nn.Linear(2466, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor.policy_net = new_mlp\n",
    "model.policy.mlp_extractor.value_net = new_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CustomExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict):\n",
    "        super().__init__(observation_space, features_dim=512)\n",
    "\n",
    "        # Define CNN for grid-based inputs\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        map_features = self.cnn(observations[\"map_features_tile_type\"].unsqueeze(1))\n",
    "        return th.cat([map_features, observations[\"unit_positions\"].flatten(1)], dim=1)\n",
    "\n",
    "\n",
    "# Replace the feature extractor\n",
    "model.policy.features_extractor = CustomExtractor(model.policy.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomCNNExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    CNN Feature Extractor for spatial inputs (map-based features).\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Dict, features_dim=128):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        # CNN for 2D map-like inputs (assuming 24x24 grid)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute CNN output size dynamically\n",
    "        with th.no_grad():\n",
    "            dummy_input = th.zeros(1, 3, 24, 24)\n",
    "            cnn_out_size = self.cnn(dummy_input).shape[1]\n",
    "\n",
    "        # MLP for non-spatial inputs\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(50, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Multi-Head Self-Attention for units\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4)\n",
    "        \n",
    "        # Final feature size\n",
    "        self.final_linear = nn.Linear(cnn_out_size + 64, features_dim)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        map_input = observations[\"map_features_tile_type\"].view(-1, 3, 24, 24)  # Reshape as (batch, channels, H, W)\n",
    "        non_spatial_input = observations[\"team_points\"]  # Example non-spatial input\n",
    "        unit_features = observations[\"unit_positions\"].view(-1, 16, 3)  # Reshape for attention\n",
    "        \n",
    "        map_features = self.cnn(map_input)\n",
    "        non_spatial_features = self.mlp(non_spatial_input)\n",
    "        attn_out, _ = self.attention(unit_features, unit_features, unit_features)\n",
    "        attn_out = attn_out.mean(dim=1)  # Pool across units\n",
    "        \n",
    "        combined = th.cat([map_features, attn_out], dim=1)\n",
    "        return self.final_linear(combined)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom PPO Policy with optimized architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            features_extractor_class=CustomCNNExtractor,\n",
    "            features_extractor_kwargs={\"features_dim\": 128},\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# Example Model Usage\n",
    "# env = YourEnvironment()\n",
    "# model = PPO(CustomActorCriticPolicy, env, verbose=1, ent_coef=0.015, vf_coef=0.75, clip_range_vf=0.15, n_steps=505, batch_size=505)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.pi_features_extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel environments (adjust based on CPU cores)\n",
    "NUM_ENVS = 8\n",
    "\n",
    "def make_env():\n",
    "    return ModifiedLuxAIS3GymEnv(numpy_output=True)  # Use your custom environment\n",
    "\n",
    "env = SubprocVecEnv([lambda: make_env() for _ in range(NUM_ENVS)])\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, n_steps=2048 * NUM_ENVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel environments (adjust based on CPU cores)\n",
    "NUM_ENVS = 8\n",
    "\n",
    "def make_env():\n",
    "    return ModifiedLuxAIS3GymEnv(numpy_output=True)  # Use your custom environment\n",
    "\n",
    "env = SubprocVecEnv([lambda: make_env() for _ in range(NUM_ENVS)])\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, n_steps=2048 * NUM_ENVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_results = model.learn(total_timesteps=2000000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_zeros = np.zeros(2, dtype=np.int32)\n",
    "temp_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_zeros[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.bool(False) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_points=jnp.zeros(shape=(2), dtype=jnp.int32),\n",
    "team_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jnp.where(True, 3, -1) != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_points.at[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_all, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action0 = np.zeros((16, 3), dtype=np.int8)\n",
    "action1 = np.zeros((16, 3), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step({\n",
    "    \"player_0\": action0,\n",
    "    \"player_1\": action1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(\"player_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_lux3_wrapper.modified_wrappers_20250228_01 import ModifiedLuxAIS3GymEnv\n",
    "import numpy as np\n",
    "from Modified_stablebaseline3_PPO.modified_ppo_20250228_01 import PPO\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import copy\n",
    "from GreedyLRScheduler import GreedyLR\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gc\n",
    "gc.enable()\n",
    "from stable_baselines3.common.buffers import DictRolloutBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 128\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_env = ModifiedLuxAIS3GymEnv(numpy_output=True)\n",
    "init_ppo = PPO(\"MultiInputPolicy\", init_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = init_ppo.policy\n",
    "model_1 = copy.deepcopy(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer = DictRolloutBuffer(1000, model_0.observation_space, model_0.action_space, model_0.device)\n",
    "temp_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buffer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rollout_data in temp_buffer.get(1):\n",
    "    print(rollout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_gain_reward_func(reward_score) -> float:\n",
    "\n",
    "    return reward_score * 20 if reward_score > 0.0 else -1\n",
    "\n",
    "def match_won_reward_func(match_won) -> float:\n",
    "\n",
    "    return 5000.0 if match_won else 0.0\n",
    "\n",
    "def match_lost_reward_func(match_lost) -> float:\n",
    "\n",
    "    return -3000.0 if match_lost else 0.0\n",
    "\n",
    "def game_won_reward_func(game_won) -> float:\n",
    "\n",
    "    return 1000000000.0 if game_won else 0.0\n",
    "\n",
    "def game_lost_reward_func(game_lost) -> float:\n",
    "\n",
    "    return -1000000000.0 if game_lost else 0.0\n",
    "\n",
    "def map_reveal_reward_func(map_reveal_score):\n",
    "\n",
    "    return map_reveal_score * 10\n",
    "\n",
    "def attack_reward_func(actions, sap_range, enemy_unit_mask) -> float:\n",
    "\n",
    "    attack_score = 0.0\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        if action_num >= 5:\n",
    "            if enemy_unit_mask.sum() != 0:\n",
    "                sap_action_range = max(abs(dx), abs(dy))\n",
    "                if sap_action_range > sap_range:\n",
    "                    attack_score -= 0.5\n",
    "            else:\n",
    "                attack_score -= 5.0\n",
    "    \n",
    "    return attack_score\n",
    "\n",
    "def next_position_calculator(action_num, unit_positions):\n",
    "    # 0: stay, 1: up, 2: right, 3: down, 4: left\n",
    "\n",
    "    if action_num == 1:\n",
    "        next_position = (unit_positions[0], unit_positions[1] - 1)\n",
    "    elif action_num == 2:\n",
    "        next_position = (unit_positions[0] + 1, unit_positions[1])\n",
    "    elif action_num == 3:\n",
    "        next_position = (unit_positions[0], unit_positions[1] + 1)\n",
    "    elif action_num == 4:\n",
    "        next_position = (unit_positions[0] - 1, unit_positions[1])\n",
    "    else:\n",
    "        next_position = unit_positions\n",
    "    \n",
    "    return next_position\n",
    "\n",
    "def movement_reward_func(actions, obs, team_id) -> float:\n",
    "\n",
    "    movement_score = 0.0\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        action_num, dx, dy = action[0], action[1], action[2]\n",
    "        unit_positions = obs[\"units\"][\"position\"][team_id][i]\n",
    "        unit_energy = obs[\"units\"][\"energy\"][team_id][i]\n",
    "\n",
    "        # give penalty if try to move unit that doesn't exist\n",
    "        if (unit_positions == (-1, -1)).sum() == 2 and action_num != 0:\n",
    "            movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if dx or dy is not 0 when not attacking\n",
    "        if action_num != 5:\n",
    "            if dx != 0 or dy != 0:\n",
    "                movement_score -= 0.25\n",
    "\n",
    "        \n",
    "        if unit_positions[0] >= 0 and unit_positions[1] >= 0:\n",
    "            # give penalty if try to move unit that has no energy\n",
    "            if unit_energy <= 0 and action_num != 0:\n",
    "                movement_score -= 0.25\n",
    "        \n",
    "        # give penalty if try to move unit out of map\n",
    "        next_position = next_position_calculator(action_num, unit_positions)\n",
    "        if next_position[0] < 0 or next_position[1] < 0 or next_position[0] > 23 or next_position[1] > 23:\n",
    "            movement_score -= 0.5\n",
    "        else:\n",
    "            movement_score += 2.0\n",
    "    \n",
    "\n",
    "    return movement_score\n",
    "\n",
    "def relic_discovery_reward_func(relic_discovery_reward) -> float:\n",
    "\n",
    "    return relic_discovery_reward * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_0,\n",
    "        model_1,\n",
    "        num_games=1000,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=None,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "    ):\n",
    "        self.model_0 = model_0\n",
    "        self.model_1 = model_1\n",
    "        self.num_games = num_games\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.optimizer_0 = AdamW(self.model_0.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "        self.optimizer_1 = AdamW(self.model_1.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay, fused=True)\n",
    "\n",
    "        self.scheduler_0 = GreedyLR(self.optimizer_0, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "        self.scheduler_1 = GreedyLR(self.optimizer_1, cooldown=3, min_lr=1e-7, max_lr=5e-4)\n",
    "\n",
    "        self.step_rollout_buffer_0 = DictRolloutBuffer(10, self.model_0.observation_space, self.model_0.action_space, device=\"cuda\")\n",
    "        self.step_rollout_buffer_1 = DictRolloutBuffer(10, self.model_1.observation_space, self.model_1.action_space, device=\"cuda\")\n",
    "\n",
    "        self.match_rollout_buffer_0 = DictRolloutBuffer(101, self.model_0.observation_space, self.model_0.action_space, device=\"cuda\")\n",
    "        self.match_rollout_buffer_1 = DictRolloutBuffer(101, self.model_1.observation_space, self.model_1.action_space, device=\"cuda\")\n",
    "\n",
    "        self.env = LuxAIS3GymEnv(numpy_output=True)\n",
    "\n",
    "        self.model_0.mlp_extractor = torch.compile(self.model_0.mlp_extractor)\n",
    "        self.model_1.mlp_extractor = torch.compile(self.model_1.mlp_extractor)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for game in range(1, self.num_games + 1):\n",
    "            print(\"=\"*15 + f\" Game {game} Started \" + \"=\"*15)\n",
    "\n",
    "            obs_all, info = self.env.reset()\n",
    "            self.env_cfg = info['params']\n",
    "\n",
    "            game_ended = False\n",
    "\n",
    "            player_0_previous_score = 0.0\n",
    "            player_1_previous_score = 0.0\n",
    "\n",
    "            first_spawn = False\n",
    "\n",
    "            self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "            self.map_explored_status = np.zeros((2, 24, 24), dtype=bool)\n",
    "\n",
    "            player_0_previous_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "            player_1_previous_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "            player_0_match_won_num = 0\n",
    "            player_1_match_won_num = 0\n",
    "\n",
    "            player_0_previous_relic_discovery_points = 0\n",
    "            player_1_previous_relic_discovery_points = 0\n",
    "\n",
    "            victor = None\n",
    "\n",
    "            game_start = True\n",
    "\n",
    "            match_number = 1\n",
    "\n",
    "            while game_ended is not True:\n",
    "\n",
    "                player_0_match_won = False\n",
    "                player_0_match_lost = False\n",
    "                player_1_match_won = False\n",
    "                player_1_match_lost = False\n",
    "\n",
    "                player_0_game_won = False\n",
    "                player_0_game_lost = False\n",
    "                player_1_game_won = False\n",
    "                player_1_game_lost = False\n",
    "\n",
    "                player_0_current_score = obs_all['player_0']['team_points'][0]\n",
    "                player_1_current_score = obs_all['player_1']['team_points'][1]\n",
    "\n",
    "                player_0_reward_score = player_0_current_score - player_0_previous_score\n",
    "                player_1_reward_score = player_1_current_score - player_1_previous_score\n",
    "\n",
    "                player_0_previous_score = player_0_current_score\n",
    "                player_1_previous_score = player_1_current_score\n",
    "\n",
    "                current_match_step = obs_all[\"player_0\"][\"match_steps\"]\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    if player_0_current_score > player_1_current_score:\n",
    "                        player_0_match_won = True\n",
    "                        player_1_match_lost = True\n",
    "                        player_0_match_won_num += 1\n",
    "                    elif player_0_current_score < player_1_current_score:\n",
    "                        player_0_match_lost = True\n",
    "                        player_1_match_won = True\n",
    "                        player_1_match_won_num += 1\n",
    "\n",
    "                if player_0_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 0 won the game.\")\n",
    "                    victor = \"player_0\"\n",
    "                    player_0_game_won = True\n",
    "                    player_1_game_lost = True\n",
    "\n",
    "                if player_1_match_won_num >= 3:\n",
    "                    game_ended = True\n",
    "                    print(\"Player 1 won the game.\")\n",
    "                    victor = \"player_1\"\n",
    "                    player_0_game_lost = True\n",
    "                    player_1_game_won = True\n",
    "\n",
    "                player_0_unit_positions = np.array(obs_all['player_0'][\"units\"][\"position\"][0])\n",
    "                player_1_unit_positions = np.array(obs_all['player_1'][\"units\"][\"position\"][1])\n",
    "\n",
    "                player_0_unit_mask = np.array(obs_all['player_0'][\"units_mask\"][0])\n",
    "                player_1_unit_mask = np.array(obs_all['player_1'][\"units_mask\"][1])\n",
    "\n",
    "                player_0_available_unit_ids = np.where(player_0_unit_mask)[0]\n",
    "                player_1_available_unit_ids = np.where(player_1_unit_mask)[0]\n",
    "\n",
    "                if player_0_available_unit_ids.shape[0] == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    if first_spawn == False:\n",
    "                        player_0_first_unit_id = player_0_available_unit_ids[0]\n",
    "                        player_0_first_unit_pos = player_0_unit_positions[player_0_first_unit_id]\n",
    "                        self.spawn_location[0] = (player_0_first_unit_pos[0], player_0_first_unit_pos[1])\n",
    "                        player_1_first_unit_id = player_1_available_unit_ids[0]\n",
    "                        player_1_first_unit_pos = player_1_unit_positions[player_1_first_unit_id]\n",
    "                        self.spawn_location[1] = (player_1_first_unit_pos[0], player_1_first_unit_pos[1])\n",
    "                        first_spawn = True\n",
    "\n",
    "                player_0_map_features = obs_all['player_0']['map_features']\n",
    "                player_1_map_features = obs_all['player_1']['map_features']\n",
    "\n",
    "                player_0_current_map_tile_type = player_0_map_features['tile_type'].T\n",
    "                player_1_current_map_tile_type = player_1_map_features['tile_type'].T\n",
    "\n",
    "                self.map_explored_status[0][player_0_current_map_tile_type != -1] = True\n",
    "                self.map_explored_status[1][player_1_current_map_tile_type != -1] = True\n",
    "\n",
    "                player_0_current_map_explored_status_score = self.map_explored_status[0].sum()\n",
    "                player_1_current_map_explored_status_score = self.map_explored_status[1].sum()\n",
    "\n",
    "                player_0_map_explored_status_reward = player_0_current_map_explored_status_score - player_0_previous_map_explored_status_score\n",
    "                player_1_map_explored_status_reward = player_1_current_map_explored_status_score - player_1_previous_map_explored_status_score\n",
    "\n",
    "                player_0_previous_map_explored_status_score = player_0_current_map_explored_status_score\n",
    "                player_1_previous_map_explored_status_score = player_1_current_map_explored_status_score\n",
    "\n",
    "                ### Reward caclulation\n",
    "                player_0_relic_point_reward = point_gain_reward_func(player_0_reward_score)\n",
    "                player_1_relic_point_reward = point_gain_reward_func(player_1_reward_score)\n",
    "\n",
    "                player_0_match_won_reward = match_won_reward_func(player_0_match_won)\n",
    "                player_0_match_lost_reward = match_lost_reward_func(player_0_match_lost)\n",
    "                player_1_match_won_reward = match_won_reward_func(player_1_match_won)\n",
    "                player_1_match_lost_reward = match_lost_reward_func(player_1_match_lost)\n",
    "\n",
    "                player_0_game_won_reward = game_won_reward_func(player_0_game_won)\n",
    "                player_0_game_lost_reward = game_lost_reward_func(player_0_game_lost)\n",
    "                player_1_game_won_reward = game_won_reward_func(player_1_game_won)\n",
    "                player_1_game_lost_reward = game_lost_reward_func(player_1_game_lost)\n",
    "\n",
    "                player_0_map_reveal_reward = map_reveal_reward_func(player_0_map_explored_status_reward)\n",
    "                player_1_map_reveal_reward = map_reveal_reward_func(player_1_map_explored_status_reward)\n",
    "\n",
    "                ### model input\n",
    "                if game_start == True:\n",
    "                    player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                    player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "                    game_start = False\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    player_0_action_distribution, _, _ = self.model_0(player_0_model_input)\n",
    "                    player_1_action_distribution, _, _ = self.model_1(player_1_model_input)\n",
    "\n",
    "                player_0_action = copy.deepcopy(player_0_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_0_action[:, 1] = player_0_action[:, 1] - 7\n",
    "                player_0_action[:, 2] = player_0_action[:, 2] - 7\n",
    "                player_1_action = copy.deepcopy(player_1_action_distribution.reshape(-1, 16, 3)).squeeze()\n",
    "                player_1_action[:, 1] = player_1_action[:, 1] - 7\n",
    "                player_1_action[:, 2] = player_1_action[:, 2] - 7\n",
    "\n",
    "                print(player_0_action)\n",
    "                print(obs_all[\"player_0\"][\"map_features\"][\"tile_type\"].T)\n",
    "\n",
    "                player_0_attack_reward = attack_reward_func(player_0_action, self.env_cfg[\"unit_sap_range\"], player_1_unit_mask)\n",
    "                player_1_attack_reward = attack_reward_func(player_1_action, self.env_cfg[\"unit_sap_range\"], player_0_unit_mask)\n",
    "\n",
    "                player_0_movement_reward = movement_reward_func(player_0_action, obs_all[\"player_0\"], 0)\n",
    "                player_1_movement_reward = movement_reward_func(player_1_action, obs_all[\"player_1\"], 1)\n",
    "\n",
    "                player_0_reward = player_0_relic_point_reward + player_0_match_won_reward + player_0_match_lost_reward + player_0_game_won_reward + player_0_game_lost_reward + player_0_map_reveal_reward + player_0_attack_reward + player_0_movement_reward\n",
    "                player_1_reward = player_1_relic_point_reward + player_1_match_won_reward + player_1_match_lost_reward + player_1_game_won_reward + player_1_game_lost_reward + player_1_map_reveal_reward + player_1_attack_reward + player_1_movement_reward\n",
    "                # player_0_reward = torch.tensor(player_0_reward, dtype=torch.float32, device=\"cuda\")\n",
    "                # player_1_reward = torch.tensor(player_1_reward, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "                player_0_features = self.model_0.extract_features(player_0_model_input)\n",
    "                player_1_features = self.model_1.extract_features(player_1_model_input)\n",
    "\n",
    "                player_0_latent_pi, player_0_latent_vf = self.model_0.mlp_extractor(player_0_features)\n",
    "                player_1_latent_pi, player_1_latent_vf = self.model_1.mlp_extractor(player_1_features)\n",
    "\n",
    "                player_0_distribution = self.model_0._get_action_dist_from_latent(player_0_latent_pi)\n",
    "                player_1_distribution = self.model_1._get_action_dist_from_latent(player_1_latent_pi)\n",
    "\n",
    "                player_0_log_prob = player_0_distribution.log_prob(player_0_action_distribution)\n",
    "                player_1_log_prob = player_1_distribution.log_prob(player_1_action_distribution)\n",
    "\n",
    "                player_0_value = self.model_0.value_net(player_0_latent_vf)\n",
    "                player_1_value = self.model_1.value_net(player_1_latent_vf)\n",
    "\n",
    "                player_0_entropy = player_0_distribution.entropy()\n",
    "                player_1_entropy = player_1_distribution.entropy()\n",
    "\n",
    "                obs_all, _, _, _, _ = self.env.step({\n",
    "                    \"player_0\": player_0_action.detach(),\n",
    "                    \"player_1\": player_1_action.detach()\n",
    "                })\n",
    "\n",
    "                player_0_model_input = self.prepare_model_input(obs_all[\"player_0\"], 0)\n",
    "                player_1_model_input = self.prepare_model_input(obs_all[\"player_1\"], 1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Compute value for the last timestep\n",
    "                    player_0_new_value = self.model_0.predict_values(player_0_model_input)  # type: ignore[arg-type]\n",
    "                    player_1_new_value = self.model_1.predict_values(player_1_model_input)\n",
    "\n",
    "                # player_0_delta = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                # player_0_advantage = player_0_delta + self.gamma * self.gae_lambda\n",
    "                player_0_advantage = player_0_reward + self.gamma * player_0_new_value - player_0_value\n",
    "                player_0_advantage = player_0_advantage.detach()\n",
    "                # player_0_advantage = torch.tensor(player_0_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_0_return = player_0_advantage + player_0_value\n",
    "\n",
    "                # player_1_delta = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                # player_1_advantage = player_1_delta + self.gamma * self.gae_lambda\n",
    "                player_1_advantage = player_1_reward + self.gamma * player_1_new_value - player_1_value\n",
    "                player_1_advantage = player_1_advantage.detach()\n",
    "                # player_1_advantage = torch.tensor(player_1_advantage, dtype=torch.float32, device=\"cuda\")\n",
    "                player_1_return = player_1_advantage + player_1_value\n",
    "\n",
    "                player_0_policy_loss_1 = player_0_advantage\n",
    "                player_0_policy_loss_2 = player_0_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_0_policy_loss = -torch.min(player_0_policy_loss_1, player_0_policy_loss_2).mean()\n",
    "\n",
    "                player_1_policy_loss_1 = player_1_advantage\n",
    "                player_1_policy_loss_2 = player_1_advantage * torch.clamp(torch.tensor(1), 1 - self.clip_range, 1 + self.clip_range)\n",
    "                player_1_policy_loss = -torch.min(player_1_policy_loss_1, player_1_policy_loss_2).mean()\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    player_0_values_pred = player_0_new_value\n",
    "                    player_1_values_pred = player_1_new_value\n",
    "                else:\n",
    "                    player_0_values_pred = player_0_value + torch.clamp(player_0_new_value - player_0_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "                    player_1_values_pred = player_1_value + torch.clamp(player_1_new_value - player_1_value, -self.clip_range_vf, self.clip_range_vf)\n",
    "\n",
    "                player_0_value_loss = F.mse_loss(player_0_return, player_0_values_pred)\n",
    "                player_1_value_loss = F.mse_loss(player_1_return, player_1_values_pred)\n",
    "\n",
    "                player_0_entropy_loss = -torch.mean(-player_0_entropy)\n",
    "                player_1_entropy_loss = -torch.mean(-player_1_entropy)\n",
    "\n",
    "                player_0_loss = player_0_policy_loss + self.ent_coef * player_0_entropy_loss + self.vf_coef * player_0_value_loss\n",
    "                player_1_loss = player_1_policy_loss + self.ent_coef * player_1_entropy_loss + self.vf_coef * player_1_value_loss\n",
    "\n",
    "                self.optimizer_0.zero_grad()\n",
    "                player_0_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_0.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_0.step()\n",
    "                self.scheduler_0.step(player_0_loss.item())\n",
    "\n",
    "                self.optimizer_1.zero_grad()\n",
    "                player_1_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model_1.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_1.step()\n",
    "                self.scheduler_1.step(player_1_loss.item())\n",
    "\n",
    "                if match_number >= 5 and current_match_step == 100:\n",
    "                    game_ended = True\n",
    "                    print(\"Game ended.\")\n",
    "\n",
    "                if current_match_step == 100:\n",
    "                    match_number += 1\n",
    "\n",
    "            if victor == \"player_0\":\n",
    "                self.synchronize_models(self.model_0, self.model_1)\n",
    "            elif victor == \"player_1\":\n",
    "                self.synchronize_models(self.model_1, self.model_0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "                \n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def prepare_model_input(self, obs, my_team_id):\n",
    "        enemy_team_id = 1 - my_team_id\n",
    "\n",
    "        self.spawn_location = np.array([[-1, -1], [-1, -1]], dtype=np.int32)\n",
    "\n",
    "        \n",
    "\n",
    "        model_input = {\n",
    "            \"enemy_energies\": obs[\"units\"][\"energy\"][enemy_team_id],\n",
    "            \"enemy_positions\": obs[\"units\"][\"position\"][enemy_team_id],\n",
    "            \"enemy_spawn_location\": self.spawn_location[enemy_team_id],\n",
    "            \"enemy_visible_mask\": obs[\"units_mask\"][enemy_team_id],\n",
    "            \"map_explored_status\": self.map_explored_status[my_team_id],\n",
    "            \"map_features_energy\": obs[\"map_features\"][\"energy\"],\n",
    "            \"map_features_tile_type\": obs[\"map_features\"][\"tile_type\"],\n",
    "            \"match_steps\": np.array([obs[\"match_steps\"]]),\n",
    "            \"my_spawn_location\": self.spawn_location[my_team_id],\n",
    "            \"relic_nodes\": obs[\"relic_nodes\"],\n",
    "            \"relic_nodes_mask\": obs[\"relic_nodes_mask\"],\n",
    "            \"sensor_mask\": obs[\"sensor_mask\"],\n",
    "            \"steps\": np.array([obs[\"steps\"]]),\n",
    "            \"team_id\": np.array([my_team_id]),\n",
    "            \"team_points\": obs[\"team_points\"],\n",
    "            \"team_wins\": obs[\"team_wins\"],\n",
    "            \"unit_active_mask\": obs[\"units_mask\"][my_team_id],\n",
    "            \"unit_energies\": obs[\"units\"][\"energy\"][my_team_id],\n",
    "            \"unit_move_cost\": np.array([self.env_cfg[\"unit_move_cost\"]]),\n",
    "            \"unit_positions\": obs[\"units\"][\"position\"][my_team_id],\n",
    "            \"unit_sap_cost\": np.array([self.env_cfg[\"unit_sap_cost\"]]),\n",
    "            \"unit_sap_range\": np.array([self.env_cfg[\"unit_sap_range\"]]),\n",
    "            \"unit_sensor_range\": np.array([self.env_cfg[\"unit_sensor_range\"]]),\n",
    "        }\n",
    "\n",
    "        model_input = {k: torch.tensor(np.expand_dims(v, axis=0), dtype=torch.int32, device=\"cuda\") for k, v in model_input.items()}\n",
    "\n",
    "        return model_input\n",
    "    \n",
    "    def synchronize_models(self, winner_model, loser_model):\n",
    "        with torch.no_grad():\n",
    "            for p1, p2 in zip(winner_model.parameters(), loser_model.parameters()):\n",
    "                p2.data.copy_(p1.data)\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainPPO(model_0, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor = model.policy.extract_features(obs)\n",
    "obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {\n",
    "    \"enemy_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"enemy_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"enemy_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"enemy_visible_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"map_explored_status\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_energy\": np.random.randint(-7, 10, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"map_features_tile_type\": np.random.randint(-1, 3, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"match_steps\": np.random.randint(0, 101, size=(1, 1,), dtype=np.int32),\n",
    "    \"my_spawn_location\": np.random.randint(-1, 24, size=(1, 2,), dtype=np.int32),\n",
    "    \"relic_nodes\": np.random.randint(-1, 24, size=(1, 6, 2), dtype=np.int32),\n",
    "    \"relic_nodes_mask\": np.random.randint(0, 2, size=(1, 6,), dtype=np.int32),\n",
    "    \"sensor_mask\": np.random.randint(0, 2, size=(1, 24, 24), dtype=np.int32),\n",
    "    \"steps\": np.random.randint(0, 506, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_id\": np.random.randint(0, 2, size=(1, 1,), dtype=np.int32),\n",
    "    \"team_points\": np.random.randint(0, 2501, size=(1, 2,), dtype=np.int32),\n",
    "    \"team_wins\": np.random.randint(0, 4, size=(1, 2,), dtype=np.int32),\n",
    "    \"unit_active_mask\": np.random.randint(0, 2, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_energies\": np.random.randint(-800, 401, size=(1, 16,), dtype=np.int32),\n",
    "    \"unit_move_cost\": np.random.randint(1, 6, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_positions\": np.random.randint(-1, 24, size=(1, 16, 2), dtype=np.int32),\n",
    "    \"unit_sap_cost\": np.random.randint(30, 51, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sap_range\": np.random.randint(3, 8, size=(1, 1, ), dtype=np.int32),\n",
    "    \"unit_sensor_range\": np.random.randint(2, 5, size=(1, 1, ), dtype=np.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {k: torch.tensor(v, dtype=torch.float32, device=\"cuda\") for k, v in obs.items()}\n",
    "\n",
    "# Convert observation to tensor and check shape\n",
    "obs_tensor = model.policy.extract_features(obs)\n",
    "print(f\"Extracted Feature Shape: {obs_tensor.shape}\")  # Expected: (batch_size, 2464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.mlp_extractor = torch.compile(model.policy.mlp_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_distribution, value, log = model.policy.forward(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = action_distribution.reshape(-1, 16, 3)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade luxai-s3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
