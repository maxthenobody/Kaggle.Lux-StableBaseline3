{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘agent’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir agent\n",
    "!cp -r ../Data/lux-ai-season-3/lux agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: luxai-s3 in /home/max1024/venvs/lux/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: jax in /home/max1024/venvs/lux/lib/python3.12/site-packages (from luxai-s3) (0.5.0)\n",
      "Requirement already satisfied: gymnax==0.0.8 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from luxai-s3) (0.0.8)\n",
      "Requirement already satisfied: tyro in /home/max1024/venvs/lux/lib/python3.12/site-packages (from luxai-s3) (0.9.13)\n",
      "Requirement already satisfied: jaxlib in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (0.5.0)\n",
      "Requirement already satisfied: chex in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (0.1.88)\n",
      "Requirement already satisfied: flax in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (0.10.2)\n",
      "Requirement already satisfied: pyyaml in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (6.0.2)\n",
      "Requirement already satisfied: gym>=0.26 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (0.26.2)\n",
      "Requirement already satisfied: gymnasium in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnax==0.0.8->luxai-s3) (0.13.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from jax->luxai-s3) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from jax->luxai-s3) (2.2.2)\n",
      "Requirement already satisfied: opt_einsum in /home/max1024/venvs/lux/lib/python3.12/site-packages (from jax->luxai-s3) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from jax->luxai-s3) (1.15.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from tyro->luxai-s3) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from tyro->luxai-s3) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from tyro->luxai-s3) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from tyro->luxai-s3) (4.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from tyro->luxai-s3) (4.12.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gym>=0.26->gymnax==0.0.8->luxai-s3) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gym>=0.26->gymnax==0.0.8->luxai-s3) (0.0.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from rich>=11.1.0->tyro->luxai-s3) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from rich>=11.1.0->tyro->luxai-s3) (2.19.1)\n",
      "Requirement already satisfied: absl-py>=0.9.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from chex->gymnax==0.0.8->luxai-s3) (2.1.0)\n",
      "Requirement already satisfied: setuptools in /home/max1024/venvs/lux/lib/python3.12/site-packages (from chex->gymnax==0.0.8->luxai-s3) (75.8.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from chex->gymnax==0.0.8->luxai-s3) (1.0.0)\n",
      "Requirement already satisfied: msgpack in /home/max1024/venvs/lux/lib/python3.12/site-packages (from flax->gymnax==0.0.8->luxai-s3) (1.1.0)\n",
      "Requirement already satisfied: optax in /home/max1024/venvs/lux/lib/python3.12/site-packages (from flax->gymnax==0.0.8->luxai-s3) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/max1024/venvs/lux/lib/python3.12/site-packages (from flax->gymnax==0.0.8->luxai-s3) (0.11.2)\n",
      "Requirement already satisfied: tensorstore in /home/max1024/venvs/lux/lib/python3.12/site-packages (from flax->gymnax==0.0.8->luxai-s3) (0.1.71)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from gymnasium->gymnax==0.0.8->luxai-s3) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from matplotlib->gymnax==0.0.8->luxai-s3) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from seaborn->gymnax==0.0.8->luxai-s3) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->luxai-s3) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from pandas>=1.2->seaborn->gymnax==0.0.8->luxai-s3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from pandas>=1.2->seaborn->gymnax==0.0.8->luxai-s3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->gymnax==0.0.8->luxai-s3) (1.17.0)\n",
      "Requirement already satisfied: etils[epy] in /home/max1024/venvs/lux/lib/python3.12/site-packages (from optax->flax->gymnax==0.0.8->luxai-s3) (1.11.0)\n",
      "Requirement already satisfied: nest_asyncio in /home/max1024/venvs/lux/lib/python3.12/site-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /home/max1024/venvs/lux/lib/python3.12/site-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (5.29.3)\n",
      "Requirement already satisfied: humanize in /home/max1024/venvs/lux/lib/python3.12/site-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (4.11.0)\n",
      "Requirement already satisfied: simplejson>=3.16.0 in /home/max1024/venvs/lux/lib/python3.12/site-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (3.19.3)\n",
      "Requirement already satisfied: fsspec in /home/max1024/venvs/lux/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/max1024/venvs/lux/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (6.5.2)\n",
      "Requirement already satisfied: zipp in /home/max1024/venvs/lux/lib/python3.12/site-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai-s3) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade luxai-s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent/agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent/agent.py\n",
    "\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import sys\n",
    "from typing import List, Tuple, Dict\n",
    "import traceback  # 상단에 추가\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class NodeType(Enum):\n",
    "    unknown = -1\n",
    "    empty = 0\n",
    "    asteroid = 1\n",
    "    nebula = 2\n",
    "\n",
    "def to_numpy(x):\n",
    "    \"\"\"JAX 배열 또는 일반 배열을 numpy 배열로 변환\"\"\"\n",
    "    if hasattr(x, 'device_buffer'):  # JAX 배열인 경우\n",
    "        return np.array(x)\n",
    "    return np.array(x)\n",
    "\n",
    "def safe_squeeze(x):\n",
    "    \"\"\"안전하게 차원 축소\"\"\"\n",
    "    x = to_numpy(x)\n",
    "    if x.ndim > 1:\n",
    "        return np.squeeze(x)\n",
    "    return x\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.type = NodeType.unknown\n",
    "        self.energy = None\n",
    "        self.is_visible = False\n",
    "        self._relic = False\n",
    "        self._reward = False\n",
    "        self._explored_for_relic = False\n",
    "        self._explored_for_reward = False\n",
    "        \n",
    "    @property\n",
    "    def coordinates(self):\n",
    "        return (self.x, self.y)\n",
    "        \n",
    "    @property\n",
    "    def is_walkable(self):\n",
    "        return self.type != NodeType.asteroid\n",
    "\n",
    "class Space:\n",
    "    def __init__(self, size=24):\n",
    "        self.size = size\n",
    "        self._nodes = [[Node(x, y) for x in range(size)] for y in range(size)]\n",
    "        self.relic_nodes = set()\n",
    "        self.reward_nodes = set()\n",
    "        \n",
    "    def get_node(self, x: int, y: int) -> Node:\n",
    "        if 0 <= x < self.size and 0 <= y < self.size:\n",
    "            return self._nodes[y][x]\n",
    "        return None\n",
    "        \n",
    "    def update(self, obs):\n",
    "        if isinstance(obs, dict):\n",
    "            sensor_mask = obs[\"sensor_mask\"]\n",
    "            map_features = obs[\"map_features\"]\n",
    "        else:\n",
    "            sensor_mask = obs.sensor_mask\n",
    "            map_features = obs.map_features\n",
    "            \n",
    "        # Update tiles and energy\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                node = self.get_node(x, y)\n",
    "                if sensor_mask[x, y]:\n",
    "                    node.is_visible = True\n",
    "                    if isinstance(map_features, dict):\n",
    "                        tile_type = int(map_features[\"tile_type\"][x, y])\n",
    "                    else:\n",
    "                        tile_type = int(map_features.tile_type[x, y])\n",
    "                    node.type = NodeType(tile_type)\n",
    "                else:\n",
    "                    node.is_visible = False\n",
    "\n",
    "class Ship:\n",
    "    def __init__(self, unit_id: int, team_id: int, fleet=None):\n",
    "        self.unit_id = unit_id\n",
    "        self.team_id = team_id\n",
    "        self.fleet = fleet\n",
    "        self.energy = 0\n",
    "        self.position = None\n",
    "        self.task = None\n",
    "        self.sub_task = None\n",
    "        self.target = None\n",
    "        self.is_active = False\n",
    "        \n",
    "    def update(self, position, energy, is_active):\n",
    "        self.position = position\n",
    "        self.energy = energy\n",
    "        self.is_active = is_active\n",
    "        \n",
    "    def clean(self):\n",
    "        self.energy = 0\n",
    "        self.position = None\n",
    "        self.task = None\n",
    "        self.sub_task = None\n",
    "        self.target = None\n",
    "        self.is_active = False\n",
    "\n",
    "class ShipMemory:\n",
    "    def __init__(self, ship_id, device=torch.device(device)):\n",
    "        self.ship_id = ship_id\n",
    "        self.device = device\n",
    "        self.clear_memory()\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.action_types = []\n",
    "        self.detail_actions = []\n",
    "        self.type_logprobs = []\n",
    "        self.detail_logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.values = []\n",
    "        self.returns = None\n",
    "        self.advantages = None\n",
    "        self.steps_collected = 0\n",
    "\n",
    "class FleetMemory:\n",
    "    def __init__(self, max_ships=16, device=torch.device(device)):\n",
    "        self.device = device\n",
    "        self.ship_memories = {i: ShipMemory(i, device) for i in range(max_ships)}\n",
    "        self.steps_collected = 0\n",
    "    \n",
    "    def push_ship_transition(self, ship_id, state=None, action_type=None, detail_action=None, \n",
    "                           type_logprob=None, detail_logprob=None, value=None, reward=None, \n",
    "                           is_terminal=None):\n",
    "        memory = self.ship_memories[ship_id]\n",
    "        \n",
    "        # Skip if ship was already terminated\n",
    "        if memory.steps_collected > 0 and memory.is_terminals and memory.is_terminals[-1]:\n",
    "            return\n",
    "            \n",
    "        # Case 1: Pushing full transition (including state)\n",
    "        if state is not None:\n",
    "            memory.states.append(state)\n",
    "            memory.steps_collected += 1\n",
    "            self.steps_collected += 1\n",
    "            \n",
    "            if action_type is not None:\n",
    "                memory.action_types.append(action_type)\n",
    "            if detail_action is not None:\n",
    "                memory.detail_actions.append(detail_action)\n",
    "            if type_logprob is not None:\n",
    "                memory.type_logprobs.append(type_logprob)\n",
    "            if detail_logprob is not None:\n",
    "                memory.detail_logprobs.append(detail_logprob)\n",
    "            if value is not None:\n",
    "                memory.values.append(value)\n",
    "                \n",
    "        # Case 2: Just updating last transition's reward/terminal\n",
    "        elif memory.steps_collected > 0:\n",
    "            if reward is not None:\n",
    "                # Update or append reward based on current length\n",
    "                if len(memory.rewards) < memory.steps_collected:\n",
    "                    memory.rewards.append(reward)\n",
    "                else:\n",
    "                    memory.rewards[-1] = reward\n",
    "                    \n",
    "            if is_terminal is not None:\n",
    "                # Update or append terminal flag based on current length\n",
    "                if len(memory.is_terminals) < memory.steps_collected:\n",
    "                    memory.is_terminals.append(is_terminal)\n",
    "                else:\n",
    "                    memory.is_terminals[-1] = is_terminal\n",
    "            \n",
    "    def update_terminal_flag(self, ship_id):\n",
    "        memory = self.ship_memories[ship_id]\n",
    "        if memory.steps_collected > 0 and memory.is_terminals and not memory.is_terminals[-1]:\n",
    "            memory.is_terminals[-1] = True\n",
    "            # print(f\"\\nDEBUG - Updated terminal flag for Ship {ship_id}\")\n",
    "            # print(f\"Steps: {memory.steps_collected}\")\n",
    "            # print(f\"Terminals: {len(memory.is_terminals)}\")\n",
    "            \n",
    "    def handle_ship_death(self, ship_id):\n",
    "        memory = self.ship_memories[ship_id]\n",
    "        if memory.steps_collected > 0:\n",
    "            if not memory.is_terminals or not memory.is_terminals[-1]:\n",
    "                self.update_terminal_flag(ship_id)\n",
    "            # print(f\"\\nDEBUG - Ship {ship_id} death handled\")\n",
    "            # print(f\"Final steps: {memory.steps_collected}\")\n",
    "            # print(f\"Final terminals: {len(memory.is_terminals)}\")\n",
    "\n",
    "    def clean_invalid_data(self):\n",
    "        for ship_id, memory in self.ship_memories.items():\n",
    "            if memory.steps_collected == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get minimum valid length\n",
    "            lengths = [\n",
    "                len(memory.states),\n",
    "                len(memory.action_types) if memory.action_types else float('inf'),\n",
    "                len(memory.detail_actions) if memory.detail_actions else float('inf'),\n",
    "                len(memory.type_logprobs) if memory.type_logprobs else float('inf'),\n",
    "                len(memory.detail_logprobs) if memory.detail_logprobs else float('inf'),\n",
    "                len(memory.values) if memory.values else float('inf'),\n",
    "                len(memory.rewards) if memory.rewards else float('inf'),\n",
    "                len(memory.is_terminals) if memory.is_terminals else float('inf')\n",
    "            ]\n",
    "            \n",
    "            min_length = min(length for length in lengths if length > 0)\n",
    "            \n",
    "            if min_length < memory.steps_collected:\n",
    "                memory.states = memory.states[:min_length]\n",
    "                if memory.action_types: memory.action_types = memory.action_types[:min_length]\n",
    "                if memory.detail_actions: memory.detail_actions = memory.detail_actions[:min_length]\n",
    "                if memory.type_logprobs: memory.type_logprobs = memory.type_logprobs[:min_length]\n",
    "                if memory.detail_logprobs: memory.detail_logprobs = memory.detail_logprobs[:min_length]\n",
    "                if memory.values: memory.values = memory.values[:min_length]\n",
    "                if memory.rewards: memory.rewards = memory.rewards[:min_length]\n",
    "                if memory.is_terminals: memory.is_terminals = memory.is_terminals[:min_length]\n",
    "                memory.steps_collected = min_length\n",
    "                print(f\"\\nDEBUG - Cleaned data for Ship {ship_id}\")\n",
    "                print(f\"New steps: {memory.steps_collected}\")\n",
    "                \n",
    "    def clear_all_memories(self):\n",
    "        for memory in self.ship_memories.values():\n",
    "            memory.clear_memory()\n",
    "        self.steps_collected = 0\n",
    "        \n",
    "    def debug_ship_status(self, ship_id, is_active):\n",
    "        memory = self.ship_memories[ship_id]\n",
    "        if memory.steps_collected > 0:\n",
    "            # print(f\"\\nDEBUG - Ship {ship_id} Status Update:\")\n",
    "            # print(f\"Active: {is_active}\")\n",
    "            # print(f\"Steps: {memory.steps_collected}\")\n",
    "            terminal_count = sum(1 for t in memory.is_terminals if t) if memory.is_terminals else 0\n",
    "            # print(f\"Terminal states: {terminal_count}/{len(memory.is_terminals) if memory.is_terminals else 0}\")\n",
    "            if not is_active:\n",
    "                self.handle_ship_death(ship_id)\n",
    "                \n",
    "class Fleet:\n",
    "    def __init__(self, team_id: int, max_units: int = 16):\n",
    "        self.team_id = team_id\n",
    "        self.ships = [Ship(i, team_id, self) for i in range(max_units)]\n",
    "        self.points = 0\n",
    "        self.memory = None\n",
    "        \n",
    "    def update(self, obs):\n",
    "        if isinstance(obs, dict):\n",
    "            self.points = float(obs[\"team_points\"][self.team_id])\n",
    "            units_mask = np.array(obs[\"units_mask\"][self.team_id])\n",
    "            positions = np.array(obs[\"units\"][\"position\"][self.team_id])\n",
    "            energies = np.array(obs[\"units\"][\"energy\"][self.team_id])\n",
    "        else:\n",
    "            # JAX 배열 처리\n",
    "            self.points = float(np.array(obs.team_points[self.team_id]))\n",
    "            units_mask = np.array(obs.units_mask[self.team_id])\n",
    "            positions = np.array(obs.units.position[self.team_id])\n",
    "            energies = np.array(obs.units.energy[self.team_id])\n",
    "        \n",
    "        # 배열 차원 확인 및 필요시 squeeze\n",
    "        if units_mask.ndim > 1:\n",
    "            units_mask = np.squeeze(units_mask)\n",
    "        if positions.ndim > 2:\n",
    "            positions = np.squeeze(positions)\n",
    "        if energies.ndim > 1:\n",
    "            energies = np.squeeze(energies)\n",
    "        \n",
    "        # 각 유닛 업데이트\n",
    "        for ship, active, pos, energy in zip(\n",
    "            self.ships,\n",
    "            units_mask,\n",
    "            positions,\n",
    "            energies\n",
    "        ):\n",
    "            was_active = ship.is_active\n",
    "            if active:\n",
    "                ship.update(pos, float(energy), True)\n",
    "            else:\n",
    "                if was_active:  # Ship이 방금 죽은 경우\n",
    "                    self.memory.handle_ship_death(ship.unit_id)\n",
    "                ship.clean()\n",
    "            self.memory.debug_ship_status(ship.unit_id, active)\n",
    "    \n",
    "    @property\n",
    "    def active_ships(self):\n",
    "        return [ship for ship in self.ships if ship.is_active]\n",
    "\n",
    "def manhattan_distance(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "def nearby_positions(x, y, distance):\n",
    "    positions = []\n",
    "    for dx in range(-distance, distance + 1):\n",
    "        for dy in range(-distance, distance + 1):\n",
    "            if abs(dx) + abs(dy) <= distance:\n",
    "                pos = (x + dx, y + dy)\n",
    "                positions.append(pos)\n",
    "    return positions\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, device=torch.device(device)):\n",
    "        self.device = device\n",
    "        self.states = []\n",
    "        self.action_types = []\n",
    "        self.detail_actions = []\n",
    "        self.type_logprobs = []\n",
    "        self.detail_logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.values = []\n",
    "        \n",
    "        self.returns = None\n",
    "        self.advantages = None\n",
    "        self.steps_collected = 0\n",
    "        \n",
    "    def push(self, reward, is_terminal):\n",
    "        try:\n",
    "            self.rewards.append(float(reward))\n",
    "            self.is_terminals.append(bool(is_terminal))\n",
    "            self.steps_collected += 1\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG - Error in Memory.push: {e}\")\n",
    "            \n",
    "    def clear_memory(self):\n",
    "        try:\n",
    "            del self.states[:]\n",
    "            del self.action_types[:]\n",
    "            del self.detail_actions[:]\n",
    "            del self.type_logprobs[:]\n",
    "            del self.detail_logprobs[:]\n",
    "            del self.rewards[:]\n",
    "            del self.is_terminals[:]\n",
    "            del self.values[:]\n",
    "            self.returns = None\n",
    "            self.advantages = None\n",
    "            self.steps_collected = 0\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG - Error in Memory.clear_memory: {e}\")\n",
    "\n",
    "    def safe_to_device(self, tensor_list):\n",
    "        \"\"\"텐서 리스트를 안전하게 device로 이동\"\"\"\n",
    "        result = []\n",
    "        for tensor in tensor_list:\n",
    "            if tensor is None:\n",
    "                result.append(torch.zeros(1, device=self.device))\n",
    "            elif isinstance(tensor, torch.Tensor):\n",
    "                # 이미 올바른 device에 있다면 그대로 사용\n",
    "                if tensor.device == self.device:\n",
    "                    result.append(tensor)\n",
    "                else:\n",
    "                    result.append(tensor.to(self.device))\n",
    "            else:\n",
    "                try:\n",
    "                    # tensor가 아닌 경우 변환 시도\n",
    "                    result.append(torch.tensor([tensor], device=self.device))\n",
    "                except:\n",
    "                    # 변환 실패 시 0 텐서 사용\n",
    "                    print(f\"DEBUG - Failed to convert to tensor: {tensor}\")\n",
    "                    result.append(torch.zeros(1, device=self.device))\n",
    "        return result\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"안전하게 배치 반환\"\"\"\n",
    "        try:\n",
    "            # 모든 리스트에 대해 safe_to_device 적용\n",
    "            states = self.safe_to_device(self.states)\n",
    "            action_types = self.safe_to_device(self.action_types)\n",
    "            detail_actions = self.safe_to_device(self.detail_actions)\n",
    "            type_logprobs = self.safe_to_device(self.type_logprobs)\n",
    "            detail_logprobs = self.safe_to_device(self.detail_logprobs)\n",
    "            values = self.safe_to_device(self.values)\n",
    "            \n",
    "            # non-tensor 데이터는 그대로 사용\n",
    "            rewards = self.rewards\n",
    "            is_terminals = self.is_terminals\n",
    "            \n",
    "            return {\n",
    "                'states': states,\n",
    "                'action_types': action_types,\n",
    "                'detail_actions': detail_actions,\n",
    "                'type_logprobs': type_logprobs,\n",
    "                'detail_logprobs': detail_logprobs,\n",
    "                'rewards': rewards,\n",
    "                'is_terminals': is_terminals,\n",
    "                'values': values\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG - Error in Memory.get_batch: {e}\")\n",
    "            print(f\"DEBUG - Memory state lengths: states={len(self.states)}, \"\n",
    "                  f\"action_types={len(self.action_types)}, \"\n",
    "                  f\"detail_actions={len(self.detail_actions)}\")\n",
    "            \n",
    "            # 에러 발생 시 안전한 기본값 반환\n",
    "            empty_tensor = torch.zeros(1, device=self.device)\n",
    "            return {\n",
    "                'states': [empty_tensor],\n",
    "                'action_types': [empty_tensor],\n",
    "                'detail_actions': [empty_tensor],\n",
    "                'type_logprobs': [empty_tensor],\n",
    "                'detail_logprobs': [empty_tensor],\n",
    "                'rewards': [0.0],\n",
    "                'is_terminals': [True],\n",
    "                'values': [empty_tensor]\n",
    "            }\n",
    "        \n",
    "    def print_debug_info(self):\n",
    "        \"\"\"현재 메모리 상태 출력\"\"\"\n",
    "        print(\"\\nDEBUG - Memory Status:\")\n",
    "        print(f\"Steps collected: {self.steps_collected}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"List lengths:\")\n",
    "        print(f\"  states: {len(self.states)}\")\n",
    "        print(f\"  action_types: {len(self.action_types)}\")\n",
    "        print(f\"  detail_actions: {len(self.detail_actions)}\")\n",
    "        print(f\"  type_logprobs: {len(self.type_logprobs)}\")\n",
    "        print(f\"  detail_logprobs: {len(self.detail_logprobs)}\")\n",
    "        print(f\"  values: {len(self.values)}\")\n",
    "        print(f\"  rewards: {len(self.rewards)}\")\n",
    "        print(f\"  is_terminals: {len(self.is_terminals)}\")\n",
    "        \n",
    "        # 샘플 데이터 출력\n",
    "        if len(self.states) > 0:\n",
    "            print(\"\\nLatest data:\")\n",
    "            print(f\"  state: {type(self.states[-1])}, device: {self.states[-1].device if isinstance(self.states[-1], torch.Tensor) else 'N/A'}\")\n",
    "            print(f\"  action_type: {type(self.action_types[-1])}, device: {self.action_types[-1].device if isinstance(self.action_types[-1], torch.Tensor) else 'N/A'}\")\n",
    "            print(f\"  detail_action: {type(self.detail_actions[-1])}, device: {self.detail_actions[-1].device if isinstance(self.detail_actions[-1], torch.Tensor) else 'N/A'}\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, \n",
    "                 lr=0.0001,\n",
    "                 gamma=0.99,\n",
    "                 eps_clip=0.15,\n",
    "                 K_epochs=4,\n",
    "                 target_steps=2048,\n",
    "                 device=torch.device(device),\n",
    "                 entropy_coef=0.01,\n",
    "                 agent=None):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.device = device\n",
    "        self.target_steps = target_steps\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.policy = SimplifiedActorCritic(state_dim).to(self.device)\n",
    "        self.policy_old = SimplifiedActorCritic(state_dim).to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.cnn.parameters(), 'lr': lr * 0.1},\n",
    "            {'params': self.policy.base_network.parameters(), 'lr': lr},\n",
    "            {'params': self.policy.feature_combiner.parameters(), 'lr': lr},\n",
    "            {'params': self.policy.action_type.parameters(), 'lr': lr * 2.0},\n",
    "            {'params': self.policy.direction.parameters(), 'lr': lr * 2.0},\n",
    "            {'params': self.policy.value.parameters(), 'lr': lr}\n",
    "        ])\n",
    "\n",
    "    def should_update(self, memory):\n",
    "        \"\"\"메모리에 충분한 데이터가 쌓였는지 확인\"\"\"\n",
    "        return memory.steps_collected >= self.target_steps\n",
    "        \n",
    "    def collect_rollout(self, state, memory, ship_id, obs, ship):\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if not isinstance(state, torch.Tensor):\n",
    "                    state = torch.FloatTensor(state)\n",
    "                state = state.to(self.device)\n",
    "                \n",
    "                features = self.policy_old(state)\n",
    "                \n",
    "                # Pass agent's space instead of ship.fleet.space\n",
    "                final_action, action_type, direction, type_logprob, direction_logprob = \\\n",
    "                    self.policy_old.get_masked_action(features, obs, ship, self.agent.space)\n",
    "                \n",
    "                memory.push_ship_transition(\n",
    "                    ship_id=ship_id,\n",
    "                    state=state.clone(),\n",
    "                    action_type=action_type.clone(),\n",
    "                    detail_action=direction.clone(),\n",
    "                    type_logprob=type_logprob.clone(),\n",
    "                    detail_logprob=direction_logprob.clone(),\n",
    "                    value=self.policy_old.value(features).detach(),\n",
    "                    reward=None,\n",
    "                    is_terminal=False\n",
    "                )\n",
    "                \n",
    "                return final_action\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR in collect_rollout:\")\n",
    "            print(f\"Exception: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            memory.print_debug_info(ship_id)\n",
    "            return [0, 0, 0]\n",
    "            \n",
    "    def act(self, state, memory=None):\n",
    "        \"\"\"act 메서드\"\"\"\n",
    "        return self.collect_rollout(state, memory) if memory is not None else self.policy.act(state)\n",
    "        \n",
    "    def update(self, memory):\n",
    "        \"\"\"PPO 업데이트\"\"\"\n",
    "        if not hasattr(memory, 'returns') or not hasattr(memory, 'advantages'):\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        batch = memory.get_batch()\n",
    "        states = torch.stack([torch.FloatTensor(s) for s in batch['states']]).to(self.device)\n",
    "        action_types = torch.stack(batch['action_types']).to(self.device)\n",
    "        directions = torch.stack(batch['detail_actions']).to(self.device)\n",
    "        old_type_logprobs = torch.stack(batch['type_logprobs']).detach()\n",
    "        old_direction_logprobs = torch.stack(batch['detail_logprobs']).detach()\n",
    "        \n",
    "        returns = memory.returns.to(self.device)\n",
    "        advantages = memory.advantages.to(self.device)\n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        \n",
    "        for _ in range(self.K_epochs):\n",
    "            features = self.policy(states)\n",
    "            \n",
    "            # 행동 타입 선택\n",
    "            action_probs = self.policy.action_type(features)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            curr_type_logprobs = action_dist.log_prob(action_types)\n",
    "            \n",
    "            # 방향 선택\n",
    "            direction_probs = self.policy.direction(features)\n",
    "            direction_dist = Categorical(direction_probs)\n",
    "            curr_direction_logprobs = direction_dist.log_prob(directions)\n",
    "            \n",
    "            state_values = self.policy.value(features).squeeze()\n",
    "            \n",
    "            # 엔트로피 보너스 계산\n",
    "            entropy = (action_dist.entropy().mean() + direction_dist.entropy().mean()) / 2\n",
    "            \n",
    "            # Ratios\n",
    "            type_ratios = torch.exp(curr_type_logprobs - old_type_logprobs)\n",
    "            direction_ratios = torch.exp(curr_direction_logprobs - old_direction_logprobs)\n",
    "            \n",
    "            # Surrogate losses\n",
    "            surr1_type = type_ratios * advantages\n",
    "            surr2_type = torch.clamp(type_ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            surr1_direction = direction_ratios * advantages\n",
    "            surr2_direction = torch.clamp(direction_ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            # 최종 손실 계산 (엔트로피 보너스 포함)\n",
    "            action_loss = -torch.min(surr1_type, surr2_type).mean() - torch.min(surr1_direction, surr2_direction).mean()\n",
    "            value_loss = 0.5 * self.MseLoss(state_values, returns)\n",
    "            \n",
    "            loss = action_loss + value_loss - self.entropy_coef * entropy\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)  # 그래디언트 클리핑 추가\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_policy_loss += float(action_loss.detach())\n",
    "            total_value_loss += float(value_loss.detach())\n",
    "        \n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        return total_policy_loss / self.K_epochs, total_value_loss / self.K_epochs\n",
    "\n",
    "class SimplifiedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(SimplifiedActorCritic, self).__init__()\n",
    "        \n",
    "        # CNN for map features\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(576, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Base network for state features\n",
    "        self.base_network = nn.Sequential(\n",
    "            nn.Linear(26, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Feature combiner\n",
    "        self.feature_combiner = nn.Sequential(\n",
    "            nn.Linear(128 + 64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Action networks\n",
    "        self.action_type = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)  # Raw logits for Wait/Move/Combat\n",
    "        )\n",
    "        \n",
    "        self.direction = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5)  # Raw logits for directions\n",
    "        )\n",
    "        \n",
    "        # Value network\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "            \n",
    "        # Split state into map and other features\n",
    "        if state.dim() == 1:\n",
    "            map_state = state[:576].view(1, 1, 24, 24)\n",
    "            other_state = state[576:].unsqueeze(0)\n",
    "        else:\n",
    "            map_state = state[:, :576].view(-1, 1, 24, 24)\n",
    "            other_state = state[:, 576:]\n",
    "        \n",
    "        # Extract features\n",
    "        map_features = self.cnn(map_state)\n",
    "        base_features = self.base_network(other_state)\n",
    "        combined = self.feature_combiner(torch.cat([map_features, base_features], dim=-1))\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    def get_masked_action(self, features, obs, ship, space, return_logprobs=True):\n",
    "        \"\"\"Get masked actions and their log probabilities\"\"\"\n",
    "        action_type_mask, direction_mask = self.get_action_mask(obs, ship, space)\n",
    "        action_type_mask = action_type_mask.to(features.device)\n",
    "        direction_mask = direction_mask.to(features.device)\n",
    "        \n",
    "        # Action type selection with masking\n",
    "        action_logits = self.action_type(features)\n",
    "        masked_logits = action_logits * action_type_mask - 1e8 * (1 - action_type_mask)\n",
    "        action_probs = F.softmax(masked_logits, dim=-1)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action_type = action_dist.sample()\n",
    "        \n",
    "        # Direction selection with masking\n",
    "        direction = torch.zeros(1, device=features.device)\n",
    "        direction_logprob = torch.zeros(1, device=features.device)\n",
    "        \n",
    "        if action_type.item() in [1, 2]:  # Move or Combat\n",
    "            direction_logits = self.direction(features)\n",
    "            masked_dir_logits = direction_logits * direction_mask - 1e8 * (1 - direction_mask)\n",
    "            direction_probs = F.softmax(masked_dir_logits, dim=-1)\n",
    "            direction_dist = Categorical(direction_probs)\n",
    "            direction = direction_dist.sample()\n",
    "            if return_logprobs:\n",
    "                direction_logprob = direction_dist.log_prob(direction)\n",
    "        \n",
    "        # Convert to final action format\n",
    "        if action_type.item() == 0:  # Wait\n",
    "            final_action = [0, 0, 0]\n",
    "        elif action_type.item() == 1:  # Move\n",
    "            final_action = [direction.item(), 0, 0]\n",
    "        else:  # Combat\n",
    "            final_action = [0, direction.item(), 0]\n",
    "            \n",
    "        if return_logprobs:\n",
    "            return (\n",
    "                final_action,\n",
    "                action_type,\n",
    "                direction,\n",
    "                action_dist.log_prob(action_type),\n",
    "                direction_logprob\n",
    "            )\n",
    "        return final_action\n",
    "\n",
    "    def get_action_mask(self, obs, ship, space):\n",
    "        \"\"\"Generate action masks based on game state\"\"\"\n",
    "        action_type_mask = torch.ones(3)  # [Wait, Move, Combat]\n",
    "        direction_mask = torch.ones(5)    # [Stay, Up, Right, Down, Left]\n",
    "        \n",
    "        # Energy constraints\n",
    "        if ship.energy < 10:\n",
    "            action_type_mask[1:] = 0  # Disable Move and Combat\n",
    "            direction_mask[1:] = 0    # Disable all movement\n",
    "        \n",
    "        # Combat availability \n",
    "        has_enemy_nearby = False\n",
    "        for dx in [-1, 0, 1]:\n",
    "            for dy in [-1, 0, 1]:\n",
    "                x, y = ship.position[0] + dx, ship.position[1] + dy\n",
    "                if 0 <= x < 24 and 0 <= y < 24:\n",
    "                    if isinstance(obs, dict):\n",
    "                        enemy_units = obs[\"units\"][\"position\"][1 - ship.team_id]\n",
    "                    else:\n",
    "                        enemy_units = obs.units.position[1 - ship.team_id]\n",
    "                    for enemy_pos in enemy_units:\n",
    "                        if enemy_pos[0] == x and enemy_pos[1] == y:\n",
    "                            has_enemy_nearby = True\n",
    "                            break\n",
    "        if not has_enemy_nearby:\n",
    "            action_type_mask[2] = 0  # Disable Combat\n",
    "        \n",
    "        # Movement constraints\n",
    "        directions = [(0,0), (0,1), (1,0), (0,-1), (-1,0)]\n",
    "        for i, (dx, dy) in enumerate(directions):\n",
    "            x, y = ship.position[0] + dx, ship.position[1] + dy\n",
    "            if not (0 <= x < 24 and 0 <= y < 24):\n",
    "                direction_mask[i] = 0\n",
    "                continue\n",
    "            \n",
    "            node = space.get_node(x, y)\n",
    "            if node and node.type == NodeType.asteroid:\n",
    "                direction_mask[i] = 0\n",
    "        \n",
    "        return action_type_mask, direction_mask\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, player: str, env_cfg=None, device=torch.device(device), train_mode=False, target_steps=2048) -> None:\n",
    "        # Basic parameters\n",
    "        self.player = player\n",
    "        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.env_cfg = env_cfg\n",
    "        self.device = device\n",
    "        self.train_mode = train_mode\n",
    "        self.prev_team_points = None\n",
    "\n",
    "        # State dimensions\n",
    "        map_dim = 576\n",
    "        base_dim = 8\n",
    "        task_dim = 12\n",
    "        target_dim = 3\n",
    "        sub_task_dim = 3\n",
    "        self.state_dim = map_dim + base_dim + task_dim + sub_task_dim + target_dim\n",
    "\n",
    "        # Initialize components\n",
    "        self.memory = FleetMemory(max_ships=16, device=device)\n",
    "        self.space = Space()\n",
    "        self.fleet = Fleet(self.team_id)\n",
    "        self.fleet.memory = self.memory\n",
    "\n",
    "        # Initialize PPO\n",
    "        self.ppo = PPO(\n",
    "            state_dim=self.state_dim,\n",
    "            lr=0.0002,\n",
    "            gamma=0.99,\n",
    "            eps_clip=0.15,\n",
    "            K_epochs=4,\n",
    "            device=device,\n",
    "            target_steps=2048,\n",
    "            entropy_coef=0.01,\n",
    "            agent=self  # Add reference to agent\n",
    "        )\n",
    "\n",
    "        # Load pre-trained model if in inference mode\n",
    "        if not self.train_mode:\n",
    "            try:\n",
    "                checkpoint = torch.load('checkpoint_episode_0000.pth', map_location=self.device)\n",
    "                self.ppo.policy.load_state_dict(checkpoint['agent0']['model'])\n",
    "                self.ppo.policy.eval()\n",
    "                print(\"Loaded pre-trained model for inference.\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(\"No pre-trained model found or error:\", e)\n",
    "    \n",
    "    def _process_state(self, obs, ship):\n",
    "        # 맵 상태를 하나의 24x24 배열로 표현\n",
    "        map_state = np.zeros((24, 24), dtype=np.float32)\n",
    "        for x in range(24):\n",
    "            for y in range(24):\n",
    "                node = self.space.get_node(x, y)\n",
    "                if node:\n",
    "                    value = 0.0\n",
    "                    if node.is_visible:\n",
    "                        value += 1.0\n",
    "                    if node._explored_for_relic:\n",
    "                        value += 2.0\n",
    "                    if node._explored_for_reward:\n",
    "                        value += 4.0\n",
    "                    if node.type == NodeType.asteroid:\n",
    "                        value += 8.0\n",
    "                    elif node.type == NodeType.nebula:\n",
    "                        value += 16.0\n",
    "                    map_state[x][y] = value/31.0  # 정규화\n",
    "        \n",
    "        # 1차원으로 변환\n",
    "        map_state = map_state.flatten()  # 576 크기\n",
    "        \n",
    "        # 기본 상태 정보\n",
    "        base_state = [\n",
    "            ship.position[0] / 24.0,\n",
    "            ship.position[1] / 24.0,\n",
    "            ship.energy / 100.0,\n",
    "            float(ship.is_active),\n",
    "            float(obs[\"team_points\"][self.team_id] if isinstance(obs, dict) else obs.team_points[self.team_id]) / 1000.0,\n",
    "            float(obs[\"team_points\"][1 - self.team_id] if isinstance(obs, dict) else obs.team_points[1 - self.team_id]) / 1000.0,\n",
    "            float(ship.energy < 50),\n",
    "            float(obs.get(\"match_steps\", 0) if isinstance(obs, dict) else getattr(obs, \"match_steps\", 0)) / 100.0\n",
    "        ]\n",
    "        \n",
    "        # 태스크 상태\n",
    "        task_state = [\n",
    "            float(ship.task == 'explore'),\n",
    "            float(ship.task == 'harvest'),\n",
    "            float(ship.task == 'combat'),\n",
    "            float(ship.task is None)\n",
    "        ] * 3\n",
    "        \n",
    "        # (4) sub_task_state 추가\n",
    "        sub_task_state = [\n",
    "            float(ship.sub_task == 'deep_scout'),\n",
    "            float(ship.sub_task == 'guard_relic'),\n",
    "            float(ship.sub_task is None)\n",
    "        ]\n",
    "\n",
    "        # 목표 상태\n",
    "        target_state = []\n",
    "        if ship.target:\n",
    "            dist = manhattan_distance(ship.position, ship.target.coordinates)\n",
    "            target_state.extend([\n",
    "                ship.target.x / 24.0,\n",
    "                ship.target.y / 24.0,\n",
    "                dist / 24.0\n",
    "            ])\n",
    "        else:\n",
    "            target_state.extend([0.0, 0.0, 1.0])\n",
    "        \n",
    "        return np.concatenate([\n",
    "            map_state,      # 576 (24*24)\n",
    "            base_state,     # 8\n",
    "            task_state,     # 12\n",
    "            sub_task_state,  # 3 (예시)\n",
    "            target_state    # 3,\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "    def calculate_reward(self, obs, ship):\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Survival & Energy management (increased base rewards)\n",
    "        if ship.energy > 50:\n",
    "            reward += 0.5  # Increased from 0.1\n",
    "        else:\n",
    "            reward -= 1.0  # Increased from 0.2\n",
    "        \n",
    "        # Team score (main objective)\n",
    "        if self.prev_team_points is not None:\n",
    "            if isinstance(obs, dict):\n",
    "                current_points = float(obs[\"team_points\"][self.team_id])\n",
    "            else:\n",
    "                current_points = float(obs.team_points[self.team_id])\n",
    "            \n",
    "            point_gained = current_points - self.prev_team_points\n",
    "            if point_gained > 0:\n",
    "                reward += point_gained * 2  # Doubled the reward for points\n",
    "        \n",
    "        # Match victory/defeat (increased rewards)\n",
    "        if isinstance(obs, dict):\n",
    "            match_done = obs.get(\"terminated\", False) or obs.get(\"truncated\", False)\n",
    "            match_steps = obs.get(\"match_steps\", 0)\n",
    "        else:\n",
    "            match_done = getattr(obs, \"terminated\", False) or getattr(obs, \"truncated\", False)\n",
    "            match_steps = getattr(obs, \"match_steps\", 0)\n",
    "            \n",
    "        if match_done or match_steps >= 100:\n",
    "            if isinstance(obs, dict):\n",
    "                current_points = float(obs[\"team_points\"][self.team_id])\n",
    "                opponent_points = float(obs[\"team_points\"][1 - self.team_id])\n",
    "            else:\n",
    "                current_points = float(obs.team_points[self.team_id])\n",
    "                opponent_points = float(obs.team_points[1 - self.team_id])\n",
    "                \n",
    "            if current_points > opponent_points:\n",
    "                reward += 10.0  # Increased from 5.0\n",
    "            elif current_points < opponent_points:\n",
    "                reward -= 5.0   # Increased from 2.0\n",
    "        \n",
    "        # Inaction penalty (increased)\n",
    "        if ship.action is None or (isinstance(ship.action, list) and all(a == 0 for a in ship.action)):\n",
    "            reward -= 0.5  # Increased from 0.1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def check_and_update_memories(self):\n",
    "        \"\"\"Properly update all ship memories, ensuring terminal states are marked\"\"\"\n",
    "        for ship in self.fleet.ships:\n",
    "            if not ship.is_active:\n",
    "                continue\n",
    "                \n",
    "            ship_memory = self.memory.ship_memories[ship.unit_id]\n",
    "            \n",
    "            # 1. Check for length consistency\n",
    "            lengths = [\n",
    "                len(ship_memory.states),\n",
    "                len(ship_memory.action_types),\n",
    "                len(ship_memory.detail_actions),\n",
    "                len(ship_memory.type_logprobs),\n",
    "                len(ship_memory.detail_logprobs),\n",
    "                len(ship_memory.values),\n",
    "                len(ship_memory.rewards),\n",
    "                len(ship_memory.is_terminals)\n",
    "            ]\n",
    "            \n",
    "            # 2. If lengths are inconsistent\n",
    "            if len(set(lengths)) > 1:\n",
    "                print(f\"Warning: Inconsistent memory lengths for ship {ship.unit_id}\")\n",
    "                min_length = min(lengths)\n",
    "                ship_memory.states = ship_memory.states[:min_length]\n",
    "                ship_memory.action_types = ship_memory.action_types[:min_length]\n",
    "                ship_memory.detail_actions = ship_memory.detail_actions[:min_length]\n",
    "                ship_memory.type_logprobs = ship_memory.type_logprobs[:min_length]\n",
    "                ship_memory.detail_logprobs = ship_memory.detail_logprobs[:min_length]\n",
    "                ship_memory.values = ship_memory.values[:min_length]\n",
    "                ship_memory.rewards = ship_memory.rewards[:min_length]\n",
    "                ship_memory.is_terminals = ship_memory.is_terminals[:min_length]\n",
    "            \n",
    "            # 3. Mark terminal states for destroyed ships\n",
    "            if not ship.is_active and lengths[0] > 0 and not ship_memory.is_terminals[-1]:\n",
    "                ship_memory.is_terminals[-1] = True\n",
    "            \n",
    "            # 4. Update step counts\n",
    "            ship_memory.steps_collected = len(ship_memory.states)\n",
    "        \n",
    "        # 5. Update total fleet memory steps\n",
    "        self.memory.steps_collected = sum(\n",
    "            memory.steps_collected \n",
    "            for memory in self.memory.ship_memories.values()\n",
    "        )\n",
    "\n",
    "    def act(self, step: int, obs, remainingOverageTime: int = 60):\n",
    "        try:\n",
    "            # Space와 Fleet 업데이트 전 이전 상태 저장\n",
    "            prev_active_ships = {ship.unit_id: ship.is_active for ship in self.fleet.ships}\n",
    "            \n",
    "            self.space.update(obs)\n",
    "            self.fleet.update(obs)\n",
    "            \n",
    "            actions = np.zeros((len(self.fleet.ships), 3), dtype=int)\n",
    "            \n",
    "            if not self.train_mode:\n",
    "                with torch.no_grad():\n",
    "                    for i, ship in enumerate(self.fleet.ships):\n",
    "                        # 이전에 활성화되어 있었고 현재는 비활성화된 경우 스킵\n",
    "                        if prev_active_ships.get(ship.unit_id, False) and not ship.is_active:\n",
    "                            continue\n",
    "                        if not ship.is_active:\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            state = self._process_state(obs, ship)\n",
    "                            final_action = self.ppo.policy.act(state)\n",
    "                            actions[i] = final_action\n",
    "                            ship.action = final_action\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing ship {i}: {e}\")\n",
    "                            continue\n",
    "            else:\n",
    "                # 학습 모드\n",
    "                for i, ship in enumerate(self.fleet.ships):\n",
    "                    # 이전에 활성화되어 있었고 현재는 비활성화된 경우 스킵\n",
    "                    if prev_active_ships.get(ship.unit_id, False) and not ship.is_active:\n",
    "                        continue\n",
    "                    if not ship.is_active:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        # 1. 상태 처리 및 행동 선택\n",
    "                        state = self._process_state(obs, ship)\n",
    "                        final_action = self.ppo.collect_rollout(\n",
    "                            state, \n",
    "                            self.memory,\n",
    "                            ship.unit_id,\n",
    "                            obs,\n",
    "                            ship\n",
    "                        )\n",
    "                        actions[i] = final_action\n",
    "                        ship.action = final_action\n",
    "                        \n",
    "                        # 2. 보상 계산\n",
    "                        reward = self.calculate_reward(obs, ship)\n",
    "                        if isinstance(obs, dict):\n",
    "                            terminated = obs.get(\"terminated\", False)\n",
    "                        else:\n",
    "                            terminated = getattr(obs, \"terminated\", False)\n",
    "                        \n",
    "                        # 3. 보상과 종료 상태만 별도로 저장\n",
    "                        self.memory.push_ship_transition(\n",
    "                            ship_id=ship.unit_id,\n",
    "                            state=None,\n",
    "                            action_type=None,\n",
    "                            detail_action=None,\n",
    "                            type_logprob=None,\n",
    "                            detail_logprob=None,\n",
    "                            value=None,\n",
    "                            reward=reward,\n",
    "                            is_terminal=terminated\n",
    "                        )\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError processing ship {i}:\")\n",
    "                        print(f\"Exception: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            # 다음 스텝을 위해 점수 기록\n",
    "            current_points = float(obs[\"team_points\"][self.team_id]) if isinstance(obs, dict) else float(obs.team_points[self.team_id])\n",
    "            self.prev_team_points = current_points\n",
    "            \n",
    "            return actions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error in act method:\")\n",
    "            print(f\"Exception: {str(e)}\")\n",
    "            return np.zeros((len(self.fleet.ships), 3), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent/train.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "from luxai_s3.params import EnvParams\n",
    "from agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import traceback  # 상단에 추가\n",
    "\n",
    "class SyncTrainer:\n",
    "    def __init__(self, agents: List[Agent], target_steps: int = 2048, gae_lambda: float = 0.95):\n",
    "        self.agents = agents\n",
    "        self.target_steps = target_steps\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def should_update(self) -> bool:\n",
    "        return all(agent.memory.steps_collected >= self.target_steps for agent in self.agents)\n",
    "    \n",
    "    def compute_gae_per_ship(self, rewards: List[float], values: List[float], \n",
    "                           next_value: float, dones: List[bool], gamma: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        try:\n",
    "            valid_data = [(r, v, d) for r, v, d in zip(rewards, values, dones) if r is not None]\n",
    "            if not valid_data:\n",
    "                return torch.tensor([]), torch.tensor([])\n",
    "                \n",
    "            rewards, values, dones = zip(*valid_data)\n",
    "            \n",
    "            gae = 0\n",
    "            returns = []\n",
    "            advantages = []\n",
    "            \n",
    "            for step in reversed(range(len(rewards))):\n",
    "                if step == len(rewards) - 1:\n",
    "                    next_non_terminal = 1.0 - float(dones[-1])\n",
    "                    next_val = next_value\n",
    "                else:\n",
    "                    next_non_terminal = 1.0 - float(dones[step + 1])\n",
    "                    next_val = values[step + 1]\n",
    "                \n",
    "                delta = rewards[step] + gamma * next_val * next_non_terminal - values[step]\n",
    "                gae = delta + gamma * self.gae_lambda * next_non_terminal * gae\n",
    "                \n",
    "                returns.insert(0, gae + values[step])\n",
    "                advantages.insert(0, gae)\n",
    "                \n",
    "            returns = torch.tensor(returns)\n",
    "            advantages = torch.tensor(advantages)\n",
    "            \n",
    "            if len(advantages) > 0:\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            return returns, advantages\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in compute_gae_per_ship: {str(e)}\")\n",
    "            return torch.tensor([]), torch.tensor([])\n",
    "\n",
    "    def sync_update(self) -> Dict[str, float]:\n",
    "        if not self.should_update():\n",
    "            return {}\n",
    "            \n",
    "        update_stats = {}\n",
    "        \n",
    "        print(\"\\nDEBUG - Starting sync_update\")\n",
    "        print(\"Checking memory states before GAE computation...\")\n",
    "        \n",
    "        # Check and update memories for all agents before computing GAE\n",
    "        self.check_all_memories()\n",
    "        \n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            try:\n",
    "                for ship_id, memory in agent.memory.ship_memories.items():\n",
    "                    if memory.steps_collected == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    print(f\"\\nDEBUG - Processing Agent {idx} Ship {ship_id}\")\n",
    "                    print(f\"Steps collected: {memory.steps_collected}\")\n",
    "                    print(f\"Terminal states: {sum(memory.is_terminals)}/{len(memory.is_terminals)}\")\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        last_state = memory.states[-1]\n",
    "                        if isinstance(last_state, torch.Tensor):\n",
    "                            last_state = last_state.to(agent.ppo.device)\n",
    "                        else:\n",
    "                            last_state = torch.FloatTensor(last_state).to(agent.ppo.device)\n",
    "                        \n",
    "                        last_features = agent.ppo.policy_old(last_state)\n",
    "                        next_value = agent.ppo.policy_old.value(last_features).item()\n",
    "                        \n",
    "                        print(f\"Computing GAE for {len(memory.rewards)} steps\")\n",
    "                        values = [v.item() for v in memory.values]\n",
    "                        returns, advantages = self.compute_gae_per_ship(\n",
    "                            rewards=memory.rewards,\n",
    "                            values=values,\n",
    "                            next_value=next_value,\n",
    "                            dones=memory.is_terminals,\n",
    "                            gamma=agent.ppo.gamma\n",
    "                        )\n",
    "                        \n",
    "                        if len(returns) > 0:\n",
    "                            memory.returns = returns\n",
    "                            memory.advantages = advantages\n",
    "                            print(f\"GAE computed successfully. Returns shape: {returns.shape}\")\n",
    "                \n",
    "                # PPO update\n",
    "                policy_loss, value_loss = agent.ppo.update(agent.memory)\n",
    "                update_stats[f'agent{idx}_policy_loss'] = policy_loss\n",
    "                update_stats[f'agent{idx}_value_loss'] = value_loss\n",
    "                print(f\"Agent {idx} updated - Policy Loss: {policy_loss:.4f}, Value Loss: {value_loss:.4f}\")\n",
    "                \n",
    "                agent.memory.clear_all_memories()\n",
    "                print(f\"Agent {idx} memories cleared\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError updating agent {idx}:\")\n",
    "                print(f\"Exception: {str(e)}\")\n",
    "                print(f\"Traceback: {traceback.format_exc()}\")\n",
    "                continue\n",
    "        \n",
    "        return update_stats\n",
    "        \n",
    "    def check_all_memories(self):\n",
    "        \"\"\"Check and fix all memory states across all agents\"\"\"\n",
    "        print(\"\\nDEBUG - Checking all memories\")\n",
    "        \n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            print(f\"\\nAgent {agent_idx} Memory Check:\")\n",
    "            for ship_id, memory in agent.memory.ship_memories.items():\n",
    "                if memory.steps_collected == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # 1. Check length consistency\n",
    "                lengths = [\n",
    "                    len(memory.states),\n",
    "                    len(memory.action_types),\n",
    "                    len(memory.detail_actions),\n",
    "                    len(memory.type_logprobs),\n",
    "                    len(memory.detail_logprobs),\n",
    "                    len(memory.values),\n",
    "                    len(memory.rewards),\n",
    "                    len(memory.is_terminals)\n",
    "                ]\n",
    "                \n",
    "                if len(set(lengths)) > 1:\n",
    "                    print(f\"Warning: Ship {ship_id} has inconsistent memory lengths:\")\n",
    "                    print(f\"  States: {len(memory.states)}\")\n",
    "                    print(f\"  Action types: {len(memory.action_types)}\")\n",
    "                    print(f\"  Detail actions: {len(memory.detail_actions)}\")\n",
    "                    print(f\"  Type logprobs: {len(memory.type_logprobs)}\")\n",
    "                    print(f\"  Detail logprobs: {len(memory.detail_logprobs)}\")\n",
    "                    print(f\"  Values: {len(memory.values)}\")\n",
    "                    print(f\"  Rewards: {len(memory.rewards)}\")\n",
    "                    print(f\"  Terminals: {len(memory.is_terminals)}\")\n",
    "                    \n",
    "                    # Truncate to shortest length\n",
    "                    min_length = min(lengths)\n",
    "                    memory.states = memory.states[:min_length]\n",
    "                    memory.action_types = memory.action_types[:min_length]\n",
    "                    memory.detail_actions = memory.detail_actions[:min_length]\n",
    "                    memory.type_logprobs = memory.type_logprobs[:min_length]\n",
    "                    memory.detail_logprobs = memory.detail_logprobs[:min_length]\n",
    "                    memory.values = memory.values[:min_length]\n",
    "                    memory.rewards = memory.rewards[:min_length]\n",
    "                    memory.is_terminals = memory.is_terminals[:min_length]\n",
    "                    print(f\"  Truncated all lists to length {min_length}\")\n",
    "                \n",
    "                # 2. Check terminal states\n",
    "                terminals_count = sum(memory.is_terminals)\n",
    "                print(f\"\\nShip {ship_id} Terminal States:\")\n",
    "                print(f\"  Total steps: {memory.steps_collected}\")\n",
    "                print(f\"  Terminal states: {terminals_count}\")\n",
    "                print(f\"  Is currently terminal: {memory.is_terminals[-1] if memory.is_terminals else False}\")\n",
    "                \n",
    "                # 3. Update step counts\n",
    "                memory.steps_collected = len(memory.states)\n",
    "                \n",
    "            # 4. Update total fleet memory steps\n",
    "            agent.memory.steps_collected = sum(\n",
    "                m.steps_collected for m in agent.memory.ship_memories.values()\n",
    "            )\n",
    "            print(f\"Agent {agent_idx} total steps: {agent.memory.steps_collected}\")\n",
    "    \n",
    "class CheckpointManager:\n",
    "    def __init__(self, save_dir='checkpoints'):\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def save_checkpoint(self, agents, training_stats, episode):\n",
    "        filename = f'checkpoint_episode_{episode:04d}.pth'\n",
    "        path = os.path.join(self.save_dir, filename)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'episode': episode,\n",
    "            'agent0': {\n",
    "                'model': agents[0].ppo.policy.state_dict(),\n",
    "                'optimizer': agents[0].ppo.optimizer.state_dict()\n",
    "            },\n",
    "            'agent1': {\n",
    "                'model': agents[1].ppo.policy.state_dict(),\n",
    "                'optimizer': agents[1].ppo.optimizer.state_dict()\n",
    "            },\n",
    "            'training_stats': training_stats,\n",
    "            'hyperparameters': {\n",
    "                'learning_rate': agents[0].ppo.optimizer.param_groups[0]['lr'],\n",
    "                'gamma': agents[0].ppo.gamma,\n",
    "                'eps_clip': agents[0].ppo.eps_clip,\n",
    "                'K_epochs': agents[0].ppo.K_epochs,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Saved checkpoint: {path}\")\n",
    "        \n",
    "        # CSV도 함께 저장\n",
    "        stats_df = pd.DataFrame(training_stats)\n",
    "        stats_df.to_csv(os.path.join(self.save_dir, f'training_stats_{episode:04d}.csv'), index=False)\n",
    "        \n",
    "    def load_checkpoint(self, agents, filename=None):\n",
    "        if filename is None:\n",
    "            filename = self._get_latest_checkpoint()\n",
    "            if filename is None:\n",
    "                raise FileNotFoundError(\"No checkpoints found\")\n",
    "        \n",
    "        path = os.path.join(self.save_dir, filename)\n",
    "        print(f\"Loading checkpoint: {path}\")\n",
    "        \n",
    "        checkpoint = torch.load(path)\n",
    "        \n",
    "        # 모델과 옵티마이저 상태 복원\n",
    "        agents[0].ppo.policy.load_state_dict(checkpoint['agent0']['model'])\n",
    "        agents[0].ppo.optimizer.load_state_dict(checkpoint['agent0']['optimizer'])\n",
    "        agents[1].ppo.policy.load_state_dict(checkpoint['agent1']['model'])\n",
    "        agents[1].ppo.optimizer.load_state_dict(checkpoint['agent1']['optimizer'])\n",
    "        \n",
    "        return checkpoint['episode'], checkpoint['training_stats']\n",
    "    \n",
    "    def _get_latest_checkpoint(self):\n",
    "        checkpoints = glob.glob(os.path.join(self.save_dir, 'checkpoint_episode_*.pth'))\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        \n",
    "        # 에피소드 번호로 정렬\n",
    "        checkpoints.sort(key=lambda x: int(re.search(r'episode_(\\d+)', x).group(1)))\n",
    "        return os.path.basename(checkpoints[-1])\n",
    "\n",
    "def process_reward(reward):\n",
    "    \"\"\"JAX → numpy float 변환 보조 함수\"\"\"\n",
    "    if hasattr(reward, 'device_buffer'):\n",
    "        return np.array(reward)\n",
    "    return reward\n",
    "\n",
    "def plot_training_stats(stats, save_dir='checkpoints'):\n",
    "    # 보상 그래프\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(stats['episodes'], stats['rewards_0'], label='Agent 0')\n",
    "    plt.plot(stats['episodes'], stats['rewards_1'], label='Agent 1')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Rewards')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss 그래프\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(stats['episodes'], stats['policy_losses_0'], label='Policy Loss')\n",
    "    plt.plot(stats['episodes'], stats['value_losses_0'], label='Value Loss')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 승리 수 그래프\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(stats['episodes'], stats['match_wins_0'], label='Agent 0 Wins')\n",
    "    plt.plot(stats['episodes'], stats['match_wins_1'], label='Agent 1 Wins')\n",
    "    plt.axhline(y=2.5, color='r', linestyle='--', alpha=0.3)  # 기대값 라인\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Wins per Episode')\n",
    "    plt.title('Match Wins (out of 5)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # FPS 그래프\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(stats['episodes'], stats['fps'])\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps per Second')\n",
    "    plt.title('Training Speed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_stats.png'))\n",
    "    plt.close()\n",
    "\n",
    "def train(num_episodes=1001, log_interval=10, resume_training=False, target_steps=2048):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    checkpoint_manager = CheckpointManager()\n",
    "    \n",
    "    env = LuxAIS3GymEnv()\n",
    "    env_params = EnvParams(\n",
    "        #map_type=1,\n",
    "        max_steps_in_match=100,\n",
    "        match_count_per_episode=5\n",
    "    )\n",
    "    \n",
    "    agent0 = Agent(\"player_0\", env_params, device=device, train_mode=True, target_steps=target_steps)\n",
    "    agent1 = Agent(\"player_1\", env_params, device=device, train_mode=True, target_steps=target_steps)\n",
    "    agents = [agent0, agent1]\n",
    "    \n",
    "    sync_trainer = SyncTrainer(agents, target_steps=target_steps)\n",
    "    \n",
    "    start_episode = 0\n",
    "    training_stats = {\n",
    "        'episodes': [],\n",
    "        'rewards_0': [],\n",
    "        'rewards_1': [],\n",
    "        'policy_losses_0': [],\n",
    "        'value_losses_0': [],\n",
    "        'steps_per_episode': [],\n",
    "        'fps': [],\n",
    "        'match_wins_0': [],\n",
    "        'match_wins_1': []\n",
    "    }\n",
    "    \n",
    "    if resume_training:\n",
    "        try:\n",
    "            start_episode, training_stats = checkpoint_manager.load_checkpoint(agents)\n",
    "            print(f\"Resumed training from episode {start_episode}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: {e}. Starting fresh training.\")\n",
    "    \n",
    "    total_steps = 0\n",
    "    start_time = time.time()\n",
    "    policy_loss0 = value_loss0 = 0.0\n",
    "    policy_loss1 = value_loss1 = 0.0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for i_episode in range(start_episode, num_episodes):\n",
    "        episode_reward_0 = 0.0\n",
    "        episode_reward_1 = 0.0\n",
    "        steps_in_episode = 0\n",
    "        match_wins_0 = 0\n",
    "        match_wins_1 = 0\n",
    "        \n",
    "        obs, info = env.reset(seed=i_episode, options=dict(params=env_params))\n",
    "        for agent in agents:\n",
    "            agent.memory.clear_all_memories()\n",
    "        \n",
    "        for match_idx in range(5):\n",
    "            match_steps = 0\n",
    "            match_reward_0 = 0.0\n",
    "            match_reward_1 = 0.0\n",
    "            match_done = False\n",
    "            \n",
    "            while not match_done and match_steps < 100:\n",
    "                match_steps += 1\n",
    "                steps_in_episode += 1\n",
    "                total_steps += 1\n",
    "                \n",
    "                # Check and update memories periodically during training\n",
    "                if match_steps % 20 == 0:  # Every 20 steps\n",
    "                    for agent in agents:\n",
    "                        agent.check_and_update_memories()\n",
    "                \n",
    "                actions0 = agent0.act(steps_in_episode, obs[\"player_0\"])\n",
    "                actions1 = agent1.act(steps_in_episode, obs[\"player_1\"])\n",
    "                \n",
    "                next_obs, reward, terminated, truncated, info = env.step({\n",
    "                    \"player_0\": actions0,\n",
    "                    \"player_1\": actions1\n",
    "                })\n",
    "                \n",
    "                r0 = float(process_reward(reward[\"player_0\"]))\n",
    "                r1 = float(process_reward(reward[\"player_1\"]))\n",
    "                match_reward_0 += r0\n",
    "                match_reward_1 += r1\n",
    "                episode_reward_0 += r0\n",
    "                episode_reward_1 += r1\n",
    "                \n",
    "                match_done = terminated[\"player_0\"] or truncated[\"player_0\"] or match_steps >= 100\n",
    "                \n",
    "                if sync_trainer.should_update():\n",
    "                    # Check and update memories before PPO update\n",
    "                    for agent in agents:\n",
    "                        agent.check_and_update_memories()\n",
    "                    update_stats = sync_trainer.sync_update()\n",
    "                    if update_stats:\n",
    "                        policy_loss0 = update_stats['agent0_policy_loss']\n",
    "                        value_loss0 = update_stats['agent0_value_loss']\n",
    "                        policy_loss1 = update_stats['agent1_policy_loss']\n",
    "                        value_loss1 = update_stats['agent1_value_loss']\n",
    "                \n",
    "                obs = next_obs\n",
    "            \n",
    "            if match_done:\n",
    "                # Check and update memories at match end\n",
    "                for agent in agents:\n",
    "                    agent.check_and_update_memories()\n",
    "                \n",
    "                if match_reward_0 > match_reward_1:\n",
    "                    match_wins_0 += 1\n",
    "                elif match_reward_1 > match_reward_0:\n",
    "                    match_wins_1 += 1\n",
    "            \n",
    "            if match_idx < 4:\n",
    "                obs = next_obs\n",
    "        \n",
    "        # 에피소드 종료 통계 저장 및 출력\n",
    "        if i_episode % log_interval == 0:\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            fps = total_steps / (elapsed if elapsed > 0 else 1e-9)\n",
    "            \n",
    "            training_stats['episodes'].append(i_episode)\n",
    "            training_stats['rewards_0'].append(episode_reward_0)\n",
    "            training_stats['rewards_1'].append(episode_reward_1)\n",
    "            training_stats['policy_losses_0'].append(policy_loss0)\n",
    "            training_stats['value_losses_0'].append(value_loss0)\n",
    "            training_stats['steps_per_episode'].append(steps_in_episode)\n",
    "            training_stats['fps'].append(fps)\n",
    "            training_stats['match_wins_0'].append(match_wins_0)\n",
    "            training_stats['match_wins_1'].append(match_wins_1)\n",
    "            \n",
    "            print(f\"Episode {i_episode:4d} | \"\n",
    "                  f\"Reward0: {episode_reward_0:.2f} (Wins: {match_wins_0}) | \"\n",
    "                  f\"Reward1: {episode_reward_1:.2f} (Wins: {match_wins_1}) | \"\n",
    "                  f\"Steps: {steps_in_episode} | \"\n",
    "                  f\"FPS: {fps:.2f} | \"\n",
    "                  f\"(PolicyLoss0: {policy_loss0:.4f}, ValueLoss0: {value_loss0:.4f})\")\n",
    "        \n",
    "        # 체크포인트 저장\n",
    "        if i_episode % 10 == 0:\n",
    "            checkpoint_manager.save_checkpoint(agents, training_stats, i_episode)\n",
    "            plot_training_stats(training_stats)\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--resume', action='store_true', help='Resume from latest checkpoint')\n",
    "    parser.add_argument('--episodes', type=int, default=1000, help='Number of episodes to train')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, help='Log interval')\n",
    "    parser.add_argument('--target-steps', type=int, default=2048, help='Steps to collect before PPO update')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    stats = train(\n",
    "        num_episodes=args.episodes,\n",
    "        log_interval=args.log_interval,\n",
    "        resume_training=args.resume,\n",
    "        target_steps=args.target_steps\n",
    "    )\n",
    "    \n",
    "    # 최종 학습 결과 시각화\n",
    "    plot_training_stats(stats)\n",
    "\n",
    "!python agent/train.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10395677,
     "sourceId": 86411,
     "sourceType": "competition"
    },
    {
     "datasetId": 6585062,
     "sourceId": 10635711,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
