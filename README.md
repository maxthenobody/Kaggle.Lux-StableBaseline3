# Lux AI Season 3 – Reinforcement Learning Agent (Kaggle Competition)
# https://www.kaggle.com/competitions/lux-ai-season-3

## Overview
This project implements an **experimental reinforcement learning agent** for the **Lux AI Season 3** competition on Kaggle (NeurIPS 2024). Lux AI is a two-player strategy game where each side controls units on a 24x24 grid to collect resources, fight opponents, and capture relics. The goal here was not just to compete, but to **learn about deep reinforcement learning** by building an agent from scratch. The agent was trained using **Proximal Policy Optimization (PPO)** from the Stable Baselines3 (SB3) library, heavily customized to handle the complex observation and action space of the game. This was a learning-focused project: the journey of designing novel neural network architectures and training pipelines was the priority, even though the final model was too large to submit under Kaggle’s hidden file size limits (hence no official leaderboard result).

## Highlights
* **Custom Gym Environment Wrapper**: Developed a custom OpenAI Gym-compatible wrapper for the Lux AI environment to output a rich **dictionary observation space** and handle the multi-agent setup. The observation includes multiple feature planes (e.g. a 24x24 map of terrain and energy, vision masks) and unit state vectors for both teams. This wrapper makes it possible to interface Lux AI with standard RL libraries.
* **Neural Network Architecture Innovations**: Designed and integrated a **custom neural network** for the agent’s policy/value function. A **CNN-based feature extractor** processes spatial 24x24 grid data (e.g. maps of resources/terrain) while other numerical features are flattened and concatenated. This **Multi-input** architecture combines convolutional layers for spatial features with fully-connected layers for non-spatial data, enabling the agent to handle the diverse observation inputs.
* **Stable Baselines3 Customization**: Extended the SB3 framework by subclassing and modifying its components to use the custom network. For example, a bespoke MultiInputPolicy was implemented to incorporate the CNN extractor and an enhanced MLP backbone. We integrated these components into SB3’s PPO training loop, effectively **injecting our custom model into the SB3 pipeline** while reusing stable training algorithms (e.g. advantage estimation, optimization routines). This demonstrates deep understanding of the RL library’s internals and how to extend them.
* **Multi-Discrete Action Handling**: The Lux AI game requires selecting actions for up to 16 units simultaneously, with each action composed of multiple parts (e.g. action type and target coordinates). We implemented a **custom action distribution** to handle this multi-discrete action space. The policy’s forward pass outputs a structured set of logits which are then sampled into per-unit actions (including conditional sub-actions for targeting). This involved building a tailored output layer and sampling procedure (using PyTorch) to ensure the agent can issue commands to all units each time-step.
* **Self-Play Training Setup**: To train in a two-player environment, the agent was configured for **self-play**. The training pipeline can initialize two instances of the policy (one for each team) so that the agent competes against a clone of itself or a previous version. This approach is crucial for multi-agent learning, and it required managing two networks and alternating their roles during experience collection. Self-play ensures the agent improves even in the absence of a human-defined opponent, by continuously adapting to its own strategies.

## Training Process

**Reinforcement Learning Setup**: We trained the agent using **PPO (Proximal Policy Optimization)**, an on-policy RL algorithm, with Stable Baselines3 as the foundation. The environment was set up with multiple parallel instances (using SB3’s VecEnv) to collect experience faster. At each training iteration, the agent (as player 0) played games against an opponent (which in self-play was a copy of the same agent or a past checkpoint). The **reward function** is the game’s scoring mechanism (points for collecting relics and winning the match), which the agent tries to maximize over the course of an episode.

**Neural Network & Policy**: The agent’s policy network receives a **multi-modal observation** and outputs actions for all units. Under the hood, our CustomFeatureExtractor first processes the observation dict: four 24×24 feature maps (e.g. explored terrain, energy on the map, etc.) go through convolutional layers, while all other features (unit counts, coordinates, team scores, etc.) are flattened. These are concatenated into one large feature vector. Next, a deep fully-connected **MLP** (with SiLU activations and layer normalization) maps this vector into latent features. PPO then splits this latent representation into two heads: one for the **policy** (action probabilities) and one for the **value function** (state-value estimation). The policy head in our case is custom-built to produce the multi-discrete action output for 16 units. Rather than treating each of the many discrete action components independently, the network outputs structured logits that are reshaped into the appropriate per-unit action distributions. We sample an action for each unit from these distributions to form the complete MultiDiscrete action. This design allowed implementing conditional actions (for example, only units that choose the “move” or “attack” action need a direction – our network outputs direction logits that are only used if the base action is of that type).

**PPO Training Loop**: With the environment and policy defined, training proceeded in iterations. Each iteration, the agent plays for a number of time-steps (collecting observations, actions, and rewards). This experience is stored in SB3’s rollout buffer. After each rollout, we perform several epochs of PPO update: the policy’s parameters are updated via gradient descent to improve the probability of rewarding actions (while avoiding too large a change per PPO’s clipped objective). We made use of **TensorBoard logging** to track key metrics like average reward, policy loss, value loss, etc., which helped in debugging and hyperparameter tuning. We experimented with hyperparameters such as learning rate (e.g. 1e-5 to 6e-4), entropy bonus (to encourage exploration), and network sizes to stabilize training. Training was computationally intensive – the final model has on the order of tens of millions of parameters – so we utilized GPU acceleration and PyTorch 2.0 optimizations (even compiling parts of the model for speed).

**Self-Play and Curriculum**: Early in development, we trained the agent against a fixed script (the provided Kaggle starter agent) to give it basic skills. As it improved, we moved to self-play: the agent would play against a copy of itself. We introduced a mechanism to occasionally update the opponent to the latest policy (or keep a pool of past versions) to ensure the learning agent always has a challenging adversary. This self-play approach is critical in competitive games to avoid the agent overfitting to a static strategy. In our implementation, we leveraged the custom dual-policy setup – the training code would alternate between using policy and policy_2 for the two players each episode, and updates were applied to the main policy network. This helped the agent gradually learn strategies that work well against different instances of itself.

## Results and Lessons Learned

**Training Outcome**: Although an official competition submission wasn’t achieved, the agent showed clear learning progress. Over many training iterations, the average rewards and win-rates (against baseline bots) improved significantly, indicating the agent was picking up useful strategies. For instance, the training curves showed the agent learning to efficiently gather energy and contest relics as training went on (see the **learning curve plot below**). Qualitatively, when we watched game replays of the trained agent, it demonstrated sensible behavior like grouping units for battles and securing resources on the map. These are encouraging signs that the complex neural network and training setup were successful in teaching the agent non-trivial tactics.

**File Size Challenge**: One unexpected hurdle was the **model size constraint** on Kaggle. Our final policy network – due to the large CNN + MLP architecture needed for the task – resulted in a model file well over 100MB. Unfortunately, the competition’s submission system had a limit (approximately 100MB per submission) that wasn’t clearly advertised in the rules. This meant our agent couldn’t be submitted for official evaluation without significant downsizing or simplification. Compressing or pruning the model further would likely hurt its performance, so we made the difficult decision to accept that this was primarily a research endeavor. While we didn’t get a public leaderboard ranking, this outcome highlighted the trade-off between model complexity and deployability. It’s a valuable lesson in considering practical constraints (like file size or runtime) in addition to raw performance. In a real-world setting or future competitions, we would incorporate such constraints into the design from the start (e.g. using smaller networks or model compression techniques).

**Technical Takeaways**: This project was a deep dive into reinforcement learning engineering. We gained experience in:
* Modifying and extending a popular RL framework (SB3) to fit a new problem – from custom policy definition to low-level algorithm tweaks.
* Handling **multi-agent RL** via self-play, and the challenges of stability and diversity it brings.
* Designing neural network architectures that merge different input modalities (spatial and tabular data) and output structured actions.
* Debugging training of a complex model: we often had to investigate why learning stalled, which involved analyzing reward scaling, gradient norms, and occasionally instrumenting the code with printouts of distributions or implementing curriculum adjustments. This mirrors real-world RL development, which requires equal parts intuition and empirical tuning.

Despite the lack of a competition medal, the **primary goal of learning was absolutely met**. The project showcases the ability to **take initiative** and build a complex AI agent from the ground up, navigate through research challenges, and adapt open-source tools beyond their original scope. This kind of experimental project is invaluable preparation for tackling practical machine learning problems in a professional environment.

## Training Curves (Placeholder)
Below is a placeholder for training performance graphs (e.g. average reward per episode over time, loss curves, etc.). These charts illustrate the agent’s learning progress. In a complete README, we would include screenshots or plots of the training curves here for visualization.

![Training Metrics Example](<images/Screenshot from 2025-03-09 23-57-56.png>)

## Project Structure
The repository is organized into several directories with Jupyter notebooks and supporting modules:
Notebooks/EDA/ – Exploratory Data Analysis. Contains notebooks for exploring the Lux AI environment and data. For example, testing_env.ipynb and simple-ppo-training.ipynb were used early on to validate the environment interface and run small-scale PPO tests. These notebooks helped define observation structures, verify reward signals, and ensure the custom gym wrapper worked correctly before full training.
Notebooks/Agent_Development/ – Iterative Agent Development. This is the core of the project, with notebooks documenting different development stages (timestamped by date). Key components include:
Environment Wrapper (Modified_lux3_wrapper/) – Python module defining ModifiedLuxAIS3GymEnv (a subclass of gym.Env). This wraps the official Lux AI environment to provide a clean Gym API. It defines the observation space (a spaces.Dict with all game features) and action space (a spaces.MultiDiscrete for all unit actions) in a format the RL algorithm can use. It also handles resets, step transitions, and converts between game state and RL-friendly data structures.
Custom PPO and Policy (Modified_stablebaseline3_PPO/ and policy classes) – Modules where Stable Baselines3’s PPO implementation is copied and adjusted for our needs. We extended SB3’s OnPolicyAlgorithm (PPO) to support dual policies for self-play and the complex action outputs. We also created custom policy classes (e.g. CustomMultiInputPolicy) and a CustomFeatureExtractor. These define the neural network architecture described above and override how actions and values are computed.
Training Notebooks (lux3-agent_development_*.ipynb) – Multiple notebooks where experiments were run (with various hyperparameters, architectures, etc.). In these, we instantiate the environment and model, then call model.learn() or custom training loops. We save intermediate models, log to TensorBoard (logs/ directory), and occasionally export models (e.g. to ONNX) for evaluation. Each notebook version represents an improvement or change (for instance, trying different network sizes or training strategies). Together they show the progression of the agent’s learning over time.
Notebooks/Testing_Agents/ – Agent Evaluation and Submission Prep. Notebooks in this folder were used to evaluate the trained agent and prepare for submission. For example, there are scripts to load a learned model and play it against the provided baseline or itself, generating replays for qualitative analysis. There is also a copy of the Kaggle Python kit starter notebook, which we used to integrate our agent into Kaggle’s format (e.g. packaging the model weights and inference logic into a agent.py). This was the staging area for making a submission package and ensuring it ran within the competition environment. (Ultimately, the final agent model file turned out to exceed the competition’s file size limit, which prevented submission—this limit wasn’t initially clear in the rules.)
